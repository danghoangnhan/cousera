{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Application\n",
    "\n",
    "Welcome to Course 4's second assignment! In this notebook, you will:\n",
    "\n",
    "- Create a mood classifer using the TF Keras Sequential API\n",
    "- Build a ConvNet to identify sign language digits using the TF Keras Functional API\n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "\n",
    "- Build and train a ConvNet in TensorFlow for a __binary__ classification problem\n",
    "- Build and train a ConvNet in TensorFlow for a __multiclass__ classification problem\n",
    "- Explain different use cases for the Sequential and Functional APIs\n",
    "\n",
    "To complete this assignment, you should already be familiar with TensorFlow. If you are not, please refer back to the **TensorFlow Tutorial** of the third week of Course 2 (\"**Improving deep neural networks**\").\n",
    "\n",
    "## Important Note on Submission to the AutoGrader\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables.\n",
    "\n",
    "If you do any of the following, you will get something like, `Grader not found` (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don't remember the changes you have made, you can get a fresh copy of the assignment by following these [instructions](https://www.coursera.org/learn/convolutional-neural-networks/supplement/DS4yP/h-ow-to-refresh-your-workspace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - Packages](#1)\n",
    "    - [1.1 - Load the Data and Split the Data into Train/Test Sets](#1-1)\n",
    "- [2 - Layers in TF Keras](#2)\n",
    "- [3 - The Sequential API](#3)\n",
    "    - [3.1 - Create the Sequential Model](#3-1)\n",
    "        - [Exercise 1 - happyModel](#ex-1)\n",
    "    - [3.2 - Train and Evaluate the Model](#3-2)\n",
    "- [4 - The Functional API](#4)\n",
    "    - [4.1 - Load the SIGNS Dataset](#4-1)\n",
    "    - [4.2 - Split the Data into Train/Test Sets](#4-2)\n",
    "    - [4.3 - Forward Propagation](#4-3)\n",
    "        - [Exercise 2 - convolutional_model](#ex-2)\n",
    "    - [4.4 - Train the Model](#4-4)\n",
    "- [5 - History Object](#5)\n",
    "- [6 - Bibliography](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages\n",
    "\n",
    "As usual, begin by loading in the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.python.framework import ops\n",
    "from cnn_utils import *\n",
    "from test_utils import summary, comparator\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - Load the Data and Split the Data into Train/Test Sets\n",
    "\n",
    "You'll be using the Happy House dataset for this part of the assignment, which contains images of peoples' faces. Your task will be to build a ConvNet that determines whether the people in the images are smiling or not -- because they only get to enter the house if they're smiling!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 600\n",
      "number of test examples = 150\n",
      "X_train shape: (600, 64, 64, 3)\n",
      "Y_train shape: (600, 1)\n",
      "X_test shape: (150, 64, 64, 3)\n",
      "Y_test shape: (150, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_happy_dataset()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "\n",
    "# Reshape\n",
    "Y_train = Y_train_orig.T\n",
    "Y_test = Y_test_orig.T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can display the images contained in the dataset. Images are **64x64** pixels in RGB format (3 channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 124\n",
    "plt.imshow(X_train_orig[index]) #display sample training image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Layers in TF Keras \n",
    "\n",
    "In the previous assignment, you created layers manually in numpy. In TF Keras, you don't have to write code directly to create layers. Rather, TF Keras has pre-defined layers you can use. \n",
    "\n",
    "When you create a layer in TF Keras, you are creating a function that takes some input and transforms it into an output you can reuse later. Nice and easy! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - The Sequential API\n",
    "\n",
    "In the previous assignment, you built helper functions using `numpy` to understand the mechanics behind convolutional neural networks. Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call. Keras is a high-level abstraction built on top of TensorFlow, which allows for even more simplified and optimized model creation and training. \n",
    "\n",
    "For the first part of this assignment, you'll create a model using TF Keras' Sequential API, which allows you to build layer by layer, and is ideal for building models where each layer has **exactly one** input tensor and **one** output tensor. \n",
    "\n",
    "As you'll see, using the Sequential API is simple and straightforward, but is only appropriate for simpler, more straightforward tasks. Later in this notebook you'll spend some time building with a more flexible, powerful alternative: the Functional API. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 - Create the Sequential Model\n",
    "\n",
    "As mentioned earlier, the TensorFlow Keras Sequential API can be used to build simple models with layer operations that proceed in a sequential order. \n",
    "\n",
    "You can also add layers incrementally to a Sequential model with the `.add()` method, or remove them using the `.pop()` method, much like you would in a regular Python list.\n",
    "\n",
    "Actually, you can think of a Sequential model as behaving like a list of layers. Like Python lists, Sequential layers are ordered, and the order in which they are specified matters.  If your model is non-linear or contains layers with multiple inputs or outputs, a Sequential model wouldn't be the right choice!\n",
    "\n",
    "For any layer construction in Keras, you'll need to specify the input shape in advance. This is because in Keras, the shape of the weights is based on the shape of the inputs. The weights are only created when the model first sees some input data. Sequential models can be created by passing a list of layers to the Sequential constructor, like you will do in the next assignment.\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - happyModel\n",
    "\n",
    "Implement the `happyModel` function below to build the following model: `ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE`. Take help from [tf.keras.layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers) \n",
    "\n",
    "Also, plug in the following parameters for all the steps:\n",
    "\n",
    " - [ZeroPadding2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ZeroPadding2D): padding 3, input shape 64 x 64 x 3\n",
    " - [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D): Use 32 7x7 filters, stride 1\n",
    " - [BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization): for axis 3\n",
    " - [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU)\n",
    " - [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D): Using default parameters\n",
    " - [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) the previous output.\n",
    " - Fully-connected ([Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) layer: Apply a fully connected layer with 1 neuron and a sigmoid activation. \n",
    " \n",
    " \n",
    " **Hint:**\n",
    " \n",
    " Use **tfl** as shorthand for **tensorflow.keras.layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95d28b191f257bdd5b70c7b8952559d5",
     "grade": false,
     "grade_id": "cell-0e56d3fc28b69aec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: happyModel\n",
    "\n",
    "def happyModel():\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE\n",
    "    \n",
    "    Note that for simplicity and grading purposes, you'll hard-code all the values\n",
    "    such as the stride and kernel (filter) sizes. \n",
    "    Normally, functions should take these values as function parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    model -- TF Keras model (object containing the information for the entire training process) \n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.ZeroPadding2D((3, 3), input_shape=(64, 64, 3)),\n",
    "        ## ZeroPadding2D with padding 3, input shape of 64 x 64 x 3\n",
    "        \n",
    "        tf.keras.layers.Conv2D(32, 7 , (1, 1)),## Conv2D with 32 7x7 filters and stride of 1\n",
    "        tf.keras.layers.BatchNormalization(axis=3),## BatchNormalization for axis 3\n",
    "        tf.keras.layers.ReLU(),## ReLU\n",
    "        tf.keras.layers.MaxPool2D(),## Max Pooling 2D with default parameters\n",
    "        tf.keras.layers.Flatten(),## Flatten layer\n",
    "        tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation='sigmoid')## Dense layer with 1 unit for output & 'sigmoid' activation             \n",
    "        ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d3575c950e2e78149be2d05d671c80d",
     "grade": true,
     "grade_id": "cell-e3e1046e5c33d775",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))]\n",
      "['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform']\n",
      "['BatchNormalization', (None, 64, 64, 32), 128]\n",
      "['ReLU', (None, 64, 64, 32), 0]\n",
      "['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid']\n",
      "['Flatten', (None, 32768), 0]\n",
      "['Dense', (None, 1), 32769, 'sigmoid']\n",
      "\u001b[32mAll tests passed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "happy_model = happyModel()\n",
    "# Print a summary for each layer\n",
    "for layer in summary(happy_model):\n",
    "    print(layer)\n",
    "    \n",
    "output = [['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))],\n",
    "            ['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform'],\n",
    "            ['BatchNormalization', (None, 64, 64, 32), 128],\n",
    "            ['ReLU', (None, 64, 64, 32), 0],\n",
    "            ['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid'],\n",
    "            ['Flatten', (None, 32768), 0],\n",
    "            ['Dense', (None, 1), 32769, 'sigmoid']]\n",
    "    \n",
    "comparator(summary(happy_model), output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "\n",
    "```\n",
    "['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))]\n",
    "['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform']\n",
    "['BatchNormalization', (None, 64, 64, 32), 128]\n",
    "['ReLU', (None, 64, 64, 32), 0]\n",
    "['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid']\n",
    "['Flatten', (None, 32768), 0]\n",
    "['Dense', (None, 1), 32769, 'sigmoid']\n",
    "All tests passed!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your model is created, you can compile it for training with an optimizer and loss of your choice. When the string `accuracy` is specified as a metric, the type of accuracy used will be automatically converted based on the loss function used. This is one of the many optimizations built into TensorFlow that make your life easier! If you'd like to read more on how the compiler operates, check the docs [here](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_model.compile(optimizer='adam',\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to check your model's parameters with the `.summary()` method. This will display the types of layers you have, the shape of the outputs, and how many parameters are in each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d (ZeroPadding2 (None, 70, 70, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        4736      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 32769     \n",
      "=================================================================\n",
      "Total params: 37,633\n",
      "Trainable params: 37,569\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "happy_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - Train and Evaluate the Model\n",
    "\n",
    "After creating the model, compiling it with your choice of optimizer and loss function, and doing a sanity check on its contents, you are now ready to build! \n",
    "\n",
    "Simply call `.fit()` to train. That's it! No need for mini-batching, saving, or complex backpropagation computations. That's all been done for you, as you're using a TensorFlow dataset with the batches specified already. You do have the option to specify epoch number or minibatch size if you like (for example, in the case of an un-batched dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/260\n",
      "38/38 [==============================] - 4s 103ms/step - loss: 0.8615 - accuracy: 0.7633\n",
      "Epoch 2/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.2933 - accuracy: 0.8833\n",
      "Epoch 3/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.2456 - accuracy: 0.9100\n",
      "Epoch 4/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.2416 - accuracy: 0.9033\n",
      "Epoch 5/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.1690 - accuracy: 0.9300\n",
      "Epoch 6/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0879 - accuracy: 0.9667\n",
      "Epoch 7/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0747 - accuracy: 0.9783\n",
      "Epoch 8/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0689 - accuracy: 0.9800\n",
      "Epoch 9/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0734 - accuracy: 0.9767\n",
      "Epoch 10/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0960 - accuracy: 0.9633\n",
      "Epoch 11/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0724 - accuracy: 0.9700\n",
      "Epoch 12/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0934 - accuracy: 0.9733\n",
      "Epoch 13/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0702 - accuracy: 0.9767\n",
      "Epoch 14/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0775 - accuracy: 0.9700\n",
      "Epoch 15/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0759 - accuracy: 0.9700\n",
      "Epoch 16/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0529 - accuracy: 0.9833\n",
      "Epoch 17/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.1642 - accuracy: 0.9417\n",
      "Epoch 18/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0749 - accuracy: 0.9767\n",
      "Epoch 19/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0566 - accuracy: 0.9817\n",
      "Epoch 20/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.1149 - accuracy: 0.9650\n",
      "Epoch 21/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0981 - accuracy: 0.9617\n",
      "Epoch 22/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0997 - accuracy: 0.9700\n",
      "Epoch 23/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0707 - accuracy: 0.9800 0s - loss: 0.0\n",
      "Epoch 24/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0838 - accuracy: 0.9717\n",
      "Epoch 25/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0492 - accuracy: 0.9833\n",
      "Epoch 26/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0955 - accuracy: 0.9733\n",
      "Epoch 27/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.1713 - accuracy: 0.9417\n",
      "Epoch 28/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0289 - accuracy: 0.9900\n",
      "Epoch 29/260\n",
      "38/38 [==============================] - 4s 100ms/step - loss: 0.0409 - accuracy: 0.9800\n",
      "Epoch 30/260\n",
      "38/38 [==============================] - 4s 92ms/step - loss: 0.1139 - accuracy: 0.9633\n",
      "Epoch 31/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.1826 - accuracy: 0.9433\n",
      "Epoch 32/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0420 - accuracy: 0.9933\n",
      "Epoch 33/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0213 - accuracy: 0.9950\n",
      "Epoch 34/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0562 - accuracy: 0.9850\n",
      "Epoch 35/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0462 - accuracy: 0.9850\n",
      "Epoch 36/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0550 - accuracy: 0.9817\n",
      "Epoch 37/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0299 - accuracy: 0.9917\n",
      "Epoch 38/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0401 - accuracy: 0.9833\n",
      "Epoch 39/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0396 - accuracy: 0.9833\n",
      "Epoch 40/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0571 - accuracy: 0.9817\n",
      "Epoch 41/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0765 - accuracy: 0.9750\n",
      "Epoch 42/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0504 - accuracy: 0.9883\n",
      "Epoch 43/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0258 - accuracy: 0.9900\n",
      "Epoch 44/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0190 - accuracy: 0.9950\n",
      "Epoch 45/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0376 - accuracy: 0.9883\n",
      "Epoch 46/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0245 - accuracy: 0.9900\n",
      "Epoch 47/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0205 - accuracy: 0.9950\n",
      "Epoch 48/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0156 - accuracy: 0.9950\n",
      "Epoch 49/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0187 - accuracy: 0.9900\n",
      "Epoch 50/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0240 - accuracy: 0.9917\n",
      "Epoch 51/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0221 - accuracy: 0.9867\n",
      "Epoch 52/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0278 - accuracy: 0.9933\n",
      "Epoch 53/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0342 - accuracy: 0.9917\n",
      "Epoch 54/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0237 - accuracy: 0.9900\n",
      "Epoch 55/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0326 - accuracy: 0.9950\n",
      "Epoch 56/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0159 - accuracy: 0.9967\n",
      "Epoch 57/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0221 - accuracy: 0.9933\n",
      "Epoch 58/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0379 - accuracy: 0.9883\n",
      "Epoch 59/260\n",
      "38/38 [==============================] - 4s 100ms/step - loss: 0.0554 - accuracy: 0.9817\n",
      "Epoch 60/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0343 - accuracy: 0.9900\n",
      "Epoch 61/260\n",
      "38/38 [==============================] - 4s 100ms/step - loss: 0.0351 - accuracy: 0.9817\n",
      "Epoch 62/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0119 - accuracy: 0.9967\n",
      "Epoch 63/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.1756 - accuracy: 0.9533\n",
      "Epoch 64/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0368 - accuracy: 0.9900\n",
      "Epoch 65/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0458 - accuracy: 0.9867\n",
      "Epoch 66/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0401 - accuracy: 0.9850\n",
      "Epoch 67/260\n",
      "38/38 [==============================] - 4s 100ms/step - loss: 0.0445 - accuracy: 0.9883\n",
      "Epoch 68/260\n",
      "38/38 [==============================] - 4s 100ms/step - loss: 0.0242 - accuracy: 0.9867\n",
      "Epoch 69/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0140 - accuracy: 0.9933\n",
      "Epoch 70/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0282 - accuracy: 0.9950\n",
      "Epoch 71/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0032 - accuracy: 0.9983\n",
      "Epoch 72/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 8.2162e-04 - accuracy: 1.0000\n",
      "Epoch 73/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 9.0249e-04 - accuracy: 1.0000\n",
      "Epoch 74/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 75/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 8.5535e-04 - accuracy: 1.0000\n",
      "Epoch 76/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0032 - accuracy: 0.9983\n",
      "Epoch 77/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0032 - accuracy: 0.9983\n",
      "Epoch 78/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 9.4470e-04 - accuracy: 1.0000\n",
      "Epoch 79/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 2.6211e-04 - accuracy: 1.0000\n",
      "Epoch 80/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 2.0363e-04 - accuracy: 1.0000\n",
      "Epoch 81/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 2.2752e-04 - accuracy: 1.0000\n",
      "Epoch 82/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 2.1391e-04 - accuracy: 1.0000\n",
      "Epoch 83/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.7810e-04 - accuracy: 1.0000\n",
      "Epoch 84/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.7053e-04 - accuracy: 1.0000\n",
      "Epoch 85/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 2.5097e-04 - accuracy: 1.0000\n",
      "Epoch 86/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 2.8937e-04 - accuracy: 1.0000\n",
      "Epoch 87/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.5079e-04 - accuracy: 1.0000\n",
      "Epoch 88/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.6234e-04 - accuracy: 1.0000\n",
      "Epoch 89/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.7061e-04 - accuracy: 1.0000\n",
      "Epoch 90/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.4575e-04 - accuracy: 1.0000\n",
      "Epoch 91/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 2.1251e-04 - accuracy: 1.0000\n",
      "Epoch 92/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.7887e-04 - accuracy: 1.0000\n",
      "Epoch 93/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.9965e-04 - accuracy: 1.0000\n",
      "Epoch 94/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.9692e-04 - accuracy: 1.0000\n",
      "Epoch 95/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.9557e-04 - accuracy: 1.0000\n",
      "Epoch 96/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.5388e-04 - accuracy: 1.0000\n",
      "Epoch 97/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.4458e-04 - accuracy: 1.0000\n",
      "Epoch 98/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.4395e-04 - accuracy: 1.0000\n",
      "Epoch 99/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.3373e-04 - accuracy: 1.0000 0s - loss: 1.5748e-04 - \n",
      "Epoch 100/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.3150e-04 - accuracy: 1.0000\n",
      "Epoch 101/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.5717e-04 - accuracy: 1.0000\n",
      "Epoch 102/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.5256e-04 - accuracy: 1.0000\n",
      "Epoch 103/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.1203e-04 - accuracy: 1.0000\n",
      "Epoch 104/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.4098e-04 - accuracy: 1.0000\n",
      "Epoch 105/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.5212e-04 - accuracy: 1.0000\n",
      "Epoch 106/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.4031e-04 - accuracy: 1.0000\n",
      "Epoch 107/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.1229e-04 - accuracy: 1.0000\n",
      "Epoch 108/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.3009e-04 - accuracy: 1.0000\n",
      "Epoch 109/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.2941e-04 - accuracy: 1.0000\n",
      "Epoch 110/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.2499e-04 - accuracy: 1.0000\n",
      "Epoch 111/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.3417e-04 - accuracy: 1.0000\n",
      "Epoch 112/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 9.0206e-05 - accuracy: 1.0000\n",
      "Epoch 113/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.4140e-04 - accuracy: 1.0000\n",
      "Epoch 114/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.3777e-04 - accuracy: 1.0000\n",
      "Epoch 115/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.3396e-04 - accuracy: 1.0000\n",
      "Epoch 116/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.6673e-04 - accuracy: 1.0000\n",
      "Epoch 117/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.1117e-04 - accuracy: 1.0000\n",
      "Epoch 118/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.6638e-04 - accuracy: 1.0000\n",
      "Epoch 119/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 2.0152e-04 - accuracy: 1.0000\n",
      "Epoch 120/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 5.1782e-04 - accuracy: 1.0000\n",
      "Epoch 121/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0037 - accuracy: 0.9983\n",
      "Epoch 122/260\n",
      "38/38 [==============================] - 4s 100ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 123/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.1313 - accuracy: 0.9650\n",
      "Epoch 124/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.4175 - accuracy: 0.9083\n",
      "Epoch 125/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.2259 - accuracy: 0.9533\n",
      "Epoch 126/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0798 - accuracy: 0.9783\n",
      "Epoch 127/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0637 - accuracy: 0.9883\n",
      "Epoch 128/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0442 - accuracy: 0.9917\n",
      "Epoch 129/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0387 - accuracy: 0.9867\n",
      "Epoch 130/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0128 - accuracy: 0.9967\n",
      "Epoch 131/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0222 - accuracy: 0.9933\n",
      "Epoch 132/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0389 - accuracy: 0.9867\n",
      "Epoch 133/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0247 - accuracy: 0.9917\n",
      "Epoch 134/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0271 - accuracy: 0.9950\n",
      "Epoch 135/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0277 - accuracy: 0.9950\n",
      "Epoch 136/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0178 - accuracy: 0.9967\n",
      "Epoch 137/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0239 - accuracy: 0.9950\n",
      "Epoch 138/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0274 - accuracy: 0.9917\n",
      "Epoch 139/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.1355 - accuracy: 0.9700\n",
      "Epoch 140/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0477 - accuracy: 0.9817\n",
      "Epoch 141/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0205 - accuracy: 0.9950\n",
      "Epoch 142/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0090 - accuracy: 0.9983\n",
      "Epoch 143/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 8.6222e-04 - accuracy: 1.0000\n",
      "Epoch 144/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0031 - accuracy: 0.9983\n",
      "Epoch 145/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 3.2671e-04 - accuracy: 1.0000\n",
      "Epoch 146/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 2.6752e-04 - accuracy: 1.0000\n",
      "Epoch 147/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 4.1270e-04 - accuracy: 1.0000\n",
      "Epoch 148/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 2.0156e-04 - accuracy: 1.0000\n",
      "Epoch 149/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.9059e-04 - accuracy: 1.0000\n",
      "Epoch 150/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.3983e-04 - accuracy: 1.0000\n",
      "Epoch 151/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.1286e-04 - accuracy: 1.0000\n",
      "Epoch 152/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.4204e-04 - accuracy: 1.0000\n",
      "Epoch 153/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 2.8734e-04 - accuracy: 1.0000\n",
      "Epoch 154/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 4.3785e-04 - accuracy: 1.0000\n",
      "Epoch 155/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 9.6834e-04 - accuracy: 1.0000\n",
      "Epoch 156/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0045 - accuracy: 0.9967\n",
      "Epoch 157/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 6.4562e-04 - accuracy: 1.0000\n",
      "Epoch 158/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0049 - accuracy: 0.9983\n",
      "Epoch 159/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0284 - accuracy: 0.9917\n",
      "Epoch 160/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0305 - accuracy: 0.9883\n",
      "Epoch 161/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0176 - accuracy: 0.9950\n",
      "Epoch 162/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0245 - accuracy: 0.9917\n",
      "Epoch 163/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0047 - accuracy: 0.9967\n",
      "Epoch 164/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0241 - accuracy: 0.9917\n",
      "Epoch 165/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0491 - accuracy: 0.9850\n",
      "Epoch 166/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 167/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 168/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 8.8149e-04 - accuracy: 1.0000\n",
      "Epoch 169/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 170/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0038 - accuracy: 0.9983\n",
      "Epoch 171/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0356 - accuracy: 0.9900\n",
      "Epoch 172/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0124 - accuracy: 0.9967\n",
      "Epoch 173/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.0047 - accuracy: 0.9983\n",
      "Epoch 174/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 8.9532e-04 - accuracy: 1.0000\n",
      "Epoch 175/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.5100e-04 - accuracy: 1.0000\n",
      "Epoch 176/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 1.3316e-04 - accuracy: 1.0000\n",
      "Epoch 177/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 7.5844e-05 - accuracy: 1.0000\n",
      "Epoch 178/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.2433e-04 - accuracy: 1.0000\n",
      "Epoch 179/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 1.3545e-04 - accuracy: 1.0000\n",
      "Epoch 180/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 7.3679e-05 - accuracy: 1.0000\n",
      "Epoch 181/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 9.0988e-05 - accuracy: 1.0000\n",
      "Epoch 182/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 8.2813e-05 - accuracy: 1.0000\n",
      "Epoch 183/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 9.8014e-05 - accuracy: 1.0000\n",
      "Epoch 184/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 9.7759e-05 - accuracy: 1.0000\n",
      "Epoch 185/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 7.7622e-05 - accuracy: 1.0000\n",
      "Epoch 186/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 7.0699e-05 - accuracy: 1.0000\n",
      "Epoch 187/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 7.9294e-05 - accuracy: 1.0000\n",
      "Epoch 188/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 7.9249e-05 - accuracy: 1.0000\n",
      "Epoch 189/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 6.6150e-05 - accuracy: 1.0000\n",
      "Epoch 190/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 5.9954e-05 - accuracy: 1.0000\n",
      "Epoch 191/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 7.0875e-05 - accuracy: 1.0000\n",
      "Epoch 192/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 7.2363e-05 - accuracy: 1.0000\n",
      "Epoch 193/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 6.3214e-05 - accuracy: 1.0000\n",
      "Epoch 194/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 5.8557e-05 - accuracy: 1.0000\n",
      "Epoch 195/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 6.0995e-05 - accuracy: 1.0000\n",
      "Epoch 196/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 6.8654e-05 - accuracy: 1.0000\n",
      "Epoch 197/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 8.7938e-05 - accuracy: 1.0000\n",
      "Epoch 198/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 6.8855e-05 - accuracy: 1.0000\n",
      "Epoch 199/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 8.2723e-05 - accuracy: 1.0000\n",
      "Epoch 200/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 6.3678e-05 - accuracy: 1.0000\n",
      "Epoch 201/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 6.4276e-05 - accuracy: 1.0000\n",
      "Epoch 202/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 6.8945e-05 - accuracy: 1.0000\n",
      "Epoch 203/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 6.4568e-05 - accuracy: 1.0000\n",
      "Epoch 204/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 1.3490e-04 - accuracy: 1.0000\n",
      "Epoch 205/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 7.0782e-05 - accuracy: 1.0000\n",
      "Epoch 206/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 6.0621e-05 - accuracy: 1.0000\n",
      "Epoch 207/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 5.5153e-05 - accuracy: 1.0000\n",
      "Epoch 208/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 6.1923e-05 - accuracy: 1.0000\n",
      "Epoch 209/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 5.8951e-05 - accuracy: 1.0000\n",
      "Epoch 210/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 5.4844e-05 - accuracy: 1.0000\n",
      "Epoch 211/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 5.0770e-05 - accuracy: 1.0000\n",
      "Epoch 212/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 4.9853e-05 - accuracy: 1.0000\n",
      "Epoch 213/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 4.7608e-05 - accuracy: 1.0000\n",
      "Epoch 214/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 6.1348e-05 - accuracy: 1.0000\n",
      "Epoch 215/260\n",
      "38/38 [==============================] - 4s 100ms/step - loss: 6.0165e-05 - accuracy: 1.0000\n",
      "Epoch 216/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 4.8064e-05 - accuracy: 1.0000\n",
      "Epoch 217/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 5.1809e-05 - accuracy: 1.0000\n",
      "Epoch 218/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 4.5217e-05 - accuracy: 1.0000\n",
      "Epoch 219/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 4.5963e-05 - accuracy: 1.0000\n",
      "Epoch 220/260\n",
      "38/38 [==============================] - 4s 100ms/step - loss: 4.7818e-05 - accuracy: 1.0000\n",
      "Epoch 221/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 6.0179e-05 - accuracy: 1.0000\n",
      "Epoch 222/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 4.4739e-05 - accuracy: 1.0000\n",
      "Epoch 223/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 4.3411e-05 - accuracy: 1.0000\n",
      "Epoch 224/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 4.6885e-05 - accuracy: 1.0000\n",
      "Epoch 225/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 4.9335e-05 - accuracy: 1.0000\n",
      "Epoch 226/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 4.1133e-05 - accuracy: 1.0000\n",
      "Epoch 227/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 5.1582e-05 - accuracy: 1.0000\n",
      "Epoch 228/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 3.5332e-05 - accuracy: 1.0000\n",
      "Epoch 229/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 5.3617e-05 - accuracy: 1.0000\n",
      "Epoch 230/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 4.6357e-05 - accuracy: 1.0000\n",
      "Epoch 231/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 4.4424e-05 - accuracy: 1.0000\n",
      "Epoch 232/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 3.9068e-05 - accuracy: 1.0000\n",
      "Epoch 233/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 3.4249e-05 - accuracy: 1.0000\n",
      "Epoch 234/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 3.5040e-05 - accuracy: 1.0000\n",
      "Epoch 235/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 4.5229e-05 - accuracy: 1.0000\n",
      "Epoch 236/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 3.2627e-05 - accuracy: 1.0000\n",
      "Epoch 237/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 4.3778e-05 - accuracy: 1.0000\n",
      "Epoch 238/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 3.3539e-05 - accuracy: 1.0000\n",
      "Epoch 239/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 4.1454e-05 - accuracy: 1.0000\n",
      "Epoch 240/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 3.5060e-05 - accuracy: 1.0000\n",
      "Epoch 241/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 3.7944e-05 - accuracy: 1.0000\n",
      "Epoch 242/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 3.5458e-05 - accuracy: 1.0000\n",
      "Epoch 243/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 3.2235e-05 - accuracy: 1.0000\n",
      "Epoch 244/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 4.0044e-05 - accuracy: 1.0000\n",
      "Epoch 245/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 3.9590e-05 - accuracy: 1.0000\n",
      "Epoch 246/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 3.2926e-05 - accuracy: 1.0000\n",
      "Epoch 247/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 3.1778e-05 - accuracy: 1.0000\n",
      "Epoch 248/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 3.0089e-05 - accuracy: 1.0000\n",
      "Epoch 249/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 3.0561e-05 - accuracy: 1.0000\n",
      "Epoch 250/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 3.0331e-05 - accuracy: 1.0000\n",
      "Epoch 251/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 3.4747e-05 - accuracy: 1.0000\n",
      "Epoch 252/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 4.7922e-05 - accuracy: 1.0000\n",
      "Epoch 253/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 2.8307e-05 - accuracy: 1.0000\n",
      "Epoch 254/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 3.4162e-05 - accuracy: 1.0000\n",
      "Epoch 255/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 3.1331e-05 - accuracy: 1.0000\n",
      "Epoch 256/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 4.0038e-05 - accuracy: 1.0000 1s - loss: 2.8\n",
      "Epoch 257/260\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 5.1726e-05 - accuracy: 1.0000\n",
      "Epoch 258/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 3.9387e-05 - accuracy: 1.0000\n",
      "Epoch 259/260\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 2.7935e-05 - accuracy: 1.0000\n",
      "Epoch 260/260\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 2.8248e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f36d48fc090>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_model.fit(X_train, Y_train, epochs=260, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that completes, just use `.evaluate()` to evaluate against your test set. This function will print the value of the loss function and the performance metrics specified during the compilation of the model. In this case, the `binary_crossentropy` and the `accuracy` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0920 - accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09195336699485779, 0.9666666388511658]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy, right? But what if you need to build a model with shared layers, branches, or multiple inputs and outputs? This is where Sequential, with its beautifully simple yet limited functionality, won't be able to help you. \n",
    "\n",
    "Next up: Enter the Functional API, your slightly more complex, highly flexible friend.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - The Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second half of the assignment, where you'll use Keras' flexible [Functional API](https://www.tensorflow.org/guide/keras/functional) to build a ConvNet that can differentiate between 6 sign language digits. \n",
    "\n",
    "The Functional API can handle models with non-linear topology, shared layers, as well as layers with multiple inputs or outputs. Imagine that, where the Sequential API requires the model to move in a linear fashion through its layers, the Functional API allows much more flexibility. Where Sequential is a straight line, a Functional model is a graph, where the nodes of the layers can connect in many more ways than one. \n",
    "\n",
    "In the visual example below, the one possible direction of the movement Sequential model is shown in contrast to a skip connection, which is just one of the many ways a Functional model can be constructed. A skip connection, as you might have guessed, skips some layer in the network and feeds the output to a later layer in the network. Don't worry, you'll be spending more time with skip connections very soon! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/seq_vs_func.png\" style=\"width:350px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-1'></a>\n",
    "### 4.1 - Load the SIGNS Dataset\n",
    "\n",
    "As a reminder, the SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (signs)\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_signs_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/SIGNS.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "The next cell will show you an example of a labelled image in the dataset. Feel free to change the value of `index` below and re-run to see different examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of an image from the dataset\n",
    "index = 9\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Split the Data into Train/Test Sets\n",
    "\n",
    "In Course 2, you built a fully-connected network for this dataset. But since this is an image dataset, it is more natural to apply a ConvNet to it.\n",
    "\n",
    "To get started, let's examine the shapes of your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Forward Propagation\n",
    "\n",
    "In TensorFlow, there are built-in functions that implement the convolution steps for you. By now, you should be familiar with how TensorFlow builds computational graphs. In the [Functional API](https://www.tensorflow.org/guide/keras/functional), you create a graph of layers. This is what allows such great flexibility.\n",
    "\n",
    "However, the following model could also be defined using the Sequential API since the information flow is on a single line. But don't deviate. What we want you to learn is to use the functional API.\n",
    "\n",
    "Begin building your graph of layers by creating an input node that functions as a callable object:\n",
    "\n",
    "- **input_img = tf.keras.Input(shape=input_shape):** \n",
    "\n",
    "Then, create a new node in the graph of layers by calling a layer on the `input_img` object: \n",
    "\n",
    "- **tf.keras.layers.Conv2D(filters= ... , kernel_size= ... , padding='same')(input_img):** Read the full documentation on [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D).\n",
    "\n",
    "- **tf.keras.layers.MaxPool2D(pool_size=(f, f), strides=(s, s), padding='same'):** `MaxPool2D()` downsamples your input using a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window.  For max pooling, you usually operate on a single example at a time and a single channel at a time. Read the full documentation on [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D).\n",
    "\n",
    "- **tf.keras.layers.ReLU():** computes the elementwise ReLU of Z (which can be any shape). You can read the full documentation on [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU).\n",
    "\n",
    "- **tf.keras.layers.Flatten()**: given a tensor \"P\", this function takes each training (or test) example in the batch and flattens it into a 1D vector.  \n",
    "\n",
    "    * If a tensor P has the shape (batch_size,h,w,c), it returns a flattened tensor with shape (batch_size, k), where $k=h \\times w \\times c$.  \"k\" equals the product of all the dimension sizes other than the first dimension.\n",
    "    \n",
    "    * For example, given a tensor with dimensions [100, 2, 3, 4], it flattens the tensor to be of shape [100, 24], where 24 = 2 * 3 * 4.  You can read the full documentation on [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten).\n",
    "\n",
    "- **tf.keras.layers.Dense(units= ... , activation='softmax')(F):** given the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation on [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense).\n",
    "\n",
    "In the last function above (`tf.keras.layers.Dense()`), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters.\n",
    "\n",
    "Lastly, before creating the model, you'll need to define the output using the last of the function's compositions (in this example, a Dense layer): \n",
    "\n",
    "- **outputs = tf.keras.layers.Dense(units=6, activation='softmax')(F)**\n",
    "\n",
    "\n",
    "#### Window, kernel, filter, pool\n",
    "\n",
    "The words \"kernel\" and \"filter\" are used to refer to the same thing. The word \"filter\" accounts for the amount of \"kernels\" that will be used in a single convolution layer. \"Pool\" is the name of the operation that takes the max or average value of the kernels. \n",
    "\n",
    "This is why the parameter `pool_size` refers to `kernel_size`, and you use `(f,f)` to refer to the filter size. \n",
    "\n",
    "Pool size and kernel size refer to the same thing in different objects - They refer to the shape of the window where the operation takes place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - convolutional_model\n",
    "\n",
    "Implement the `convolutional_model` function below to build the following model: `CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE`. Use the functions above! \n",
    "\n",
    "Also, plug in the following parameters for all the steps:\n",
    "\n",
    " - [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D): Use 8 4 by 4 filters, stride 1, padding is \"SAME\"\n",
    " - [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU)\n",
    " - [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D): Use an 8 by 8 filter size and an 8 by 8 stride, padding is \"SAME\"\n",
    " - **Conv2D**: Use 16 2 by 2 filters, stride 1, padding is \"SAME\"\n",
    " - **ReLU**\n",
    " - **MaxPool2D**: Use a 4 by 4 filter size and a 4 by 4 stride, padding is \"SAME\"\n",
    " - [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) the previous output.\n",
    " - Fully-connected ([Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) layer: Apply a fully connected layer with 6 neurons and a softmax activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f58643806aa8380c96225fc8b4c5e7aa",
     "grade": false,
     "grade_id": "cell-dac51744a9e03f51",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: convolutional_model\n",
    "\n",
    "def convolutional_model(input_shape):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE\n",
    "    \n",
    "    Note that for simplicity and grading purposes, you'll hard-code some values\n",
    "    such as the stride and kernel (filter) sizes. \n",
    "    Normally, functions should take these values as function parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    input_img -- input dataset, of shape (input_shape)\n",
    "\n",
    "    Returns:\n",
    "    model -- TF Keras model (object containing the information for the entire training process) \n",
    "    \"\"\"\n",
    "\n",
    "    input_img = tf.keras.Input(shape=input_shape)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    ## CONV2D: 8 filters 4x4, stride of 1, padding 'SAME'\n",
    "    Z1 = tfl.Conv2D(8, 4, (1,1), 'same')(input_img)\n",
    "    ## RELU\n",
    "    A1 = tfl.ReLU()(Z1)\n",
    "    ## MAXPOOL: window 8x8, stride 8, padding 'SAME'\n",
    "    P1 = tfl.MaxPool2D((8,8), 8, 'same')(A1)\n",
    "    ## CONV2D: 16 filters 2x2, stride 1, padding 'SAME'\n",
    "    Z2 = tfl.Conv2D(16, 2, (1,1), 'same')(P1)\n",
    "    ## RELU\n",
    "    A2 = tfl.ReLU()(Z2)\n",
    "    ## MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    P2 = tfl.MaxPool2D((4,4), 4, 'same')(A2)\n",
    "    ## FLATTEN\n",
    "    F = tfl.Flatten()(P2)\n",
    "    ## Dense layer\n",
    "    ## 6 neurons in output layer. Hint: one of the arguments should be \"activation='softmax'\" \n",
    "    outputs = tfl.Dense(units=6,\n",
    "                        activation='softmax')(F)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    model = tf.keras.Model(inputs=input_img, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "483d626949930a0b0ef20997e7c6ba72",
     "grade": true,
     "grade_id": "cell-45d22e92042174c9",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 8)         392       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 16)          528       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 1,310\n",
      "Trainable params: 1,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\u001b[32mAll tests passed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "conv_model = convolutional_model((64, 64, 3))\n",
    "conv_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "conv_model.summary()\n",
    "    \n",
    "output = [['InputLayer', [(None, 64, 64, 3)], 0],\n",
    "        ['Conv2D', (None, 64, 64, 8), 392, 'same', 'linear', 'GlorotUniform'],\n",
    "        ['ReLU', (None, 64, 64, 8), 0],\n",
    "        ['MaxPooling2D', (None, 8, 8, 8), 0, (8, 8), (8, 8), 'same'],\n",
    "        ['Conv2D', (None, 8, 8, 16), 528, 'same', 'linear', 'GlorotUniform'],\n",
    "        ['ReLU', (None, 8, 8, 16), 0],\n",
    "        ['MaxPooling2D', (None, 2, 2, 16), 0, (4, 4), (4, 4), 'same'],\n",
    "        ['Flatten', (None, 64), 0],\n",
    "        ['Dense', (None, 6), 390, 'softmax']]\n",
    "    \n",
    "comparator(summary(conv_model), output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the Sequential and Functional APIs return a TF Keras model object. The only difference is how inputs are handled inside the object model! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-4'></a>\n",
    "### 4.4 - Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 1.8020 - accuracy: 0.1407 - val_loss: 1.7940 - val_accuracy: 0.1750\n",
      "Epoch 2/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 1.7925 - accuracy: 0.1935 - val_loss: 1.7923 - val_accuracy: 0.1500\n",
      "Epoch 3/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 1.7898 - accuracy: 0.1833 - val_loss: 1.7891 - val_accuracy: 0.1417\n",
      "Epoch 4/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 1.7875 - accuracy: 0.1667 - val_loss: 1.7867 - val_accuracy: 0.1333\n",
      "Epoch 5/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 1.7851 - accuracy: 0.1870 - val_loss: 1.7841 - val_accuracy: 0.1667\n",
      "Epoch 6/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.7826 - accuracy: 0.1963 - val_loss: 1.7816 - val_accuracy: 0.2333\n",
      "Epoch 7/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 1.7799 - accuracy: 0.2157 - val_loss: 1.7792 - val_accuracy: 0.2500\n",
      "Epoch 8/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 1.7768 - accuracy: 0.2176 - val_loss: 1.7761 - val_accuracy: 0.2333\n",
      "Epoch 9/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 1.7730 - accuracy: 0.2361 - val_loss: 1.7708 - val_accuracy: 0.3000\n",
      "Epoch 10/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.7667 - accuracy: 0.2741 - val_loss: 1.7642 - val_accuracy: 0.3000\n",
      "Epoch 11/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.7605 - accuracy: 0.2759 - val_loss: 1.7578 - val_accuracy: 0.3333\n",
      "Epoch 12/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 1.7531 - accuracy: 0.3157 - val_loss: 1.7505 - val_accuracy: 0.3667\n",
      "Epoch 13/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 1.7442 - accuracy: 0.3444 - val_loss: 1.7402 - val_accuracy: 0.3833\n",
      "Epoch 14/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 1.7337 - accuracy: 0.3713 - val_loss: 1.7274 - val_accuracy: 0.4083\n",
      "Epoch 15/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.7210 - accuracy: 0.3833 - val_loss: 1.7124 - val_accuracy: 0.4417\n",
      "Epoch 16/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 1.7063 - accuracy: 0.3991 - val_loss: 1.6961 - val_accuracy: 0.4500\n",
      "Epoch 17/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 1.6890 - accuracy: 0.4111 - val_loss: 1.6778 - val_accuracy: 0.4750\n",
      "Epoch 18/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 1.6698 - accuracy: 0.4148 - val_loss: 1.6568 - val_accuracy: 0.4917\n",
      "Epoch 19/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 1.6462 - accuracy: 0.4343 - val_loss: 1.6350 - val_accuracy: 0.4750\n",
      "Epoch 20/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 1.6188 - accuracy: 0.4500 - val_loss: 1.6082 - val_accuracy: 0.4500\n",
      "Epoch 21/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 1.5886 - accuracy: 0.4481 - val_loss: 1.5766 - val_accuracy: 0.4583\n",
      "Epoch 22/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.5558 - accuracy: 0.4537 - val_loss: 1.5427 - val_accuracy: 0.4667\n",
      "Epoch 23/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.5203 - accuracy: 0.4806 - val_loss: 1.5104 - val_accuracy: 0.4750\n",
      "Epoch 24/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.4851 - accuracy: 0.4852 - val_loss: 1.4763 - val_accuracy: 0.5083\n",
      "Epoch 25/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 1.4495 - accuracy: 0.4991 - val_loss: 1.4431 - val_accuracy: 0.5000\n",
      "Epoch 26/2000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 1.4144 - accuracy: 0.5102 - val_loss: 1.4117 - val_accuracy: 0.5250\n",
      "Epoch 27/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 1.3780 - accuracy: 0.5167 - val_loss: 1.3786 - val_accuracy: 0.5333\n",
      "Epoch 28/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.3432 - accuracy: 0.5389 - val_loss: 1.3487 - val_accuracy: 0.5167\n",
      "Epoch 29/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.3085 - accuracy: 0.5481 - val_loss: 1.3164 - val_accuracy: 0.5250\n",
      "Epoch 30/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.2730 - accuracy: 0.5611 - val_loss: 1.2859 - val_accuracy: 0.5333\n",
      "Epoch 31/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 1.2397 - accuracy: 0.5769 - val_loss: 1.2557 - val_accuracy: 0.5333\n",
      "Epoch 32/2000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 1.2076 - accuracy: 0.5907 - val_loss: 1.2277 - val_accuracy: 0.5333\n",
      "Epoch 33/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.1766 - accuracy: 0.5917 - val_loss: 1.2006 - val_accuracy: 0.5500\n",
      "Epoch 34/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.1465 - accuracy: 0.6019 - val_loss: 1.1745 - val_accuracy: 0.5667\n",
      "Epoch 35/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.1174 - accuracy: 0.6259 - val_loss: 1.1509 - val_accuracy: 0.5833\n",
      "Epoch 36/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.0906 - accuracy: 0.6389 - val_loss: 1.1262 - val_accuracy: 0.5833\n",
      "Epoch 37/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.0634 - accuracy: 0.6509 - val_loss: 1.1044 - val_accuracy: 0.5917\n",
      "Epoch 38/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 1.0383 - accuracy: 0.6574 - val_loss: 1.0801 - val_accuracy: 0.6167\n",
      "Epoch 39/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 1.0131 - accuracy: 0.6750 - val_loss: 1.0597 - val_accuracy: 0.6333\n",
      "Epoch 40/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.9898 - accuracy: 0.6806 - val_loss: 1.0417 - val_accuracy: 0.6167\n",
      "Epoch 41/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.9682 - accuracy: 0.6806 - val_loss: 1.0205 - val_accuracy: 0.6417\n",
      "Epoch 42/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.9470 - accuracy: 0.6833 - val_loss: 1.0041 - val_accuracy: 0.6417\n",
      "Epoch 43/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.9267 - accuracy: 0.6926 - val_loss: 0.9872 - val_accuracy: 0.6417\n",
      "Epoch 44/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.9069 - accuracy: 0.7083 - val_loss: 0.9709 - val_accuracy: 0.6333\n",
      "Epoch 45/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.8871 - accuracy: 0.7139 - val_loss: 0.9502 - val_accuracy: 0.6417\n",
      "Epoch 46/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.8691 - accuracy: 0.7241 - val_loss: 0.9318 - val_accuracy: 0.6500\n",
      "Epoch 47/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.8508 - accuracy: 0.7287 - val_loss: 0.9142 - val_accuracy: 0.6583\n",
      "Epoch 48/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.8334 - accuracy: 0.7343 - val_loss: 0.9006 - val_accuracy: 0.6667\n",
      "Epoch 49/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.8181 - accuracy: 0.7361 - val_loss: 0.8879 - val_accuracy: 0.6667\n",
      "Epoch 50/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.8038 - accuracy: 0.7417 - val_loss: 0.8750 - val_accuracy: 0.6667\n",
      "Epoch 51/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.7903 - accuracy: 0.7435 - val_loss: 0.8615 - val_accuracy: 0.7000\n",
      "Epoch 52/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.7758 - accuracy: 0.7500 - val_loss: 0.8500 - val_accuracy: 0.7000\n",
      "Epoch 53/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.7635 - accuracy: 0.7509 - val_loss: 0.8370 - val_accuracy: 0.7000\n",
      "Epoch 54/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.7506 - accuracy: 0.7556 - val_loss: 0.8248 - val_accuracy: 0.7250\n",
      "Epoch 55/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.7388 - accuracy: 0.7593 - val_loss: 0.8131 - val_accuracy: 0.7250\n",
      "Epoch 56/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.7275 - accuracy: 0.7630 - val_loss: 0.8010 - val_accuracy: 0.7333\n",
      "Epoch 57/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.7164 - accuracy: 0.7722 - val_loss: 0.7896 - val_accuracy: 0.7250\n",
      "Epoch 58/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.7060 - accuracy: 0.7778 - val_loss: 0.7786 - val_accuracy: 0.7250\n",
      "Epoch 59/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.6958 - accuracy: 0.7815 - val_loss: 0.7683 - val_accuracy: 0.7333\n",
      "Epoch 60/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.6861 - accuracy: 0.7861 - val_loss: 0.7584 - val_accuracy: 0.7333\n",
      "Epoch 61/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.6770 - accuracy: 0.7889 - val_loss: 0.7483 - val_accuracy: 0.7333\n",
      "Epoch 62/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.6680 - accuracy: 0.7926 - val_loss: 0.7395 - val_accuracy: 0.7333\n",
      "Epoch 63/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.6591 - accuracy: 0.7954 - val_loss: 0.7307 - val_accuracy: 0.7417\n",
      "Epoch 64/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.6513 - accuracy: 0.7981 - val_loss: 0.7219 - val_accuracy: 0.7333\n",
      "Epoch 65/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.6421 - accuracy: 0.8019 - val_loss: 0.7129 - val_accuracy: 0.7333\n",
      "Epoch 66/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.6354 - accuracy: 0.8000 - val_loss: 0.7055 - val_accuracy: 0.7333\n",
      "Epoch 67/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.6271 - accuracy: 0.8065 - val_loss: 0.6966 - val_accuracy: 0.7417\n",
      "Epoch 68/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.6202 - accuracy: 0.8074 - val_loss: 0.6893 - val_accuracy: 0.7333\n",
      "Epoch 69/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.6125 - accuracy: 0.8083 - val_loss: 0.6813 - val_accuracy: 0.7333\n",
      "Epoch 70/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.6061 - accuracy: 0.8074 - val_loss: 0.6744 - val_accuracy: 0.7500\n",
      "Epoch 71/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.5987 - accuracy: 0.8111 - val_loss: 0.6667 - val_accuracy: 0.7417\n",
      "Epoch 72/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.5926 - accuracy: 0.8120 - val_loss: 0.6603 - val_accuracy: 0.7500\n",
      "Epoch 73/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.5856 - accuracy: 0.8139 - val_loss: 0.6532 - val_accuracy: 0.7417\n",
      "Epoch 74/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.5800 - accuracy: 0.8139 - val_loss: 0.6469 - val_accuracy: 0.7583\n",
      "Epoch 75/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.5733 - accuracy: 0.8157 - val_loss: 0.6404 - val_accuracy: 0.7583\n",
      "Epoch 76/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.5683 - accuracy: 0.8194 - val_loss: 0.6348 - val_accuracy: 0.7583\n",
      "Epoch 77/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.5619 - accuracy: 0.8185 - val_loss: 0.6284 - val_accuracy: 0.7833\n",
      "Epoch 78/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.5561 - accuracy: 0.8204 - val_loss: 0.6223 - val_accuracy: 0.7833\n",
      "Epoch 79/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.5509 - accuracy: 0.8213 - val_loss: 0.6175 - val_accuracy: 0.7833\n",
      "Epoch 80/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.5447 - accuracy: 0.8250 - val_loss: 0.6114 - val_accuracy: 0.8000\n",
      "Epoch 81/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.5394 - accuracy: 0.8278 - val_loss: 0.6061 - val_accuracy: 0.8000\n",
      "Epoch 82/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.5348 - accuracy: 0.8306 - val_loss: 0.6014 - val_accuracy: 0.8000\n",
      "Epoch 83/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.5292 - accuracy: 0.8343 - val_loss: 0.5961 - val_accuracy: 0.8000\n",
      "Epoch 84/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.5247 - accuracy: 0.8343 - val_loss: 0.5914 - val_accuracy: 0.8000\n",
      "Epoch 85/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.5198 - accuracy: 0.8343 - val_loss: 0.5864 - val_accuracy: 0.8000\n",
      "Epoch 86/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.5148 - accuracy: 0.8370 - val_loss: 0.5821 - val_accuracy: 0.8000\n",
      "Epoch 87/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.5106 - accuracy: 0.8380 - val_loss: 0.5776 - val_accuracy: 0.8000\n",
      "Epoch 88/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.5057 - accuracy: 0.8389 - val_loss: 0.5730 - val_accuracy: 0.8083\n",
      "Epoch 89/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.5015 - accuracy: 0.8426 - val_loss: 0.5687 - val_accuracy: 0.8083\n",
      "Epoch 90/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.4972 - accuracy: 0.8435 - val_loss: 0.5642 - val_accuracy: 0.8083\n",
      "Epoch 91/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.4927 - accuracy: 0.8454 - val_loss: 0.5602 - val_accuracy: 0.8083\n",
      "Epoch 92/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.4890 - accuracy: 0.8481 - val_loss: 0.5570 - val_accuracy: 0.8083\n",
      "Epoch 93/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.4848 - accuracy: 0.8537 - val_loss: 0.5524 - val_accuracy: 0.8083\n",
      "Epoch 94/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.4809 - accuracy: 0.8509 - val_loss: 0.5489 - val_accuracy: 0.8083\n",
      "Epoch 95/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.4774 - accuracy: 0.8537 - val_loss: 0.5462 - val_accuracy: 0.8083\n",
      "Epoch 96/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.4733 - accuracy: 0.8574 - val_loss: 0.5421 - val_accuracy: 0.8083\n",
      "Epoch 97/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.4699 - accuracy: 0.8556 - val_loss: 0.5386 - val_accuracy: 0.8083\n",
      "Epoch 98/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.4667 - accuracy: 0.8565 - val_loss: 0.5367 - val_accuracy: 0.8083\n",
      "Epoch 99/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.4629 - accuracy: 0.8593 - val_loss: 0.5321 - val_accuracy: 0.8083\n",
      "Epoch 100/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.4596 - accuracy: 0.8583 - val_loss: 0.5289 - val_accuracy: 0.8083\n",
      "Epoch 101/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.4565 - accuracy: 0.8593 - val_loss: 0.5265 - val_accuracy: 0.8083\n",
      "Epoch 102/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.4528 - accuracy: 0.8630 - val_loss: 0.5224 - val_accuracy: 0.8083\n",
      "Epoch 103/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.4497 - accuracy: 0.8648 - val_loss: 0.5199 - val_accuracy: 0.8083\n",
      "Epoch 104/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.4467 - accuracy: 0.8648 - val_loss: 0.5175 - val_accuracy: 0.8083\n",
      "Epoch 105/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.4433 - accuracy: 0.8694 - val_loss: 0.5134 - val_accuracy: 0.8167\n",
      "Epoch 106/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.4399 - accuracy: 0.8676 - val_loss: 0.5108 - val_accuracy: 0.8167\n",
      "Epoch 107/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.4372 - accuracy: 0.8676 - val_loss: 0.5094 - val_accuracy: 0.8167\n",
      "Epoch 108/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.4341 - accuracy: 0.8685 - val_loss: 0.5052 - val_accuracy: 0.8250\n",
      "Epoch 109/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.4307 - accuracy: 0.8713 - val_loss: 0.5034 - val_accuracy: 0.8250\n",
      "Epoch 110/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.4280 - accuracy: 0.8694 - val_loss: 0.5017 - val_accuracy: 0.8250\n",
      "Epoch 111/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.4252 - accuracy: 0.8704 - val_loss: 0.4979 - val_accuracy: 0.8250\n",
      "Epoch 112/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.4217 - accuracy: 0.8713 - val_loss: 0.4958 - val_accuracy: 0.8250\n",
      "Epoch 113/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.4193 - accuracy: 0.8694 - val_loss: 0.4962 - val_accuracy: 0.8333\n",
      "Epoch 114/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.4163 - accuracy: 0.8731 - val_loss: 0.4919 - val_accuracy: 0.8250\n",
      "Epoch 115/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.4137 - accuracy: 0.8722 - val_loss: 0.4896 - val_accuracy: 0.8250\n",
      "Epoch 116/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.4109 - accuracy: 0.8722 - val_loss: 0.4891 - val_accuracy: 0.8333\n",
      "Epoch 117/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.4080 - accuracy: 0.8741 - val_loss: 0.4856 - val_accuracy: 0.8333\n",
      "Epoch 118/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.4057 - accuracy: 0.8769 - val_loss: 0.4843 - val_accuracy: 0.8333\n",
      "Epoch 119/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.4033 - accuracy: 0.8759 - val_loss: 0.4833 - val_accuracy: 0.8333\n",
      "Epoch 120/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.4005 - accuracy: 0.8787 - val_loss: 0.4802 - val_accuracy: 0.8333\n",
      "Epoch 121/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3982 - accuracy: 0.8787 - val_loss: 0.4794 - val_accuracy: 0.8333\n",
      "Epoch 122/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3958 - accuracy: 0.8806 - val_loss: 0.4782 - val_accuracy: 0.8417\n",
      "Epoch 123/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3934 - accuracy: 0.8815 - val_loss: 0.4765 - val_accuracy: 0.8333\n",
      "Epoch 124/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3909 - accuracy: 0.8824 - val_loss: 0.4754 - val_accuracy: 0.8333\n",
      "Epoch 125/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3889 - accuracy: 0.8833 - val_loss: 0.4740 - val_accuracy: 0.8417\n",
      "Epoch 126/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3864 - accuracy: 0.8833 - val_loss: 0.4715 - val_accuracy: 0.8333\n",
      "Epoch 127/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.3840 - accuracy: 0.8843 - val_loss: 0.4698 - val_accuracy: 0.8417\n",
      "Epoch 128/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3822 - accuracy: 0.8870 - val_loss: 0.4703 - val_accuracy: 0.8500\n",
      "Epoch 129/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3799 - accuracy: 0.8861 - val_loss: 0.4667 - val_accuracy: 0.8417\n",
      "Epoch 130/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3778 - accuracy: 0.8870 - val_loss: 0.4657 - val_accuracy: 0.8417\n",
      "Epoch 131/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.3756 - accuracy: 0.8861 - val_loss: 0.4646 - val_accuracy: 0.8417\n",
      "Epoch 132/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.3734 - accuracy: 0.8870 - val_loss: 0.4628 - val_accuracy: 0.8417\n",
      "Epoch 133/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3715 - accuracy: 0.8880 - val_loss: 0.4613 - val_accuracy: 0.8417\n",
      "Epoch 134/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3697 - accuracy: 0.8880 - val_loss: 0.4608 - val_accuracy: 0.8417\n",
      "Epoch 135/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3673 - accuracy: 0.8889 - val_loss: 0.4586 - val_accuracy: 0.8417\n",
      "Epoch 136/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3655 - accuracy: 0.8907 - val_loss: 0.4579 - val_accuracy: 0.8417\n",
      "Epoch 137/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.3637 - accuracy: 0.8907 - val_loss: 0.4571 - val_accuracy: 0.8500\n",
      "Epoch 138/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3615 - accuracy: 0.8889 - val_loss: 0.4545 - val_accuracy: 0.8500\n",
      "Epoch 139/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3592 - accuracy: 0.8926 - val_loss: 0.4535 - val_accuracy: 0.8500\n",
      "Epoch 140/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3574 - accuracy: 0.8926 - val_loss: 0.4529 - val_accuracy: 0.8583\n",
      "Epoch 141/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3553 - accuracy: 0.8926 - val_loss: 0.4513 - val_accuracy: 0.8583\n",
      "Epoch 142/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3536 - accuracy: 0.8926 - val_loss: 0.4504 - val_accuracy: 0.8667\n",
      "Epoch 143/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3520 - accuracy: 0.8944 - val_loss: 0.4499 - val_accuracy: 0.8667\n",
      "Epoch 144/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3498 - accuracy: 0.8944 - val_loss: 0.4480 - val_accuracy: 0.8667\n",
      "Epoch 145/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3480 - accuracy: 0.8963 - val_loss: 0.4473 - val_accuracy: 0.8667\n",
      "Epoch 146/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3463 - accuracy: 0.8972 - val_loss: 0.4468 - val_accuracy: 0.8667\n",
      "Epoch 147/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3444 - accuracy: 0.8981 - val_loss: 0.4453 - val_accuracy: 0.8667\n",
      "Epoch 148/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3425 - accuracy: 0.9009 - val_loss: 0.4447 - val_accuracy: 0.8667\n",
      "Epoch 149/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3408 - accuracy: 0.8991 - val_loss: 0.4440 - val_accuracy: 0.8667\n",
      "Epoch 150/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3390 - accuracy: 0.9028 - val_loss: 0.4426 - val_accuracy: 0.8667\n",
      "Epoch 151/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3375 - accuracy: 0.8991 - val_loss: 0.4424 - val_accuracy: 0.8667\n",
      "Epoch 152/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3357 - accuracy: 0.9019 - val_loss: 0.4406 - val_accuracy: 0.8667\n",
      "Epoch 153/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3342 - accuracy: 0.9037 - val_loss: 0.4399 - val_accuracy: 0.8667\n",
      "Epoch 154/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.3325 - accuracy: 0.9046 - val_loss: 0.4396 - val_accuracy: 0.8667\n",
      "Epoch 155/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3309 - accuracy: 0.9065 - val_loss: 0.4385 - val_accuracy: 0.8667\n",
      "Epoch 156/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3293 - accuracy: 0.9065 - val_loss: 0.4372 - val_accuracy: 0.8667\n",
      "Epoch 157/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3276 - accuracy: 0.9074 - val_loss: 0.4365 - val_accuracy: 0.8667\n",
      "Epoch 158/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3261 - accuracy: 0.9083 - val_loss: 0.4362 - val_accuracy: 0.8667\n",
      "Epoch 159/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3245 - accuracy: 0.9074 - val_loss: 0.4352 - val_accuracy: 0.8667\n",
      "Epoch 160/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.3230 - accuracy: 0.9074 - val_loss: 0.4349 - val_accuracy: 0.8667\n",
      "Epoch 161/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.3216 - accuracy: 0.9074 - val_loss: 0.4343 - val_accuracy: 0.8667\n",
      "Epoch 162/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.3201 - accuracy: 0.9093 - val_loss: 0.4337 - val_accuracy: 0.8667\n",
      "Epoch 163/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3186 - accuracy: 0.9093 - val_loss: 0.4326 - val_accuracy: 0.8667\n",
      "Epoch 164/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3172 - accuracy: 0.9102 - val_loss: 0.4317 - val_accuracy: 0.8667\n",
      "Epoch 165/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3156 - accuracy: 0.9102 - val_loss: 0.4306 - val_accuracy: 0.8667\n",
      "Epoch 166/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3143 - accuracy: 0.9120 - val_loss: 0.4301 - val_accuracy: 0.8667\n",
      "Epoch 167/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3127 - accuracy: 0.9102 - val_loss: 0.4291 - val_accuracy: 0.8667\n",
      "Epoch 168/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3115 - accuracy: 0.9111 - val_loss: 0.4291 - val_accuracy: 0.8667\n",
      "Epoch 169/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3101 - accuracy: 0.9120 - val_loss: 0.4274 - val_accuracy: 0.8667\n",
      "Epoch 170/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3087 - accuracy: 0.9130 - val_loss: 0.4277 - val_accuracy: 0.8667\n",
      "Epoch 171/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3072 - accuracy: 0.9130 - val_loss: 0.4261 - val_accuracy: 0.8667\n",
      "Epoch 172/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3059 - accuracy: 0.9139 - val_loss: 0.4264 - val_accuracy: 0.8667\n",
      "Epoch 173/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3047 - accuracy: 0.9130 - val_loss: 0.4253 - val_accuracy: 0.8667\n",
      "Epoch 174/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.3034 - accuracy: 0.9139 - val_loss: 0.4249 - val_accuracy: 0.8667\n",
      "Epoch 175/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3020 - accuracy: 0.9130 - val_loss: 0.4240 - val_accuracy: 0.8667\n",
      "Epoch 176/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3008 - accuracy: 0.9139 - val_loss: 0.4238 - val_accuracy: 0.8667\n",
      "Epoch 177/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2994 - accuracy: 0.9139 - val_loss: 0.4229 - val_accuracy: 0.8667\n",
      "Epoch 178/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2983 - accuracy: 0.9139 - val_loss: 0.4224 - val_accuracy: 0.8667\n",
      "Epoch 179/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2969 - accuracy: 0.9148 - val_loss: 0.4220 - val_accuracy: 0.8667\n",
      "Epoch 180/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2956 - accuracy: 0.9157 - val_loss: 0.4211 - val_accuracy: 0.8667\n",
      "Epoch 181/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2944 - accuracy: 0.9157 - val_loss: 0.4210 - val_accuracy: 0.8667\n",
      "Epoch 182/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2931 - accuracy: 0.9157 - val_loss: 0.4206 - val_accuracy: 0.8667\n",
      "Epoch 183/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2921 - accuracy: 0.9157 - val_loss: 0.4200 - val_accuracy: 0.8667\n",
      "Epoch 184/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2905 - accuracy: 0.9167 - val_loss: 0.4190 - val_accuracy: 0.8667\n",
      "Epoch 185/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2896 - accuracy: 0.9157 - val_loss: 0.4197 - val_accuracy: 0.8667\n",
      "Epoch 186/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2882 - accuracy: 0.9176 - val_loss: 0.4182 - val_accuracy: 0.8667\n",
      "Epoch 187/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2872 - accuracy: 0.9157 - val_loss: 0.4184 - val_accuracy: 0.8667\n",
      "Epoch 188/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2859 - accuracy: 0.9157 - val_loss: 0.4171 - val_accuracy: 0.8667\n",
      "Epoch 189/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2847 - accuracy: 0.9176 - val_loss: 0.4176 - val_accuracy: 0.8667\n",
      "Epoch 190/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2836 - accuracy: 0.9167 - val_loss: 0.4167 - val_accuracy: 0.8667\n",
      "Epoch 191/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2824 - accuracy: 0.9167 - val_loss: 0.4159 - val_accuracy: 0.8667\n",
      "Epoch 192/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2813 - accuracy: 0.9157 - val_loss: 0.4150 - val_accuracy: 0.8667\n",
      "Epoch 193/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2801 - accuracy: 0.9157 - val_loss: 0.4146 - val_accuracy: 0.8667\n",
      "Epoch 194/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2791 - accuracy: 0.9157 - val_loss: 0.4141 - val_accuracy: 0.8667\n",
      "Epoch 195/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2779 - accuracy: 0.9157 - val_loss: 0.4135 - val_accuracy: 0.8667\n",
      "Epoch 196/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2768 - accuracy: 0.9167 - val_loss: 0.4128 - val_accuracy: 0.8667\n",
      "Epoch 197/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2757 - accuracy: 0.9176 - val_loss: 0.4125 - val_accuracy: 0.8667\n",
      "Epoch 198/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2746 - accuracy: 0.9176 - val_loss: 0.4116 - val_accuracy: 0.8667\n",
      "Epoch 199/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2736 - accuracy: 0.9176 - val_loss: 0.4110 - val_accuracy: 0.8667\n",
      "Epoch 200/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2726 - accuracy: 0.9176 - val_loss: 0.4108 - val_accuracy: 0.8667\n",
      "Epoch 201/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2715 - accuracy: 0.9176 - val_loss: 0.4101 - val_accuracy: 0.8667\n",
      "Epoch 202/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2704 - accuracy: 0.9185 - val_loss: 0.4099 - val_accuracy: 0.8667\n",
      "Epoch 203/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2694 - accuracy: 0.9176 - val_loss: 0.4095 - val_accuracy: 0.8667\n",
      "Epoch 204/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2685 - accuracy: 0.9185 - val_loss: 0.4088 - val_accuracy: 0.8667\n",
      "Epoch 205/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2675 - accuracy: 0.9204 - val_loss: 0.4088 - val_accuracy: 0.8667\n",
      "Epoch 206/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2665 - accuracy: 0.9204 - val_loss: 0.4076 - val_accuracy: 0.8667\n",
      "Epoch 207/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2656 - accuracy: 0.9222 - val_loss: 0.4082 - val_accuracy: 0.8667\n",
      "Epoch 208/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2647 - accuracy: 0.9204 - val_loss: 0.4074 - val_accuracy: 0.8667\n",
      "Epoch 209/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2637 - accuracy: 0.9222 - val_loss: 0.4070 - val_accuracy: 0.8667\n",
      "Epoch 210/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2629 - accuracy: 0.9222 - val_loss: 0.4067 - val_accuracy: 0.8667\n",
      "Epoch 211/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2619 - accuracy: 0.9222 - val_loss: 0.4060 - val_accuracy: 0.8667\n",
      "Epoch 212/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2609 - accuracy: 0.9231 - val_loss: 0.4059 - val_accuracy: 0.8667\n",
      "Epoch 213/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2600 - accuracy: 0.9231 - val_loss: 0.4056 - val_accuracy: 0.8667\n",
      "Epoch 214/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2591 - accuracy: 0.9241 - val_loss: 0.4052 - val_accuracy: 0.8667\n",
      "Epoch 215/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2583 - accuracy: 0.9222 - val_loss: 0.4045 - val_accuracy: 0.8667\n",
      "Epoch 216/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2575 - accuracy: 0.9241 - val_loss: 0.4041 - val_accuracy: 0.8667\n",
      "Epoch 217/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2565 - accuracy: 0.9259 - val_loss: 0.4034 - val_accuracy: 0.8667\n",
      "Epoch 218/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2558 - accuracy: 0.9241 - val_loss: 0.4035 - val_accuracy: 0.8667\n",
      "Epoch 219/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2548 - accuracy: 0.9250 - val_loss: 0.4028 - val_accuracy: 0.8667\n",
      "Epoch 220/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2540 - accuracy: 0.9250 - val_loss: 0.4025 - val_accuracy: 0.8667\n",
      "Epoch 221/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2531 - accuracy: 0.9250 - val_loss: 0.4016 - val_accuracy: 0.8667\n",
      "Epoch 222/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2523 - accuracy: 0.9241 - val_loss: 0.4015 - val_accuracy: 0.8667\n",
      "Epoch 223/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2514 - accuracy: 0.9241 - val_loss: 0.4007 - val_accuracy: 0.8667\n",
      "Epoch 224/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2505 - accuracy: 0.9241 - val_loss: 0.4005 - val_accuracy: 0.8750\n",
      "Epoch 225/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2496 - accuracy: 0.9259 - val_loss: 0.3997 - val_accuracy: 0.8667\n",
      "Epoch 226/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2487 - accuracy: 0.9250 - val_loss: 0.3993 - val_accuracy: 0.8667\n",
      "Epoch 227/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2477 - accuracy: 0.9269 - val_loss: 0.3992 - val_accuracy: 0.8750\n",
      "Epoch 228/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2468 - accuracy: 0.9259 - val_loss: 0.3986 - val_accuracy: 0.8750\n",
      "Epoch 229/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2461 - accuracy: 0.9269 - val_loss: 0.3978 - val_accuracy: 0.8750\n",
      "Epoch 230/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2450 - accuracy: 0.9278 - val_loss: 0.3968 - val_accuracy: 0.8750\n",
      "Epoch 231/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2439 - accuracy: 0.9278 - val_loss: 0.3967 - val_accuracy: 0.8750\n",
      "Epoch 232/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2429 - accuracy: 0.9278 - val_loss: 0.3965 - val_accuracy: 0.8750\n",
      "Epoch 233/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2422 - accuracy: 0.9278 - val_loss: 0.3964 - val_accuracy: 0.8750\n",
      "Epoch 234/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2413 - accuracy: 0.9278 - val_loss: 0.3952 - val_accuracy: 0.8667\n",
      "Epoch 235/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2404 - accuracy: 0.9296 - val_loss: 0.3950 - val_accuracy: 0.8667\n",
      "Epoch 236/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2395 - accuracy: 0.9296 - val_loss: 0.3940 - val_accuracy: 0.8583\n",
      "Epoch 237/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2384 - accuracy: 0.9306 - val_loss: 0.3936 - val_accuracy: 0.8583\n",
      "Epoch 238/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2375 - accuracy: 0.9296 - val_loss: 0.3929 - val_accuracy: 0.8583\n",
      "Epoch 239/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2366 - accuracy: 0.9306 - val_loss: 0.3928 - val_accuracy: 0.8583\n",
      "Epoch 240/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2357 - accuracy: 0.9296 - val_loss: 0.3917 - val_accuracy: 0.8583\n",
      "Epoch 241/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2347 - accuracy: 0.9324 - val_loss: 0.3914 - val_accuracy: 0.8583\n",
      "Epoch 242/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2339 - accuracy: 0.9324 - val_loss: 0.3909 - val_accuracy: 0.8583\n",
      "Epoch 243/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2331 - accuracy: 0.9315 - val_loss: 0.3902 - val_accuracy: 0.8583\n",
      "Epoch 244/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2321 - accuracy: 0.9324 - val_loss: 0.3898 - val_accuracy: 0.8667\n",
      "Epoch 245/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2312 - accuracy: 0.9324 - val_loss: 0.3894 - val_accuracy: 0.8667\n",
      "Epoch 246/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2303 - accuracy: 0.9333 - val_loss: 0.3885 - val_accuracy: 0.8667\n",
      "Epoch 247/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2293 - accuracy: 0.9343 - val_loss: 0.3878 - val_accuracy: 0.8667\n",
      "Epoch 248/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2286 - accuracy: 0.9343 - val_loss: 0.3875 - val_accuracy: 0.8667\n",
      "Epoch 249/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2277 - accuracy: 0.9343 - val_loss: 0.3863 - val_accuracy: 0.8667\n",
      "Epoch 250/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2268 - accuracy: 0.9343 - val_loss: 0.3859 - val_accuracy: 0.8667\n",
      "Epoch 251/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2259 - accuracy: 0.9343 - val_loss: 0.3855 - val_accuracy: 0.8667\n",
      "Epoch 252/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2249 - accuracy: 0.9343 - val_loss: 0.3849 - val_accuracy: 0.8667\n",
      "Epoch 253/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2241 - accuracy: 0.9352 - val_loss: 0.3844 - val_accuracy: 0.8667\n",
      "Epoch 254/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2232 - accuracy: 0.9361 - val_loss: 0.3837 - val_accuracy: 0.8667\n",
      "Epoch 255/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2222 - accuracy: 0.9361 - val_loss: 0.3833 - val_accuracy: 0.8667\n",
      "Epoch 256/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2213 - accuracy: 0.9361 - val_loss: 0.3826 - val_accuracy: 0.8667\n",
      "Epoch 257/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2203 - accuracy: 0.9361 - val_loss: 0.3822 - val_accuracy: 0.8667\n",
      "Epoch 258/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2194 - accuracy: 0.9361 - val_loss: 0.3816 - val_accuracy: 0.8667\n",
      "Epoch 259/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2184 - accuracy: 0.9361 - val_loss: 0.3813 - val_accuracy: 0.8667\n",
      "Epoch 260/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2175 - accuracy: 0.9352 - val_loss: 0.3801 - val_accuracy: 0.8667\n",
      "Epoch 261/2000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.2165 - accuracy: 0.9352 - val_loss: 0.3795 - val_accuracy: 0.8667\n",
      "Epoch 262/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2154 - accuracy: 0.9343 - val_loss: 0.3791 - val_accuracy: 0.8667\n",
      "Epoch 263/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2145 - accuracy: 0.9343 - val_loss: 0.3787 - val_accuracy: 0.8667\n",
      "Epoch 264/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2137 - accuracy: 0.9343 - val_loss: 0.3781 - val_accuracy: 0.8667\n",
      "Epoch 265/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2129 - accuracy: 0.9343 - val_loss: 0.3779 - val_accuracy: 0.8667\n",
      "Epoch 266/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2120 - accuracy: 0.9333 - val_loss: 0.3771 - val_accuracy: 0.8667\n",
      "Epoch 267/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2110 - accuracy: 0.9352 - val_loss: 0.3767 - val_accuracy: 0.8667\n",
      "Epoch 268/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2101 - accuracy: 0.9352 - val_loss: 0.3765 - val_accuracy: 0.8667\n",
      "Epoch 269/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2093 - accuracy: 0.9370 - val_loss: 0.3763 - val_accuracy: 0.8667\n",
      "Epoch 270/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2083 - accuracy: 0.9370 - val_loss: 0.3762 - val_accuracy: 0.8667\n",
      "Epoch 271/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2074 - accuracy: 0.9389 - val_loss: 0.3757 - val_accuracy: 0.8667\n",
      "Epoch 272/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2066 - accuracy: 0.9389 - val_loss: 0.3758 - val_accuracy: 0.8667\n",
      "Epoch 273/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2058 - accuracy: 0.9389 - val_loss: 0.3753 - val_accuracy: 0.8667\n",
      "Epoch 274/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2050 - accuracy: 0.9389 - val_loss: 0.3755 - val_accuracy: 0.8667\n",
      "Epoch 275/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2043 - accuracy: 0.9389 - val_loss: 0.3751 - val_accuracy: 0.8667\n",
      "Epoch 276/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2035 - accuracy: 0.9389 - val_loss: 0.3751 - val_accuracy: 0.8667\n",
      "Epoch 277/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2027 - accuracy: 0.9389 - val_loss: 0.3752 - val_accuracy: 0.8583\n",
      "Epoch 278/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2021 - accuracy: 0.9380 - val_loss: 0.3752 - val_accuracy: 0.8667\n",
      "Epoch 279/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2012 - accuracy: 0.9398 - val_loss: 0.3752 - val_accuracy: 0.8583\n",
      "Epoch 280/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2005 - accuracy: 0.9398 - val_loss: 0.3749 - val_accuracy: 0.8583\n",
      "Epoch 281/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1997 - accuracy: 0.9398 - val_loss: 0.3748 - val_accuracy: 0.8583\n",
      "Epoch 282/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1990 - accuracy: 0.9398 - val_loss: 0.3750 - val_accuracy: 0.8583\n",
      "Epoch 283/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1983 - accuracy: 0.9398 - val_loss: 0.3750 - val_accuracy: 0.8583\n",
      "Epoch 284/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1975 - accuracy: 0.9407 - val_loss: 0.3748 - val_accuracy: 0.8667\n",
      "Epoch 285/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1968 - accuracy: 0.9407 - val_loss: 0.3750 - val_accuracy: 0.8667\n",
      "Epoch 286/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1962 - accuracy: 0.9417 - val_loss: 0.3749 - val_accuracy: 0.8667\n",
      "Epoch 287/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1954 - accuracy: 0.9417 - val_loss: 0.3750 - val_accuracy: 0.8667\n",
      "Epoch 288/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1947 - accuracy: 0.9407 - val_loss: 0.3749 - val_accuracy: 0.8667\n",
      "Epoch 289/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1941 - accuracy: 0.9417 - val_loss: 0.3748 - val_accuracy: 0.8667\n",
      "Epoch 290/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1934 - accuracy: 0.9398 - val_loss: 0.3748 - val_accuracy: 0.8667\n",
      "Epoch 291/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1928 - accuracy: 0.9407 - val_loss: 0.3746 - val_accuracy: 0.8667\n",
      "Epoch 292/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1921 - accuracy: 0.9398 - val_loss: 0.3743 - val_accuracy: 0.8667\n",
      "Epoch 293/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1915 - accuracy: 0.9398 - val_loss: 0.3743 - val_accuracy: 0.8667\n",
      "Epoch 294/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1909 - accuracy: 0.9398 - val_loss: 0.3743 - val_accuracy: 0.8667\n",
      "Epoch 295/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1903 - accuracy: 0.9398 - val_loss: 0.3743 - val_accuracy: 0.8667\n",
      "Epoch 296/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1896 - accuracy: 0.9398 - val_loss: 0.3740 - val_accuracy: 0.8667\n",
      "Epoch 297/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1890 - accuracy: 0.9398 - val_loss: 0.3737 - val_accuracy: 0.8667\n",
      "Epoch 298/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1883 - accuracy: 0.9398 - val_loss: 0.3737 - val_accuracy: 0.8667\n",
      "Epoch 299/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1877 - accuracy: 0.9407 - val_loss: 0.3734 - val_accuracy: 0.8667\n",
      "Epoch 300/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1871 - accuracy: 0.9398 - val_loss: 0.3730 - val_accuracy: 0.8667\n",
      "Epoch 301/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1865 - accuracy: 0.9398 - val_loss: 0.3732 - val_accuracy: 0.8667\n",
      "Epoch 302/2000\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.1860 - accuracy: 0.9398 - val_loss: 0.3726 - val_accuracy: 0.8667\n",
      "Epoch 303/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1853 - accuracy: 0.9398 - val_loss: 0.3725 - val_accuracy: 0.8667\n",
      "Epoch 304/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1847 - accuracy: 0.9407 - val_loss: 0.3726 - val_accuracy: 0.8667\n",
      "Epoch 305/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1842 - accuracy: 0.9407 - val_loss: 0.3725 - val_accuracy: 0.8667\n",
      "Epoch 306/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1836 - accuracy: 0.9417 - val_loss: 0.3726 - val_accuracy: 0.8667\n",
      "Epoch 307/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1831 - accuracy: 0.9417 - val_loss: 0.3724 - val_accuracy: 0.8667\n",
      "Epoch 308/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1825 - accuracy: 0.9426 - val_loss: 0.3718 - val_accuracy: 0.8667\n",
      "Epoch 309/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1820 - accuracy: 0.9407 - val_loss: 0.3716 - val_accuracy: 0.8667\n",
      "Epoch 310/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1814 - accuracy: 0.9417 - val_loss: 0.3715 - val_accuracy: 0.8667\n",
      "Epoch 311/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1808 - accuracy: 0.9417 - val_loss: 0.3715 - val_accuracy: 0.8667\n",
      "Epoch 312/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1804 - accuracy: 0.9407 - val_loss: 0.3716 - val_accuracy: 0.8667\n",
      "Epoch 313/2000\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.1800 - accuracy: 0.9407 - val_loss: 0.3715 - val_accuracy: 0.8667\n",
      "Epoch 314/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1794 - accuracy: 0.9417 - val_loss: 0.3715 - val_accuracy: 0.8667\n",
      "Epoch 315/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1789 - accuracy: 0.9417 - val_loss: 0.3716 - val_accuracy: 0.8667\n",
      "Epoch 316/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1786 - accuracy: 0.9407 - val_loss: 0.3711 - val_accuracy: 0.8667\n",
      "Epoch 317/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1781 - accuracy: 0.9407 - val_loss: 0.3713 - val_accuracy: 0.8667\n",
      "Epoch 318/2000\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 0.1776 - accuracy: 0.9398 - val_loss: 0.3711 - val_accuracy: 0.8667\n",
      "Epoch 319/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1772 - accuracy: 0.9407 - val_loss: 0.3712 - val_accuracy: 0.8667\n",
      "Epoch 320/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1767 - accuracy: 0.9407 - val_loss: 0.3708 - val_accuracy: 0.8667\n",
      "Epoch 321/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1762 - accuracy: 0.9407 - val_loss: 0.3707 - val_accuracy: 0.8667\n",
      "Epoch 322/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1758 - accuracy: 0.9407 - val_loss: 0.3705 - val_accuracy: 0.8667\n",
      "Epoch 323/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1754 - accuracy: 0.9407 - val_loss: 0.3701 - val_accuracy: 0.8667\n",
      "Epoch 324/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1748 - accuracy: 0.9407 - val_loss: 0.3695 - val_accuracy: 0.8667\n",
      "Epoch 325/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1744 - accuracy: 0.9407 - val_loss: 0.3692 - val_accuracy: 0.8667\n",
      "Epoch 326/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1739 - accuracy: 0.9417 - val_loss: 0.3691 - val_accuracy: 0.8667\n",
      "Epoch 327/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1734 - accuracy: 0.9407 - val_loss: 0.3688 - val_accuracy: 0.8667\n",
      "Epoch 328/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1732 - accuracy: 0.9426 - val_loss: 0.3687 - val_accuracy: 0.8667\n",
      "Epoch 329/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1727 - accuracy: 0.9417 - val_loss: 0.3683 - val_accuracy: 0.8667\n",
      "Epoch 330/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1722 - accuracy: 0.9426 - val_loss: 0.3684 - val_accuracy: 0.8667\n",
      "Epoch 331/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1717 - accuracy: 0.9426 - val_loss: 0.3679 - val_accuracy: 0.8667\n",
      "Epoch 332/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1714 - accuracy: 0.9417 - val_loss: 0.3675 - val_accuracy: 0.8667\n",
      "Epoch 333/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1711 - accuracy: 0.9426 - val_loss: 0.3672 - val_accuracy: 0.8583\n",
      "Epoch 334/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1706 - accuracy: 0.9426 - val_loss: 0.3671 - val_accuracy: 0.8667\n",
      "Epoch 335/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1703 - accuracy: 0.9426 - val_loss: 0.3666 - val_accuracy: 0.8583\n",
      "Epoch 336/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1698 - accuracy: 0.9426 - val_loss: 0.3663 - val_accuracy: 0.8667\n",
      "Epoch 337/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1695 - accuracy: 0.9417 - val_loss: 0.3658 - val_accuracy: 0.8583\n",
      "Epoch 338/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1691 - accuracy: 0.9426 - val_loss: 0.3655 - val_accuracy: 0.8667\n",
      "Epoch 339/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1687 - accuracy: 0.9417 - val_loss: 0.3649 - val_accuracy: 0.8667\n",
      "Epoch 340/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1683 - accuracy: 0.9435 - val_loss: 0.3646 - val_accuracy: 0.8583\n",
      "Epoch 341/2000\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.1680 - accuracy: 0.9444 - val_loss: 0.3640 - val_accuracy: 0.8583\n",
      "Epoch 342/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1676 - accuracy: 0.9454 - val_loss: 0.3640 - val_accuracy: 0.8583\n",
      "Epoch 343/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1673 - accuracy: 0.9444 - val_loss: 0.3632 - val_accuracy: 0.8583\n",
      "Epoch 344/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1669 - accuracy: 0.9444 - val_loss: 0.3631 - val_accuracy: 0.8583\n",
      "Epoch 345/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1666 - accuracy: 0.9444 - val_loss: 0.3622 - val_accuracy: 0.8583\n",
      "Epoch 346/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1662 - accuracy: 0.9444 - val_loss: 0.3614 - val_accuracy: 0.8583\n",
      "Epoch 347/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1658 - accuracy: 0.9444 - val_loss: 0.3608 - val_accuracy: 0.8583\n",
      "Epoch 348/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1655 - accuracy: 0.9444 - val_loss: 0.3603 - val_accuracy: 0.8583\n",
      "Epoch 349/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1651 - accuracy: 0.9444 - val_loss: 0.3598 - val_accuracy: 0.8583\n",
      "Epoch 350/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1647 - accuracy: 0.9463 - val_loss: 0.3580 - val_accuracy: 0.8583\n",
      "Epoch 351/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1643 - accuracy: 0.9444 - val_loss: 0.3581 - val_accuracy: 0.8583\n",
      "Epoch 352/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1639 - accuracy: 0.9454 - val_loss: 0.3570 - val_accuracy: 0.8583\n",
      "Epoch 353/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1635 - accuracy: 0.9463 - val_loss: 0.3563 - val_accuracy: 0.8583\n",
      "Epoch 354/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1632 - accuracy: 0.9463 - val_loss: 0.3554 - val_accuracy: 0.8583\n",
      "Epoch 355/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1628 - accuracy: 0.9463 - val_loss: 0.3547 - val_accuracy: 0.8583\n",
      "Epoch 356/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1625 - accuracy: 0.9463 - val_loss: 0.3538 - val_accuracy: 0.8583\n",
      "Epoch 357/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1622 - accuracy: 0.9463 - val_loss: 0.3530 - val_accuracy: 0.8583\n",
      "Epoch 358/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1617 - accuracy: 0.9463 - val_loss: 0.3519 - val_accuracy: 0.8583\n",
      "Epoch 359/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1615 - accuracy: 0.9463 - val_loss: 0.3510 - val_accuracy: 0.8583\n",
      "Epoch 360/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1610 - accuracy: 0.9463 - val_loss: 0.3501 - val_accuracy: 0.8583\n",
      "Epoch 361/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1607 - accuracy: 0.9463 - val_loss: 0.3491 - val_accuracy: 0.8583\n",
      "Epoch 362/2000\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 0.1603 - accuracy: 0.9463 - val_loss: 0.3480 - val_accuracy: 0.8583\n",
      "Epoch 363/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1599 - accuracy: 0.9472 - val_loss: 0.3467 - val_accuracy: 0.8667\n",
      "Epoch 364/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1596 - accuracy: 0.9472 - val_loss: 0.3464 - val_accuracy: 0.8583\n",
      "Epoch 365/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1592 - accuracy: 0.9472 - val_loss: 0.3449 - val_accuracy: 0.8667\n",
      "Epoch 366/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1587 - accuracy: 0.9481 - val_loss: 0.3438 - val_accuracy: 0.8667\n",
      "Epoch 367/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1584 - accuracy: 0.9481 - val_loss: 0.3425 - val_accuracy: 0.8667\n",
      "Epoch 368/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1580 - accuracy: 0.9500 - val_loss: 0.3417 - val_accuracy: 0.8667\n",
      "Epoch 369/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1575 - accuracy: 0.9491 - val_loss: 0.3405 - val_accuracy: 0.8750\n",
      "Epoch 370/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1571 - accuracy: 0.9500 - val_loss: 0.3389 - val_accuracy: 0.8750\n",
      "Epoch 371/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1567 - accuracy: 0.9500 - val_loss: 0.3382 - val_accuracy: 0.8750\n",
      "Epoch 372/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1563 - accuracy: 0.9500 - val_loss: 0.3370 - val_accuracy: 0.8833\n",
      "Epoch 373/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1560 - accuracy: 0.9500 - val_loss: 0.3357 - val_accuracy: 0.8917\n",
      "Epoch 374/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1554 - accuracy: 0.9509 - val_loss: 0.3347 - val_accuracy: 0.8917\n",
      "Epoch 375/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1551 - accuracy: 0.9500 - val_loss: 0.3337 - val_accuracy: 0.8917\n",
      "Epoch 376/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1546 - accuracy: 0.9509 - val_loss: 0.3330 - val_accuracy: 0.8917\n",
      "Epoch 377/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1542 - accuracy: 0.9509 - val_loss: 0.3318 - val_accuracy: 0.9000\n",
      "Epoch 378/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1537 - accuracy: 0.9519 - val_loss: 0.3305 - val_accuracy: 0.9000\n",
      "Epoch 379/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1532 - accuracy: 0.9519 - val_loss: 0.3297 - val_accuracy: 0.9000\n",
      "Epoch 380/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1528 - accuracy: 0.9519 - val_loss: 0.3284 - val_accuracy: 0.9000\n",
      "Epoch 381/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1523 - accuracy: 0.9519 - val_loss: 0.3274 - val_accuracy: 0.9000\n",
      "Epoch 382/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1519 - accuracy: 0.9519 - val_loss: 0.3265 - val_accuracy: 0.9000\n",
      "Epoch 383/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1514 - accuracy: 0.9509 - val_loss: 0.3257 - val_accuracy: 0.9000\n",
      "Epoch 384/2000\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.1510 - accuracy: 0.9519 - val_loss: 0.3247 - val_accuracy: 0.9000\n",
      "Epoch 385/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1507 - accuracy: 0.9519 - val_loss: 0.3233 - val_accuracy: 0.9000\n",
      "Epoch 386/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1502 - accuracy: 0.9509 - val_loss: 0.3228 - val_accuracy: 0.9000\n",
      "Epoch 387/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1498 - accuracy: 0.9519 - val_loss: 0.3218 - val_accuracy: 0.9000\n",
      "Epoch 388/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1494 - accuracy: 0.9519 - val_loss: 0.3209 - val_accuracy: 0.9000\n",
      "Epoch 389/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1489 - accuracy: 0.9528 - val_loss: 0.3200 - val_accuracy: 0.9000\n",
      "Epoch 390/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1484 - accuracy: 0.9537 - val_loss: 0.3192 - val_accuracy: 0.9000\n",
      "Epoch 391/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1479 - accuracy: 0.9537 - val_loss: 0.3180 - val_accuracy: 0.9000\n",
      "Epoch 392/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1474 - accuracy: 0.9537 - val_loss: 0.3173 - val_accuracy: 0.9000\n",
      "Epoch 393/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1470 - accuracy: 0.9537 - val_loss: 0.3171 - val_accuracy: 0.9000\n",
      "Epoch 394/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1467 - accuracy: 0.9556 - val_loss: 0.3156 - val_accuracy: 0.9000\n",
      "Epoch 395/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1461 - accuracy: 0.9556 - val_loss: 0.3158 - val_accuracy: 0.9000\n",
      "Epoch 396/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1457 - accuracy: 0.9565 - val_loss: 0.3143 - val_accuracy: 0.9000\n",
      "Epoch 397/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1451 - accuracy: 0.9565 - val_loss: 0.3141 - val_accuracy: 0.9000\n",
      "Epoch 398/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1446 - accuracy: 0.9565 - val_loss: 0.3138 - val_accuracy: 0.9000\n",
      "Epoch 399/2000\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 0.1443 - accuracy: 0.9556 - val_loss: 0.3126 - val_accuracy: 0.9000\n",
      "Epoch 400/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1437 - accuracy: 0.9565 - val_loss: 0.3127 - val_accuracy: 0.9000\n",
      "Epoch 401/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1432 - accuracy: 0.9565 - val_loss: 0.3115 - val_accuracy: 0.9000\n",
      "Epoch 402/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1429 - accuracy: 0.9565 - val_loss: 0.3116 - val_accuracy: 0.9000\n",
      "Epoch 403/2000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.1422 - accuracy: 0.9565 - val_loss: 0.3112 - val_accuracy: 0.9000\n",
      "Epoch 404/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1417 - accuracy: 0.9583 - val_loss: 0.3104 - val_accuracy: 0.9000\n",
      "Epoch 405/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1413 - accuracy: 0.9583 - val_loss: 0.3107 - val_accuracy: 0.9000\n",
      "Epoch 406/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1406 - accuracy: 0.9583 - val_loss: 0.3097 - val_accuracy: 0.9000\n",
      "Epoch 407/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1400 - accuracy: 0.9583 - val_loss: 0.3095 - val_accuracy: 0.9000\n",
      "Epoch 408/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1398 - accuracy: 0.9583 - val_loss: 0.3091 - val_accuracy: 0.8917\n",
      "Epoch 409/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1390 - accuracy: 0.9593 - val_loss: 0.3089 - val_accuracy: 0.8917\n",
      "Epoch 410/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1386 - accuracy: 0.9593 - val_loss: 0.3084 - val_accuracy: 0.8917\n",
      "Epoch 411/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1381 - accuracy: 0.9602 - val_loss: 0.3082 - val_accuracy: 0.8917\n",
      "Epoch 412/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1375 - accuracy: 0.9602 - val_loss: 0.3075 - val_accuracy: 0.8917\n",
      "Epoch 413/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1369 - accuracy: 0.9602 - val_loss: 0.3076 - val_accuracy: 0.8917\n",
      "Epoch 414/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1364 - accuracy: 0.9593 - val_loss: 0.3069 - val_accuracy: 0.8917\n",
      "Epoch 415/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1358 - accuracy: 0.9602 - val_loss: 0.3069 - val_accuracy: 0.8917\n",
      "Epoch 416/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1352 - accuracy: 0.9593 - val_loss: 0.3062 - val_accuracy: 0.8917\n",
      "Epoch 417/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1347 - accuracy: 0.9602 - val_loss: 0.3061 - val_accuracy: 0.8917\n",
      "Epoch 418/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1342 - accuracy: 0.9593 - val_loss: 0.3055 - val_accuracy: 0.8917\n",
      "Epoch 419/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1335 - accuracy: 0.9602 - val_loss: 0.3054 - val_accuracy: 0.8833\n",
      "Epoch 420/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1330 - accuracy: 0.9593 - val_loss: 0.3049 - val_accuracy: 0.8833\n",
      "Epoch 421/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1325 - accuracy: 0.9602 - val_loss: 0.3045 - val_accuracy: 0.8833\n",
      "Epoch 422/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1319 - accuracy: 0.9602 - val_loss: 0.3043 - val_accuracy: 0.8833\n",
      "Epoch 423/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1315 - accuracy: 0.9611 - val_loss: 0.3039 - val_accuracy: 0.8833\n",
      "Epoch 424/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1309 - accuracy: 0.9602 - val_loss: 0.3036 - val_accuracy: 0.8833\n",
      "Epoch 425/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1305 - accuracy: 0.9620 - val_loss: 0.3034 - val_accuracy: 0.8833\n",
      "Epoch 426/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1300 - accuracy: 0.9620 - val_loss: 0.3026 - val_accuracy: 0.8833\n",
      "Epoch 427/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1294 - accuracy: 0.9620 - val_loss: 0.3025 - val_accuracy: 0.8833\n",
      "Epoch 428/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1289 - accuracy: 0.9630 - val_loss: 0.3022 - val_accuracy: 0.8833\n",
      "Epoch 429/2000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.1285 - accuracy: 0.9630 - val_loss: 0.3017 - val_accuracy: 0.8833\n",
      "Epoch 430/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1279 - accuracy: 0.9611 - val_loss: 0.3020 - val_accuracy: 0.8833\n",
      "Epoch 431/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1275 - accuracy: 0.9620 - val_loss: 0.3011 - val_accuracy: 0.8833\n",
      "Epoch 432/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1270 - accuracy: 0.9620 - val_loss: 0.3009 - val_accuracy: 0.8833\n",
      "Epoch 433/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1265 - accuracy: 0.9620 - val_loss: 0.3008 - val_accuracy: 0.8833\n",
      "Epoch 434/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1260 - accuracy: 0.9620 - val_loss: 0.3005 - val_accuracy: 0.8833\n",
      "Epoch 435/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1255 - accuracy: 0.9620 - val_loss: 0.3003 - val_accuracy: 0.8833\n",
      "Epoch 436/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1250 - accuracy: 0.9620 - val_loss: 0.2998 - val_accuracy: 0.8833\n",
      "Epoch 437/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1245 - accuracy: 0.9620 - val_loss: 0.2996 - val_accuracy: 0.8833\n",
      "Epoch 438/2000\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.1240 - accuracy: 0.9630 - val_loss: 0.2987 - val_accuracy: 0.8833\n",
      "Epoch 439/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1236 - accuracy: 0.9630 - val_loss: 0.2991 - val_accuracy: 0.8833\n",
      "Epoch 440/2000\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.1231 - accuracy: 0.9639 - val_loss: 0.2986 - val_accuracy: 0.8833\n",
      "Epoch 441/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1226 - accuracy: 0.9630 - val_loss: 0.2982 - val_accuracy: 0.8833\n",
      "Epoch 442/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1221 - accuracy: 0.9639 - val_loss: 0.2980 - val_accuracy: 0.8833\n",
      "Epoch 443/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1216 - accuracy: 0.9648 - val_loss: 0.2972 - val_accuracy: 0.8833\n",
      "Epoch 444/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1211 - accuracy: 0.9648 - val_loss: 0.2972 - val_accuracy: 0.8833\n",
      "Epoch 445/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1206 - accuracy: 0.9648 - val_loss: 0.2968 - val_accuracy: 0.8833\n",
      "Epoch 446/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1202 - accuracy: 0.9648 - val_loss: 0.2964 - val_accuracy: 0.8833\n",
      "Epoch 447/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1197 - accuracy: 0.9657 - val_loss: 0.2953 - val_accuracy: 0.8917\n",
      "Epoch 448/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1192 - accuracy: 0.9667 - val_loss: 0.2949 - val_accuracy: 0.9000\n",
      "Epoch 449/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1187 - accuracy: 0.9667 - val_loss: 0.2943 - val_accuracy: 0.9000\n",
      "Epoch 450/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1183 - accuracy: 0.9667 - val_loss: 0.2942 - val_accuracy: 0.9000\n",
      "Epoch 451/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1179 - accuracy: 0.9676 - val_loss: 0.2939 - val_accuracy: 0.9000\n",
      "Epoch 452/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1174 - accuracy: 0.9667 - val_loss: 0.2937 - val_accuracy: 0.9000\n",
      "Epoch 453/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1170 - accuracy: 0.9676 - val_loss: 0.2933 - val_accuracy: 0.9000\n",
      "Epoch 454/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1166 - accuracy: 0.9685 - val_loss: 0.2932 - val_accuracy: 0.9000\n",
      "Epoch 455/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1162 - accuracy: 0.9667 - val_loss: 0.2929 - val_accuracy: 0.9000\n",
      "Epoch 456/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1158 - accuracy: 0.9685 - val_loss: 0.2924 - val_accuracy: 0.9000\n",
      "Epoch 457/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1154 - accuracy: 0.9685 - val_loss: 0.2921 - val_accuracy: 0.9000\n",
      "Epoch 458/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1150 - accuracy: 0.9685 - val_loss: 0.2922 - val_accuracy: 0.9000\n",
      "Epoch 459/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1146 - accuracy: 0.9676 - val_loss: 0.2920 - val_accuracy: 0.9000\n",
      "Epoch 460/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1142 - accuracy: 0.9685 - val_loss: 0.2913 - val_accuracy: 0.9000\n",
      "Epoch 461/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1139 - accuracy: 0.9694 - val_loss: 0.2913 - val_accuracy: 0.9000\n",
      "Epoch 462/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1134 - accuracy: 0.9685 - val_loss: 0.2911 - val_accuracy: 0.9000\n",
      "Epoch 463/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1130 - accuracy: 0.9694 - val_loss: 0.2914 - val_accuracy: 0.9000\n",
      "Epoch 464/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1126 - accuracy: 0.9713 - val_loss: 0.2911 - val_accuracy: 0.9000\n",
      "Epoch 465/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1123 - accuracy: 0.9713 - val_loss: 0.2910 - val_accuracy: 0.9000\n",
      "Epoch 466/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1119 - accuracy: 0.9713 - val_loss: 0.2905 - val_accuracy: 0.9000\n",
      "Epoch 467/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1114 - accuracy: 0.9713 - val_loss: 0.2908 - val_accuracy: 0.9000\n",
      "Epoch 468/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1110 - accuracy: 0.9722 - val_loss: 0.2903 - val_accuracy: 0.9000\n",
      "Epoch 469/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1106 - accuracy: 0.9713 - val_loss: 0.2905 - val_accuracy: 0.9000\n",
      "Epoch 470/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1101 - accuracy: 0.9731 - val_loss: 0.2899 - val_accuracy: 0.9000\n",
      "Epoch 471/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1097 - accuracy: 0.9722 - val_loss: 0.2892 - val_accuracy: 0.9000\n",
      "Epoch 472/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1093 - accuracy: 0.9731 - val_loss: 0.2892 - val_accuracy: 0.9000\n",
      "Epoch 473/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1089 - accuracy: 0.9731 - val_loss: 0.2894 - val_accuracy: 0.9000\n",
      "Epoch 474/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1086 - accuracy: 0.9731 - val_loss: 0.2889 - val_accuracy: 0.9000\n",
      "Epoch 475/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1082 - accuracy: 0.9731 - val_loss: 0.2887 - val_accuracy: 0.9000\n",
      "Epoch 476/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1078 - accuracy: 0.9731 - val_loss: 0.2882 - val_accuracy: 0.9000\n",
      "Epoch 477/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1074 - accuracy: 0.9731 - val_loss: 0.2881 - val_accuracy: 0.9000\n",
      "Epoch 478/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1070 - accuracy: 0.9741 - val_loss: 0.2879 - val_accuracy: 0.9000\n",
      "Epoch 479/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1066 - accuracy: 0.9741 - val_loss: 0.2876 - val_accuracy: 0.9000\n",
      "Epoch 480/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1062 - accuracy: 0.9741 - val_loss: 0.2877 - val_accuracy: 0.9000\n",
      "Epoch 481/2000\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.1058 - accuracy: 0.9750 - val_loss: 0.2870 - val_accuracy: 0.9000\n",
      "Epoch 482/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1054 - accuracy: 0.9741 - val_loss: 0.2878 - val_accuracy: 0.9000\n",
      "Epoch 483/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1051 - accuracy: 0.9750 - val_loss: 0.2869 - val_accuracy: 0.9000\n",
      "Epoch 484/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1047 - accuracy: 0.9750 - val_loss: 0.2868 - val_accuracy: 0.9000\n",
      "Epoch 485/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1043 - accuracy: 0.9750 - val_loss: 0.2865 - val_accuracy: 0.9000\n",
      "Epoch 486/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1039 - accuracy: 0.9750 - val_loss: 0.2863 - val_accuracy: 0.9000\n",
      "Epoch 487/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1035 - accuracy: 0.9769 - val_loss: 0.2862 - val_accuracy: 0.9000\n",
      "Epoch 488/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1032 - accuracy: 0.9769 - val_loss: 0.2865 - val_accuracy: 0.9000\n",
      "Epoch 489/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1028 - accuracy: 0.9769 - val_loss: 0.2860 - val_accuracy: 0.9000\n",
      "Epoch 490/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.1025 - accuracy: 0.9769 - val_loss: 0.2867 - val_accuracy: 0.9000\n",
      "Epoch 491/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1021 - accuracy: 0.9769 - val_loss: 0.2858 - val_accuracy: 0.9000\n",
      "Epoch 492/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1017 - accuracy: 0.9778 - val_loss: 0.2861 - val_accuracy: 0.9000\n",
      "Epoch 493/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1014 - accuracy: 0.9787 - val_loss: 0.2862 - val_accuracy: 0.9000\n",
      "Epoch 494/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1010 - accuracy: 0.9787 - val_loss: 0.2852 - val_accuracy: 0.9000\n",
      "Epoch 495/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.1006 - accuracy: 0.9778 - val_loss: 0.2857 - val_accuracy: 0.9000\n",
      "Epoch 496/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1003 - accuracy: 0.9787 - val_loss: 0.2847 - val_accuracy: 0.9000\n",
      "Epoch 497/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1000 - accuracy: 0.9787 - val_loss: 0.2857 - val_accuracy: 0.9000\n",
      "Epoch 498/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0996 - accuracy: 0.9787 - val_loss: 0.2852 - val_accuracy: 0.9000\n",
      "Epoch 499/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0993 - accuracy: 0.9787 - val_loss: 0.2854 - val_accuracy: 0.9000\n",
      "Epoch 500/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0990 - accuracy: 0.9787 - val_loss: 0.2853 - val_accuracy: 0.8917\n",
      "Epoch 501/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0986 - accuracy: 0.9787 - val_loss: 0.2850 - val_accuracy: 0.9000\n",
      "Epoch 502/2000\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 0.0983 - accuracy: 0.9787 - val_loss: 0.2851 - val_accuracy: 0.8917\n",
      "Epoch 503/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0980 - accuracy: 0.9787 - val_loss: 0.2848 - val_accuracy: 0.8917\n",
      "Epoch 504/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0976 - accuracy: 0.9787 - val_loss: 0.2852 - val_accuracy: 0.8917\n",
      "Epoch 505/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0974 - accuracy: 0.9787 - val_loss: 0.2846 - val_accuracy: 0.8917\n",
      "Epoch 506/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0970 - accuracy: 0.9787 - val_loss: 0.2851 - val_accuracy: 0.8917\n",
      "Epoch 507/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0967 - accuracy: 0.9787 - val_loss: 0.2843 - val_accuracy: 0.8917\n",
      "Epoch 508/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0964 - accuracy: 0.9787 - val_loss: 0.2844 - val_accuracy: 0.8917\n",
      "Epoch 509/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0961 - accuracy: 0.9787 - val_loss: 0.2844 - val_accuracy: 0.8917\n",
      "Epoch 510/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0958 - accuracy: 0.9787 - val_loss: 0.2841 - val_accuracy: 0.8917\n",
      "Epoch 511/2000\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.0955 - accuracy: 0.9787 - val_loss: 0.2838 - val_accuracy: 0.8917\n",
      "Epoch 512/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0951 - accuracy: 0.9787 - val_loss: 0.2837 - val_accuracy: 0.8917\n",
      "Epoch 513/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0948 - accuracy: 0.9787 - val_loss: 0.2840 - val_accuracy: 0.8917\n",
      "Epoch 514/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0946 - accuracy: 0.9787 - val_loss: 0.2835 - val_accuracy: 0.8917\n",
      "Epoch 515/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0943 - accuracy: 0.9787 - val_loss: 0.2841 - val_accuracy: 0.8917\n",
      "Epoch 516/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0939 - accuracy: 0.9806 - val_loss: 0.2838 - val_accuracy: 0.8917\n",
      "Epoch 517/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0936 - accuracy: 0.9796 - val_loss: 0.2834 - val_accuracy: 0.8917\n",
      "Epoch 518/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0933 - accuracy: 0.9806 - val_loss: 0.2836 - val_accuracy: 0.8917\n",
      "Epoch 519/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0929 - accuracy: 0.9806 - val_loss: 0.2838 - val_accuracy: 0.8917\n",
      "Epoch 520/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0927 - accuracy: 0.9806 - val_loss: 0.2834 - val_accuracy: 0.8917\n",
      "Epoch 521/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0923 - accuracy: 0.9806 - val_loss: 0.2837 - val_accuracy: 0.8917\n",
      "Epoch 522/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0920 - accuracy: 0.9806 - val_loss: 0.2835 - val_accuracy: 0.8917\n",
      "Epoch 523/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0917 - accuracy: 0.9815 - val_loss: 0.2833 - val_accuracy: 0.8917\n",
      "Epoch 524/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0914 - accuracy: 0.9815 - val_loss: 0.2833 - val_accuracy: 0.8917\n",
      "Epoch 525/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0911 - accuracy: 0.9815 - val_loss: 0.2834 - val_accuracy: 0.8917\n",
      "Epoch 526/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0908 - accuracy: 0.9815 - val_loss: 0.2834 - val_accuracy: 0.8917\n",
      "Epoch 527/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0906 - accuracy: 0.9824 - val_loss: 0.2833 - val_accuracy: 0.8917\n",
      "Epoch 528/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0902 - accuracy: 0.9824 - val_loss: 0.2827 - val_accuracy: 0.8917\n",
      "Epoch 529/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0899 - accuracy: 0.9824 - val_loss: 0.2835 - val_accuracy: 0.8917\n",
      "Epoch 530/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0896 - accuracy: 0.9833 - val_loss: 0.2836 - val_accuracy: 0.8917\n",
      "Epoch 531/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0892 - accuracy: 0.9833 - val_loss: 0.2833 - val_accuracy: 0.8917\n",
      "Epoch 532/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0890 - accuracy: 0.9843 - val_loss: 0.2832 - val_accuracy: 0.8917\n",
      "Epoch 533/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0887 - accuracy: 0.9833 - val_loss: 0.2837 - val_accuracy: 0.8917\n",
      "Epoch 534/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0883 - accuracy: 0.9843 - val_loss: 0.2835 - val_accuracy: 0.8917\n",
      "Epoch 535/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0880 - accuracy: 0.9843 - val_loss: 0.2840 - val_accuracy: 0.8917\n",
      "Epoch 536/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0877 - accuracy: 0.9843 - val_loss: 0.2831 - val_accuracy: 0.8917\n",
      "Epoch 537/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0874 - accuracy: 0.9843 - val_loss: 0.2838 - val_accuracy: 0.8917\n",
      "Epoch 538/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0871 - accuracy: 0.9843 - val_loss: 0.2839 - val_accuracy: 0.8917\n",
      "Epoch 539/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0868 - accuracy: 0.9843 - val_loss: 0.2838 - val_accuracy: 0.8917\n",
      "Epoch 540/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0865 - accuracy: 0.9843 - val_loss: 0.2835 - val_accuracy: 0.8917\n",
      "Epoch 541/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0861 - accuracy: 0.9843 - val_loss: 0.2842 - val_accuracy: 0.8917\n",
      "Epoch 542/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0858 - accuracy: 0.9843 - val_loss: 0.2835 - val_accuracy: 0.8917\n",
      "Epoch 543/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0856 - accuracy: 0.9861 - val_loss: 0.2841 - val_accuracy: 0.8917\n",
      "Epoch 544/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0853 - accuracy: 0.9843 - val_loss: 0.2836 - val_accuracy: 0.8917\n",
      "Epoch 545/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0849 - accuracy: 0.9843 - val_loss: 0.2840 - val_accuracy: 0.8917\n",
      "Epoch 546/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0846 - accuracy: 0.9852 - val_loss: 0.2841 - val_accuracy: 0.8917\n",
      "Epoch 547/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0844 - accuracy: 0.9861 - val_loss: 0.2839 - val_accuracy: 0.8917\n",
      "Epoch 548/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0840 - accuracy: 0.9852 - val_loss: 0.2839 - val_accuracy: 0.8917\n",
      "Epoch 549/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0838 - accuracy: 0.9852 - val_loss: 0.2840 - val_accuracy: 0.8917\n",
      "Epoch 550/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0835 - accuracy: 0.9861 - val_loss: 0.2842 - val_accuracy: 0.8833\n",
      "Epoch 551/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0832 - accuracy: 0.9861 - val_loss: 0.2843 - val_accuracy: 0.8917\n",
      "Epoch 552/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0829 - accuracy: 0.9861 - val_loss: 0.2842 - val_accuracy: 0.8917\n",
      "Epoch 553/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0826 - accuracy: 0.9861 - val_loss: 0.2841 - val_accuracy: 0.8833\n",
      "Epoch 554/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0822 - accuracy: 0.9870 - val_loss: 0.2839 - val_accuracy: 0.9000\n",
      "Epoch 555/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0820 - accuracy: 0.9861 - val_loss: 0.2840 - val_accuracy: 0.9000\n",
      "Epoch 556/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0817 - accuracy: 0.9861 - val_loss: 0.2846 - val_accuracy: 0.8917\n",
      "Epoch 557/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0815 - accuracy: 0.9870 - val_loss: 0.2840 - val_accuracy: 0.9000\n",
      "Epoch 558/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0811 - accuracy: 0.9870 - val_loss: 0.2831 - val_accuracy: 0.8917\n",
      "Epoch 559/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0808 - accuracy: 0.9861 - val_loss: 0.2834 - val_accuracy: 0.9000\n",
      "Epoch 560/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0805 - accuracy: 0.9861 - val_loss: 0.2838 - val_accuracy: 0.8917\n",
      "Epoch 561/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0802 - accuracy: 0.9870 - val_loss: 0.2843 - val_accuracy: 0.8917\n",
      "Epoch 562/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0799 - accuracy: 0.9880 - val_loss: 0.2840 - val_accuracy: 0.9000\n",
      "Epoch 563/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0797 - accuracy: 0.9880 - val_loss: 0.2833 - val_accuracy: 0.8917\n",
      "Epoch 564/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0794 - accuracy: 0.9870 - val_loss: 0.2832 - val_accuracy: 0.9000\n",
      "Epoch 565/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0791 - accuracy: 0.9880 - val_loss: 0.2842 - val_accuracy: 0.8917\n",
      "Epoch 566/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0789 - accuracy: 0.9880 - val_loss: 0.2836 - val_accuracy: 0.8917\n",
      "Epoch 567/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0786 - accuracy: 0.9880 - val_loss: 0.2829 - val_accuracy: 0.9000\n",
      "Epoch 568/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0782 - accuracy: 0.9880 - val_loss: 0.2839 - val_accuracy: 0.8917\n",
      "Epoch 569/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0780 - accuracy: 0.9880 - val_loss: 0.2832 - val_accuracy: 0.9000\n",
      "Epoch 570/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0777 - accuracy: 0.9880 - val_loss: 0.2837 - val_accuracy: 0.8917\n",
      "Epoch 571/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0775 - accuracy: 0.9889 - val_loss: 0.2836 - val_accuracy: 0.9000\n",
      "Epoch 572/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0772 - accuracy: 0.9889 - val_loss: 0.2830 - val_accuracy: 0.8917\n",
      "Epoch 573/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0769 - accuracy: 0.9889 - val_loss: 0.2833 - val_accuracy: 0.9000\n",
      "Epoch 574/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0767 - accuracy: 0.9889 - val_loss: 0.2839 - val_accuracy: 0.8917\n",
      "Epoch 575/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0765 - accuracy: 0.9889 - val_loss: 0.2836 - val_accuracy: 0.8917\n",
      "Epoch 576/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0762 - accuracy: 0.9898 - val_loss: 0.2830 - val_accuracy: 0.9000\n",
      "Epoch 577/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0759 - accuracy: 0.9889 - val_loss: 0.2836 - val_accuracy: 0.8917\n",
      "Epoch 578/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0757 - accuracy: 0.9889 - val_loss: 0.2840 - val_accuracy: 0.8917\n",
      "Epoch 579/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0754 - accuracy: 0.9898 - val_loss: 0.2831 - val_accuracy: 0.9000\n",
      "Epoch 580/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0751 - accuracy: 0.9898 - val_loss: 0.2832 - val_accuracy: 0.9000\n",
      "Epoch 581/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0749 - accuracy: 0.9898 - val_loss: 0.2830 - val_accuracy: 0.8917\n",
      "Epoch 582/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0746 - accuracy: 0.9898 - val_loss: 0.2830 - val_accuracy: 0.9000\n",
      "Epoch 583/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0744 - accuracy: 0.9898 - val_loss: 0.2835 - val_accuracy: 0.9000\n",
      "Epoch 584/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0742 - accuracy: 0.9898 - val_loss: 0.2836 - val_accuracy: 0.8917\n",
      "Epoch 585/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0739 - accuracy: 0.9898 - val_loss: 0.2830 - val_accuracy: 0.9000\n",
      "Epoch 586/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0736 - accuracy: 0.9898 - val_loss: 0.2841 - val_accuracy: 0.8917\n",
      "Epoch 587/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0734 - accuracy: 0.9898 - val_loss: 0.2834 - val_accuracy: 0.9000\n",
      "Epoch 588/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0731 - accuracy: 0.9898 - val_loss: 0.2835 - val_accuracy: 0.8917\n",
      "Epoch 589/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0728 - accuracy: 0.9898 - val_loss: 0.2831 - val_accuracy: 0.8917\n",
      "Epoch 590/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0726 - accuracy: 0.9898 - val_loss: 0.2834 - val_accuracy: 0.9000\n",
      "Epoch 591/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0724 - accuracy: 0.9898 - val_loss: 0.2835 - val_accuracy: 0.8917\n",
      "Epoch 592/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0721 - accuracy: 0.9898 - val_loss: 0.2833 - val_accuracy: 0.9000\n",
      "Epoch 593/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0719 - accuracy: 0.9898 - val_loss: 0.2836 - val_accuracy: 0.8917\n",
      "Epoch 594/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0716 - accuracy: 0.9898 - val_loss: 0.2837 - val_accuracy: 0.8917\n",
      "Epoch 595/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0714 - accuracy: 0.9898 - val_loss: 0.2833 - val_accuracy: 0.9000\n",
      "Epoch 596/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0711 - accuracy: 0.9898 - val_loss: 0.2832 - val_accuracy: 0.9000\n",
      "Epoch 597/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0708 - accuracy: 0.9898 - val_loss: 0.2840 - val_accuracy: 0.8917\n",
      "Epoch 598/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0705 - accuracy: 0.9898 - val_loss: 0.2832 - val_accuracy: 0.8917\n",
      "Epoch 599/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0703 - accuracy: 0.9898 - val_loss: 0.2836 - val_accuracy: 0.9000\n",
      "Epoch 600/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0701 - accuracy: 0.9898 - val_loss: 0.2838 - val_accuracy: 0.8917\n",
      "Epoch 601/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0698 - accuracy: 0.9898 - val_loss: 0.2836 - val_accuracy: 0.8917\n",
      "Epoch 602/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0696 - accuracy: 0.9898 - val_loss: 0.2835 - val_accuracy: 0.8917\n",
      "Epoch 603/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0694 - accuracy: 0.9898 - val_loss: 0.2835 - val_accuracy: 0.8917\n",
      "Epoch 604/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0691 - accuracy: 0.9898 - val_loss: 0.2826 - val_accuracy: 0.8917\n",
      "Epoch 605/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0688 - accuracy: 0.9898 - val_loss: 0.2829 - val_accuracy: 0.8917\n",
      "Epoch 606/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0686 - accuracy: 0.9898 - val_loss: 0.2827 - val_accuracy: 0.9000\n",
      "Epoch 607/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0683 - accuracy: 0.9898 - val_loss: 0.2833 - val_accuracy: 0.8917\n",
      "Epoch 608/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0681 - accuracy: 0.9907 - val_loss: 0.2825 - val_accuracy: 0.9000\n",
      "Epoch 609/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0678 - accuracy: 0.9907 - val_loss: 0.2829 - val_accuracy: 0.8917\n",
      "Epoch 610/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0675 - accuracy: 0.9907 - val_loss: 0.2821 - val_accuracy: 0.9000\n",
      "Epoch 611/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0673 - accuracy: 0.9907 - val_loss: 0.2825 - val_accuracy: 0.9000\n",
      "Epoch 612/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0670 - accuracy: 0.9907 - val_loss: 0.2823 - val_accuracy: 0.9083\n",
      "Epoch 613/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0668 - accuracy: 0.9907 - val_loss: 0.2825 - val_accuracy: 0.9083\n",
      "Epoch 614/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0665 - accuracy: 0.9907 - val_loss: 0.2816 - val_accuracy: 0.9167\n",
      "Epoch 615/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0663 - accuracy: 0.9907 - val_loss: 0.2818 - val_accuracy: 0.9167\n",
      "Epoch 616/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0660 - accuracy: 0.9907 - val_loss: 0.2817 - val_accuracy: 0.9083\n",
      "Epoch 617/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0658 - accuracy: 0.9907 - val_loss: 0.2824 - val_accuracy: 0.9167\n",
      "Epoch 618/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0656 - accuracy: 0.9907 - val_loss: 0.2815 - val_accuracy: 0.9167\n",
      "Epoch 619/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0652 - accuracy: 0.9907 - val_loss: 0.2815 - val_accuracy: 0.9167\n",
      "Epoch 620/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0650 - accuracy: 0.9907 - val_loss: 0.2815 - val_accuracy: 0.9167\n",
      "Epoch 621/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0648 - accuracy: 0.9907 - val_loss: 0.2814 - val_accuracy: 0.9167\n",
      "Epoch 622/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0645 - accuracy: 0.9907 - val_loss: 0.2810 - val_accuracy: 0.9167\n",
      "Epoch 623/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0643 - accuracy: 0.9907 - val_loss: 0.2813 - val_accuracy: 0.9167\n",
      "Epoch 624/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0640 - accuracy: 0.9926 - val_loss: 0.2803 - val_accuracy: 0.9167\n",
      "Epoch 625/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0638 - accuracy: 0.9917 - val_loss: 0.2811 - val_accuracy: 0.9167\n",
      "Epoch 626/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0636 - accuracy: 0.9917 - val_loss: 0.2806 - val_accuracy: 0.9167\n",
      "Epoch 627/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0632 - accuracy: 0.9917 - val_loss: 0.2808 - val_accuracy: 0.9167\n",
      "Epoch 628/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0630 - accuracy: 0.9926 - val_loss: 0.2802 - val_accuracy: 0.9167\n",
      "Epoch 629/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0627 - accuracy: 0.9926 - val_loss: 0.2810 - val_accuracy: 0.9167\n",
      "Epoch 630/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0625 - accuracy: 0.9926 - val_loss: 0.2809 - val_accuracy: 0.9167\n",
      "Epoch 631/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0623 - accuracy: 0.9944 - val_loss: 0.2806 - val_accuracy: 0.9167\n",
      "Epoch 632/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0621 - accuracy: 0.9926 - val_loss: 0.2806 - val_accuracy: 0.9167\n",
      "Epoch 633/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0619 - accuracy: 0.9926 - val_loss: 0.2806 - val_accuracy: 0.9167\n",
      "Epoch 634/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0616 - accuracy: 0.9935 - val_loss: 0.2804 - val_accuracy: 0.9167\n",
      "Epoch 635/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0614 - accuracy: 0.9926 - val_loss: 0.2806 - val_accuracy: 0.9167\n",
      "Epoch 636/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0611 - accuracy: 0.9935 - val_loss: 0.2806 - val_accuracy: 0.9167\n",
      "Epoch 637/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0609 - accuracy: 0.9944 - val_loss: 0.2809 - val_accuracy: 0.9167\n",
      "Epoch 638/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0607 - accuracy: 0.9944 - val_loss: 0.2805 - val_accuracy: 0.9167\n",
      "Epoch 639/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0604 - accuracy: 0.9935 - val_loss: 0.2815 - val_accuracy: 0.9167\n",
      "Epoch 640/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0602 - accuracy: 0.9944 - val_loss: 0.2809 - val_accuracy: 0.9167\n",
      "Epoch 641/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0600 - accuracy: 0.9935 - val_loss: 0.2816 - val_accuracy: 0.9167\n",
      "Epoch 642/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0598 - accuracy: 0.9944 - val_loss: 0.2816 - val_accuracy: 0.9167\n",
      "Epoch 643/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0595 - accuracy: 0.9944 - val_loss: 0.2813 - val_accuracy: 0.9167\n",
      "Epoch 644/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0593 - accuracy: 0.9944 - val_loss: 0.2816 - val_accuracy: 0.9167\n",
      "Epoch 645/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0591 - accuracy: 0.9935 - val_loss: 0.2825 - val_accuracy: 0.9167\n",
      "Epoch 646/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0589 - accuracy: 0.9944 - val_loss: 0.2816 - val_accuracy: 0.9167\n",
      "Epoch 647/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0586 - accuracy: 0.9944 - val_loss: 0.2818 - val_accuracy: 0.9167\n",
      "Epoch 648/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0584 - accuracy: 0.9944 - val_loss: 0.2824 - val_accuracy: 0.9167\n",
      "Epoch 649/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0582 - accuracy: 0.9944 - val_loss: 0.2820 - val_accuracy: 0.9167\n",
      "Epoch 650/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0579 - accuracy: 0.9944 - val_loss: 0.2825 - val_accuracy: 0.9167\n",
      "Epoch 651/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0578 - accuracy: 0.9944 - val_loss: 0.2832 - val_accuracy: 0.9167\n",
      "Epoch 652/2000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.0575 - accuracy: 0.9954 - val_loss: 0.2826 - val_accuracy: 0.9167\n",
      "Epoch 653/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0572 - accuracy: 0.9944 - val_loss: 0.2829 - val_accuracy: 0.9167\n",
      "Epoch 654/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0570 - accuracy: 0.9944 - val_loss: 0.2834 - val_accuracy: 0.9167\n",
      "Epoch 655/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0568 - accuracy: 0.9954 - val_loss: 0.2830 - val_accuracy: 0.9167\n",
      "Epoch 656/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0565 - accuracy: 0.9944 - val_loss: 0.2833 - val_accuracy: 0.9167\n",
      "Epoch 657/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0564 - accuracy: 0.9954 - val_loss: 0.2831 - val_accuracy: 0.9167\n",
      "Epoch 658/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0561 - accuracy: 0.9944 - val_loss: 0.2837 - val_accuracy: 0.9167\n",
      "Epoch 659/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0559 - accuracy: 0.9954 - val_loss: 0.2842 - val_accuracy: 0.9167\n",
      "Epoch 660/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0557 - accuracy: 0.9954 - val_loss: 0.2839 - val_accuracy: 0.9167\n",
      "Epoch 661/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0555 - accuracy: 0.9954 - val_loss: 0.2841 - val_accuracy: 0.9167\n",
      "Epoch 662/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0553 - accuracy: 0.9954 - val_loss: 0.2837 - val_accuracy: 0.9167\n",
      "Epoch 663/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0550 - accuracy: 0.9954 - val_loss: 0.2840 - val_accuracy: 0.9167\n",
      "Epoch 664/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0548 - accuracy: 0.9954 - val_loss: 0.2838 - val_accuracy: 0.9167\n",
      "Epoch 665/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0546 - accuracy: 0.9954 - val_loss: 0.2844 - val_accuracy: 0.9167\n",
      "Epoch 666/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0544 - accuracy: 0.9954 - val_loss: 0.2847 - val_accuracy: 0.9167\n",
      "Epoch 667/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0542 - accuracy: 0.9954 - val_loss: 0.2840 - val_accuracy: 0.9167\n",
      "Epoch 668/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0540 - accuracy: 0.9954 - val_loss: 0.2845 - val_accuracy: 0.9167\n",
      "Epoch 669/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0537 - accuracy: 0.9954 - val_loss: 0.2843 - val_accuracy: 0.9167\n",
      "Epoch 670/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0536 - accuracy: 0.9954 - val_loss: 0.2846 - val_accuracy: 0.9167\n",
      "Epoch 671/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0534 - accuracy: 0.9954 - val_loss: 0.2842 - val_accuracy: 0.9167\n",
      "Epoch 672/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0532 - accuracy: 0.9954 - val_loss: 0.2850 - val_accuracy: 0.9167\n",
      "Epoch 673/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0530 - accuracy: 0.9954 - val_loss: 0.2849 - val_accuracy: 0.9167\n",
      "Epoch 674/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0528 - accuracy: 0.9954 - val_loss: 0.2841 - val_accuracy: 0.9167\n",
      "Epoch 675/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0526 - accuracy: 0.9954 - val_loss: 0.2846 - val_accuracy: 0.9167\n",
      "Epoch 676/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0524 - accuracy: 0.9954 - val_loss: 0.2844 - val_accuracy: 0.9167\n",
      "Epoch 677/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0522 - accuracy: 0.9954 - val_loss: 0.2846 - val_accuracy: 0.9167\n",
      "Epoch 678/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0520 - accuracy: 0.9954 - val_loss: 0.2846 - val_accuracy: 0.9167\n",
      "Epoch 679/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0518 - accuracy: 0.9954 - val_loss: 0.2839 - val_accuracy: 0.9167\n",
      "Epoch 680/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0517 - accuracy: 0.9954 - val_loss: 0.2842 - val_accuracy: 0.9167\n",
      "Epoch 681/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0515 - accuracy: 0.9954 - val_loss: 0.2839 - val_accuracy: 0.9167\n",
      "Epoch 682/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0513 - accuracy: 0.9954 - val_loss: 0.2842 - val_accuracy: 0.9167\n",
      "Epoch 683/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0511 - accuracy: 0.9954 - val_loss: 0.2843 - val_accuracy: 0.9167\n",
      "Epoch 684/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0509 - accuracy: 0.9954 - val_loss: 0.2839 - val_accuracy: 0.9167\n",
      "Epoch 685/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0507 - accuracy: 0.9954 - val_loss: 0.2840 - val_accuracy: 0.9167\n",
      "Epoch 686/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0506 - accuracy: 0.9954 - val_loss: 0.2840 - val_accuracy: 0.9083\n",
      "Epoch 687/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0505 - accuracy: 0.9954 - val_loss: 0.2840 - val_accuracy: 0.9167\n",
      "Epoch 688/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0502 - accuracy: 0.9954 - val_loss: 0.2834 - val_accuracy: 0.9167\n",
      "Epoch 689/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0500 - accuracy: 0.9954 - val_loss: 0.2842 - val_accuracy: 0.9167\n",
      "Epoch 690/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0499 - accuracy: 0.9954 - val_loss: 0.2837 - val_accuracy: 0.9167\n",
      "Epoch 691/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0496 - accuracy: 0.9954 - val_loss: 0.2836 - val_accuracy: 0.9167\n",
      "Epoch 692/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0495 - accuracy: 0.9963 - val_loss: 0.2838 - val_accuracy: 0.9167\n",
      "Epoch 693/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0493 - accuracy: 0.9954 - val_loss: 0.2830 - val_accuracy: 0.9167\n",
      "Epoch 694/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0491 - accuracy: 0.9963 - val_loss: 0.2835 - val_accuracy: 0.9167\n",
      "Epoch 695/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0489 - accuracy: 0.9963 - val_loss: 0.2829 - val_accuracy: 0.9167\n",
      "Epoch 696/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0488 - accuracy: 0.9972 - val_loss: 0.2834 - val_accuracy: 0.9167\n",
      "Epoch 697/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0486 - accuracy: 0.9963 - val_loss: 0.2833 - val_accuracy: 0.9083\n",
      "Epoch 698/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0485 - accuracy: 0.9972 - val_loss: 0.2835 - val_accuracy: 0.9167\n",
      "Epoch 699/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0483 - accuracy: 0.9963 - val_loss: 0.2829 - val_accuracy: 0.9083\n",
      "Epoch 700/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0481 - accuracy: 0.9972 - val_loss: 0.2839 - val_accuracy: 0.9083\n",
      "Epoch 701/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0480 - accuracy: 0.9972 - val_loss: 0.2833 - val_accuracy: 0.9083\n",
      "Epoch 702/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0477 - accuracy: 0.9981 - val_loss: 0.2832 - val_accuracy: 0.9083\n",
      "Epoch 703/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0476 - accuracy: 0.9972 - val_loss: 0.2836 - val_accuracy: 0.9083\n",
      "Epoch 704/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0475 - accuracy: 0.9981 - val_loss: 0.2834 - val_accuracy: 0.9083\n",
      "Epoch 705/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0473 - accuracy: 0.9981 - val_loss: 0.2838 - val_accuracy: 0.9083\n",
      "Epoch 706/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0471 - accuracy: 0.9981 - val_loss: 0.2835 - val_accuracy: 0.9083\n",
      "Epoch 707/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0470 - accuracy: 0.9981 - val_loss: 0.2838 - val_accuracy: 0.9083\n",
      "Epoch 708/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0468 - accuracy: 0.9981 - val_loss: 0.2838 - val_accuracy: 0.9083\n",
      "Epoch 709/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0466 - accuracy: 0.9981 - val_loss: 0.2831 - val_accuracy: 0.9083\n",
      "Epoch 710/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0464 - accuracy: 0.9981 - val_loss: 0.2836 - val_accuracy: 0.9083\n",
      "Epoch 711/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0462 - accuracy: 0.9981 - val_loss: 0.2836 - val_accuracy: 0.9083\n",
      "Epoch 712/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0461 - accuracy: 0.9981 - val_loss: 0.2841 - val_accuracy: 0.9083\n",
      "Epoch 713/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0460 - accuracy: 0.9981 - val_loss: 0.2833 - val_accuracy: 0.9083\n",
      "Epoch 714/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0458 - accuracy: 0.9981 - val_loss: 0.2842 - val_accuracy: 0.9083\n",
      "Epoch 715/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0456 - accuracy: 0.9981 - val_loss: 0.2839 - val_accuracy: 0.9083\n",
      "Epoch 716/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0454 - accuracy: 0.9981 - val_loss: 0.2836 - val_accuracy: 0.9083\n",
      "Epoch 717/2000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.0453 - accuracy: 0.9981 - val_loss: 0.2844 - val_accuracy: 0.9083\n",
      "Epoch 718/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0452 - accuracy: 0.9981 - val_loss: 0.2842 - val_accuracy: 0.9083\n",
      "Epoch 719/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0450 - accuracy: 0.9981 - val_loss: 0.2849 - val_accuracy: 0.9083\n",
      "Epoch 720/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0449 - accuracy: 0.9981 - val_loss: 0.2841 - val_accuracy: 0.9083\n",
      "Epoch 721/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0446 - accuracy: 0.9981 - val_loss: 0.2844 - val_accuracy: 0.9083\n",
      "Epoch 722/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0445 - accuracy: 0.9981 - val_loss: 0.2846 - val_accuracy: 0.9083\n",
      "Epoch 723/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0444 - accuracy: 0.9981 - val_loss: 0.2848 - val_accuracy: 0.9083\n",
      "Epoch 724/2000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.0442 - accuracy: 0.9981 - val_loss: 0.2847 - val_accuracy: 0.9083\n",
      "Epoch 725/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0441 - accuracy: 0.9981 - val_loss: 0.2845 - val_accuracy: 0.9083\n",
      "Epoch 726/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0439 - accuracy: 0.9981 - val_loss: 0.2850 - val_accuracy: 0.9083\n",
      "Epoch 727/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0438 - accuracy: 0.9981 - val_loss: 0.2848 - val_accuracy: 0.9083\n",
      "Epoch 728/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0436 - accuracy: 0.9981 - val_loss: 0.2851 - val_accuracy: 0.9083\n",
      "Epoch 729/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0434 - accuracy: 0.9981 - val_loss: 0.2847 - val_accuracy: 0.9083\n",
      "Epoch 730/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0433 - accuracy: 0.9981 - val_loss: 0.2852 - val_accuracy: 0.9083\n",
      "Epoch 731/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0431 - accuracy: 0.9981 - val_loss: 0.2855 - val_accuracy: 0.9083\n",
      "Epoch 732/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0430 - accuracy: 0.9981 - val_loss: 0.2856 - val_accuracy: 0.9083\n",
      "Epoch 733/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0429 - accuracy: 0.9981 - val_loss: 0.2855 - val_accuracy: 0.9083\n",
      "Epoch 734/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0426 - accuracy: 0.9981 - val_loss: 0.2855 - val_accuracy: 0.9083\n",
      "Epoch 735/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0425 - accuracy: 0.9981 - val_loss: 0.2856 - val_accuracy: 0.9083\n",
      "Epoch 736/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0423 - accuracy: 0.9981 - val_loss: 0.2859 - val_accuracy: 0.9083\n",
      "Epoch 737/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0422 - accuracy: 0.9981 - val_loss: 0.2861 - val_accuracy: 0.9083\n",
      "Epoch 738/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0421 - accuracy: 0.9981 - val_loss: 0.2863 - val_accuracy: 0.9083\n",
      "Epoch 739/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0419 - accuracy: 0.9981 - val_loss: 0.2856 - val_accuracy: 0.9083\n",
      "Epoch 740/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0417 - accuracy: 0.9981 - val_loss: 0.2864 - val_accuracy: 0.9083\n",
      "Epoch 741/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0416 - accuracy: 0.9981 - val_loss: 0.2867 - val_accuracy: 0.9083\n",
      "Epoch 742/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0415 - accuracy: 0.9981 - val_loss: 0.2861 - val_accuracy: 0.9083\n",
      "Epoch 743/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0413 - accuracy: 0.9981 - val_loss: 0.2871 - val_accuracy: 0.9083\n",
      "Epoch 744/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0411 - accuracy: 0.9981 - val_loss: 0.2865 - val_accuracy: 0.9083\n",
      "Epoch 745/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0410 - accuracy: 0.9981 - val_loss: 0.2874 - val_accuracy: 0.9083\n",
      "Epoch 746/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0408 - accuracy: 0.9981 - val_loss: 0.2873 - val_accuracy: 0.9083\n",
      "Epoch 747/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0406 - accuracy: 0.9981 - val_loss: 0.2871 - val_accuracy: 0.9083\n",
      "Epoch 748/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0406 - accuracy: 0.9981 - val_loss: 0.2876 - val_accuracy: 0.9083\n",
      "Epoch 749/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0404 - accuracy: 0.9981 - val_loss: 0.2873 - val_accuracy: 0.9083\n",
      "Epoch 750/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0402 - accuracy: 0.9981 - val_loss: 0.2878 - val_accuracy: 0.9083\n",
      "Epoch 751/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0401 - accuracy: 0.9981 - val_loss: 0.2878 - val_accuracy: 0.9083\n",
      "Epoch 752/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0399 - accuracy: 0.9981 - val_loss: 0.2875 - val_accuracy: 0.9083\n",
      "Epoch 753/2000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.0398 - accuracy: 0.9972 - val_loss: 0.2883 - val_accuracy: 0.9083\n",
      "Epoch 754/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0396 - accuracy: 0.9972 - val_loss: 0.2879 - val_accuracy: 0.9083\n",
      "Epoch 755/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0395 - accuracy: 0.9981 - val_loss: 0.2888 - val_accuracy: 0.9083\n",
      "Epoch 756/2000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.0394 - accuracy: 0.9972 - val_loss: 0.2890 - val_accuracy: 0.9083\n",
      "Epoch 757/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0392 - accuracy: 0.9981 - val_loss: 0.2886 - val_accuracy: 0.9083\n",
      "Epoch 758/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0391 - accuracy: 0.9972 - val_loss: 0.2895 - val_accuracy: 0.9083\n",
      "Epoch 759/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0389 - accuracy: 0.9972 - val_loss: 0.2891 - val_accuracy: 0.9083\n",
      "Epoch 760/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0388 - accuracy: 0.9972 - val_loss: 0.2898 - val_accuracy: 0.9083\n",
      "Epoch 761/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0387 - accuracy: 0.9972 - val_loss: 0.2898 - val_accuracy: 0.9083\n",
      "Epoch 762/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0385 - accuracy: 0.9972 - val_loss: 0.2899 - val_accuracy: 0.9083\n",
      "Epoch 763/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0384 - accuracy: 0.9972 - val_loss: 0.2901 - val_accuracy: 0.9083\n",
      "Epoch 764/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0382 - accuracy: 0.9972 - val_loss: 0.2901 - val_accuracy: 0.9083\n",
      "Epoch 765/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0381 - accuracy: 0.9972 - val_loss: 0.2902 - val_accuracy: 0.9083\n",
      "Epoch 766/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0379 - accuracy: 0.9972 - val_loss: 0.2906 - val_accuracy: 0.9083\n",
      "Epoch 767/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0378 - accuracy: 0.9972 - val_loss: 0.2907 - val_accuracy: 0.9083\n",
      "Epoch 768/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0376 - accuracy: 0.9972 - val_loss: 0.2913 - val_accuracy: 0.9083\n",
      "Epoch 769/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0374 - accuracy: 0.9972 - val_loss: 0.2913 - val_accuracy: 0.9083\n",
      "Epoch 770/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0373 - accuracy: 0.9972 - val_loss: 0.2917 - val_accuracy: 0.9083\n",
      "Epoch 771/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0372 - accuracy: 0.9972 - val_loss: 0.2922 - val_accuracy: 0.9083\n",
      "Epoch 772/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0371 - accuracy: 0.9972 - val_loss: 0.2916 - val_accuracy: 0.9083\n",
      "Epoch 773/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0369 - accuracy: 0.9972 - val_loss: 0.2920 - val_accuracy: 0.9083\n",
      "Epoch 774/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0368 - accuracy: 0.9972 - val_loss: 0.2924 - val_accuracy: 0.9083\n",
      "Epoch 775/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0367 - accuracy: 0.9972 - val_loss: 0.2922 - val_accuracy: 0.9083\n",
      "Epoch 776/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0365 - accuracy: 0.9972 - val_loss: 0.2931 - val_accuracy: 0.9083\n",
      "Epoch 777/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0364 - accuracy: 0.9972 - val_loss: 0.2924 - val_accuracy: 0.9083\n",
      "Epoch 778/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0362 - accuracy: 0.9972 - val_loss: 0.2932 - val_accuracy: 0.9083\n",
      "Epoch 779/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0361 - accuracy: 0.9972 - val_loss: 0.2932 - val_accuracy: 0.9083\n",
      "Epoch 780/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0360 - accuracy: 0.9972 - val_loss: 0.2936 - val_accuracy: 0.9083\n",
      "Epoch 781/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0359 - accuracy: 0.9972 - val_loss: 0.2938 - val_accuracy: 0.9083\n",
      "Epoch 782/2000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.0357 - accuracy: 0.9972 - val_loss: 0.2929 - val_accuracy: 0.9083\n",
      "Epoch 783/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0355 - accuracy: 0.9972 - val_loss: 0.2940 - val_accuracy: 0.9083\n",
      "Epoch 784/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0355 - accuracy: 0.9972 - val_loss: 0.2936 - val_accuracy: 0.9083\n",
      "Epoch 785/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0353 - accuracy: 0.9972 - val_loss: 0.2944 - val_accuracy: 0.9083\n",
      "Epoch 786/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0352 - accuracy: 0.9972 - val_loss: 0.2936 - val_accuracy: 0.9083\n",
      "Epoch 787/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0351 - accuracy: 0.9972 - val_loss: 0.2946 - val_accuracy: 0.9000\n",
      "Epoch 788/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0349 - accuracy: 0.9972 - val_loss: 0.2942 - val_accuracy: 0.9083\n",
      "Epoch 789/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0349 - accuracy: 0.9972 - val_loss: 0.2944 - val_accuracy: 0.9000\n",
      "Epoch 790/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0347 - accuracy: 0.9972 - val_loss: 0.2945 - val_accuracy: 0.9000\n",
      "Epoch 791/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0347 - accuracy: 0.9972 - val_loss: 0.2947 - val_accuracy: 0.9000\n",
      "Epoch 792/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0345 - accuracy: 0.9972 - val_loss: 0.2954 - val_accuracy: 0.9000\n",
      "Epoch 793/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0344 - accuracy: 0.9972 - val_loss: 0.2945 - val_accuracy: 0.9000\n",
      "Epoch 794/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0343 - accuracy: 0.9972 - val_loss: 0.2956 - val_accuracy: 0.9000\n",
      "Epoch 795/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0342 - accuracy: 0.9972 - val_loss: 0.2949 - val_accuracy: 0.9000\n",
      "Epoch 796/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0339 - accuracy: 0.9972 - val_loss: 0.2953 - val_accuracy: 0.9000\n",
      "Epoch 797/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0339 - accuracy: 0.9972 - val_loss: 0.2955 - val_accuracy: 0.9000\n",
      "Epoch 798/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0337 - accuracy: 0.9972 - val_loss: 0.2968 - val_accuracy: 0.9000\n",
      "Epoch 799/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0336 - accuracy: 0.9972 - val_loss: 0.2968 - val_accuracy: 0.9000\n",
      "Epoch 800/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0335 - accuracy: 0.9972 - val_loss: 0.2971 - val_accuracy: 0.9000\n",
      "Epoch 801/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0334 - accuracy: 0.9972 - val_loss: 0.2964 - val_accuracy: 0.9000\n",
      "Epoch 802/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0332 - accuracy: 0.9972 - val_loss: 0.2976 - val_accuracy: 0.9000\n",
      "Epoch 803/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0331 - accuracy: 0.9981 - val_loss: 0.2974 - val_accuracy: 0.9000\n",
      "Epoch 804/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0330 - accuracy: 0.9981 - val_loss: 0.2979 - val_accuracy: 0.9000\n",
      "Epoch 805/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0328 - accuracy: 0.9981 - val_loss: 0.2980 - val_accuracy: 0.9000\n",
      "Epoch 806/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0327 - accuracy: 0.9981 - val_loss: 0.2991 - val_accuracy: 0.9000\n",
      "Epoch 807/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0326 - accuracy: 0.9981 - val_loss: 0.2982 - val_accuracy: 0.9000\n",
      "Epoch 808/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0323 - accuracy: 0.9981 - val_loss: 0.2991 - val_accuracy: 0.9000\n",
      "Epoch 809/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0322 - accuracy: 0.9981 - val_loss: 0.2998 - val_accuracy: 0.9000\n",
      "Epoch 810/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0321 - accuracy: 0.9981 - val_loss: 0.3001 - val_accuracy: 0.9000\n",
      "Epoch 811/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0319 - accuracy: 0.9981 - val_loss: 0.2999 - val_accuracy: 0.9000\n",
      "Epoch 812/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0317 - accuracy: 0.9981 - val_loss: 0.3011 - val_accuracy: 0.9000\n",
      "Epoch 813/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0317 - accuracy: 0.9981 - val_loss: 0.3007 - val_accuracy: 0.9000\n",
      "Epoch 814/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0315 - accuracy: 0.9981 - val_loss: 0.3011 - val_accuracy: 0.9000\n",
      "Epoch 815/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0313 - accuracy: 0.9981 - val_loss: 0.3024 - val_accuracy: 0.9000\n",
      "Epoch 816/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0311 - accuracy: 0.9981 - val_loss: 0.3014 - val_accuracy: 0.9000\n",
      "Epoch 817/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0310 - accuracy: 0.9981 - val_loss: 0.3020 - val_accuracy: 0.9000\n",
      "Epoch 818/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0308 - accuracy: 0.9981 - val_loss: 0.3023 - val_accuracy: 0.9000\n",
      "Epoch 819/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0307 - accuracy: 0.9981 - val_loss: 0.3039 - val_accuracy: 0.9000\n",
      "Epoch 820/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0306 - accuracy: 0.9981 - val_loss: 0.3035 - val_accuracy: 0.9000\n",
      "Epoch 821/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0304 - accuracy: 0.9981 - val_loss: 0.3044 - val_accuracy: 0.9000\n",
      "Epoch 822/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0303 - accuracy: 0.9981 - val_loss: 0.3039 - val_accuracy: 0.9000\n",
      "Epoch 823/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0301 - accuracy: 0.9981 - val_loss: 0.3051 - val_accuracy: 0.9000\n",
      "Epoch 824/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0301 - accuracy: 0.9981 - val_loss: 0.3048 - val_accuracy: 0.9000\n",
      "Epoch 825/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0299 - accuracy: 0.9981 - val_loss: 0.3052 - val_accuracy: 0.9000\n",
      "Epoch 826/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0297 - accuracy: 0.9981 - val_loss: 0.3063 - val_accuracy: 0.9000\n",
      "Epoch 827/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0295 - accuracy: 0.9981 - val_loss: 0.3060 - val_accuracy: 0.9000\n",
      "Epoch 828/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0294 - accuracy: 0.9981 - val_loss: 0.3070 - val_accuracy: 0.9000\n",
      "Epoch 829/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0293 - accuracy: 0.9981 - val_loss: 0.3063 - val_accuracy: 0.9000\n",
      "Epoch 830/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0291 - accuracy: 0.9981 - val_loss: 0.3078 - val_accuracy: 0.9000\n",
      "Epoch 831/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0289 - accuracy: 0.9981 - val_loss: 0.3079 - val_accuracy: 0.9000\n",
      "Epoch 832/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0288 - accuracy: 0.9991 - val_loss: 0.3085 - val_accuracy: 0.9000\n",
      "Epoch 833/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0286 - accuracy: 0.9981 - val_loss: 0.3080 - val_accuracy: 0.9000\n",
      "Epoch 834/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0284 - accuracy: 0.9991 - val_loss: 0.3091 - val_accuracy: 0.9000\n",
      "Epoch 835/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0283 - accuracy: 0.9991 - val_loss: 0.3097 - val_accuracy: 0.9000\n",
      "Epoch 836/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0282 - accuracy: 0.9991 - val_loss: 0.3096 - val_accuracy: 0.9000\n",
      "Epoch 837/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0280 - accuracy: 0.9991 - val_loss: 0.3105 - val_accuracy: 0.9000\n",
      "Epoch 838/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0279 - accuracy: 0.9991 - val_loss: 0.3112 - val_accuracy: 0.9000\n",
      "Epoch 839/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0277 - accuracy: 0.9991 - val_loss: 0.3107 - val_accuracy: 0.9000\n",
      "Epoch 840/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0275 - accuracy: 0.9991 - val_loss: 0.3119 - val_accuracy: 0.9000\n",
      "Epoch 841/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0275 - accuracy: 0.9991 - val_loss: 0.3113 - val_accuracy: 0.9000\n",
      "Epoch 842/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0273 - accuracy: 0.9991 - val_loss: 0.3128 - val_accuracy: 0.9000\n",
      "Epoch 843/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0271 - accuracy: 0.9991 - val_loss: 0.3123 - val_accuracy: 0.9000\n",
      "Epoch 844/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0270 - accuracy: 0.9991 - val_loss: 0.3132 - val_accuracy: 0.9000\n",
      "Epoch 845/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0269 - accuracy: 0.9991 - val_loss: 0.3129 - val_accuracy: 0.9000\n",
      "Epoch 846/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0267 - accuracy: 0.9991 - val_loss: 0.3144 - val_accuracy: 0.9000\n",
      "Epoch 847/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0266 - accuracy: 0.9991 - val_loss: 0.3142 - val_accuracy: 0.9000\n",
      "Epoch 848/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0265 - accuracy: 0.9991 - val_loss: 0.3148 - val_accuracy: 0.9000\n",
      "Epoch 849/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0263 - accuracy: 0.9991 - val_loss: 0.3153 - val_accuracy: 0.9000\n",
      "Epoch 850/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0261 - accuracy: 0.9991 - val_loss: 0.3157 - val_accuracy: 0.9000\n",
      "Epoch 851/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0261 - accuracy: 0.9991 - val_loss: 0.3156 - val_accuracy: 0.9000\n",
      "Epoch 852/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0259 - accuracy: 0.9991 - val_loss: 0.3158 - val_accuracy: 0.9000\n",
      "Epoch 853/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0258 - accuracy: 0.9991 - val_loss: 0.3173 - val_accuracy: 0.9000\n",
      "Epoch 854/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0257 - accuracy: 0.9991 - val_loss: 0.3166 - val_accuracy: 0.9000\n",
      "Epoch 855/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0255 - accuracy: 0.9991 - val_loss: 0.3173 - val_accuracy: 0.9000\n",
      "Epoch 856/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0254 - accuracy: 0.9991 - val_loss: 0.3178 - val_accuracy: 0.9000\n",
      "Epoch 857/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0253 - accuracy: 0.9991 - val_loss: 0.3165 - val_accuracy: 0.9000\n",
      "Epoch 858/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0252 - accuracy: 0.9991 - val_loss: 0.3186 - val_accuracy: 0.9000\n",
      "Epoch 859/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0250 - accuracy: 0.9991 - val_loss: 0.3181 - val_accuracy: 0.9000\n",
      "Epoch 860/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0249 - accuracy: 0.9991 - val_loss: 0.3188 - val_accuracy: 0.9000\n",
      "Epoch 861/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0248 - accuracy: 0.9991 - val_loss: 0.3194 - val_accuracy: 0.9000\n",
      "Epoch 862/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0247 - accuracy: 0.9991 - val_loss: 0.3190 - val_accuracy: 0.9000\n",
      "Epoch 863/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0246 - accuracy: 0.9991 - val_loss: 0.3195 - val_accuracy: 0.9000\n",
      "Epoch 864/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0245 - accuracy: 0.9991 - val_loss: 0.3193 - val_accuracy: 0.9000\n",
      "Epoch 865/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0243 - accuracy: 0.9991 - val_loss: 0.3204 - val_accuracy: 0.9000\n",
      "Epoch 866/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0242 - accuracy: 0.9991 - val_loss: 0.3202 - val_accuracy: 0.9000\n",
      "Epoch 867/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0241 - accuracy: 0.9991 - val_loss: 0.3204 - val_accuracy: 0.9000\n",
      "Epoch 868/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0240 - accuracy: 0.9991 - val_loss: 0.3213 - val_accuracy: 0.9000\n",
      "Epoch 869/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0238 - accuracy: 0.9991 - val_loss: 0.3201 - val_accuracy: 0.9000\n",
      "Epoch 870/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0237 - accuracy: 0.9991 - val_loss: 0.3217 - val_accuracy: 0.9000\n",
      "Epoch 871/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0236 - accuracy: 0.9991 - val_loss: 0.3211 - val_accuracy: 0.9000\n",
      "Epoch 872/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0235 - accuracy: 0.9991 - val_loss: 0.3216 - val_accuracy: 0.9000\n",
      "Epoch 873/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0234 - accuracy: 0.9991 - val_loss: 0.3217 - val_accuracy: 0.9000\n",
      "Epoch 874/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0233 - accuracy: 0.9991 - val_loss: 0.3208 - val_accuracy: 0.9000\n",
      "Epoch 875/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0231 - accuracy: 0.9991 - val_loss: 0.3230 - val_accuracy: 0.9000\n",
      "Epoch 876/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0230 - accuracy: 0.9991 - val_loss: 0.3218 - val_accuracy: 0.9000\n",
      "Epoch 877/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0229 - accuracy: 0.9991 - val_loss: 0.3218 - val_accuracy: 0.9000\n",
      "Epoch 878/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0228 - accuracy: 0.9991 - val_loss: 0.3230 - val_accuracy: 0.9000\n",
      "Epoch 879/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0227 - accuracy: 0.9991 - val_loss: 0.3226 - val_accuracy: 0.9000\n",
      "Epoch 880/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0226 - accuracy: 0.9991 - val_loss: 0.3232 - val_accuracy: 0.9000\n",
      "Epoch 881/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0225 - accuracy: 0.9991 - val_loss: 0.3222 - val_accuracy: 0.9000\n",
      "Epoch 882/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0224 - accuracy: 0.9991 - val_loss: 0.3231 - val_accuracy: 0.9000\n",
      "Epoch 883/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0223 - accuracy: 0.9991 - val_loss: 0.3229 - val_accuracy: 0.9000\n",
      "Epoch 884/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0222 - accuracy: 0.9991 - val_loss: 0.3229 - val_accuracy: 0.9000\n",
      "Epoch 885/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0221 - accuracy: 0.9991 - val_loss: 0.3240 - val_accuracy: 0.9000\n",
      "Epoch 886/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0220 - accuracy: 0.9991 - val_loss: 0.3230 - val_accuracy: 0.9000\n",
      "Epoch 887/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0219 - accuracy: 0.9991 - val_loss: 0.3231 - val_accuracy: 0.9000\n",
      "Epoch 888/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0218 - accuracy: 0.9991 - val_loss: 0.3240 - val_accuracy: 0.9000\n",
      "Epoch 889/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0217 - accuracy: 0.9991 - val_loss: 0.3237 - val_accuracy: 0.9000\n",
      "Epoch 890/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0216 - accuracy: 0.9991 - val_loss: 0.3236 - val_accuracy: 0.9083\n",
      "Epoch 891/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0215 - accuracy: 0.9991 - val_loss: 0.3235 - val_accuracy: 0.9083\n",
      "Epoch 892/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0214 - accuracy: 0.9991 - val_loss: 0.3233 - val_accuracy: 0.9083\n",
      "Epoch 893/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0213 - accuracy: 0.9991 - val_loss: 0.3240 - val_accuracy: 0.9083\n",
      "Epoch 894/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0212 - accuracy: 0.9991 - val_loss: 0.3236 - val_accuracy: 0.9083\n",
      "Epoch 895/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0211 - accuracy: 0.9991 - val_loss: 0.3241 - val_accuracy: 0.9083\n",
      "Epoch 896/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0210 - accuracy: 0.9991 - val_loss: 0.3239 - val_accuracy: 0.9083\n",
      "Epoch 897/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0209 - accuracy: 0.9991 - val_loss: 0.3242 - val_accuracy: 0.9083\n",
      "Epoch 898/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0209 - accuracy: 0.9991 - val_loss: 0.3241 - val_accuracy: 0.9083\n",
      "Epoch 899/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0207 - accuracy: 0.9991 - val_loss: 0.3249 - val_accuracy: 0.9083\n",
      "Epoch 900/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0207 - accuracy: 0.9991 - val_loss: 0.3249 - val_accuracy: 0.9083\n",
      "Epoch 901/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0206 - accuracy: 0.9991 - val_loss: 0.3242 - val_accuracy: 0.9083\n",
      "Epoch 902/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0204 - accuracy: 0.9991 - val_loss: 0.3252 - val_accuracy: 0.9083\n",
      "Epoch 903/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0204 - accuracy: 0.9991 - val_loss: 0.3250 - val_accuracy: 0.9083\n",
      "Epoch 904/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0203 - accuracy: 0.9991 - val_loss: 0.3246 - val_accuracy: 0.9083\n",
      "Epoch 905/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0202 - accuracy: 0.9991 - val_loss: 0.3248 - val_accuracy: 0.9083\n",
      "Epoch 906/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0201 - accuracy: 0.9991 - val_loss: 0.3249 - val_accuracy: 0.9083\n",
      "Epoch 907/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0200 - accuracy: 0.9991 - val_loss: 0.3255 - val_accuracy: 0.9083\n",
      "Epoch 908/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0199 - accuracy: 0.9991 - val_loss: 0.3258 - val_accuracy: 0.9083\n",
      "Epoch 909/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0199 - accuracy: 0.9991 - val_loss: 0.3257 - val_accuracy: 0.9083\n",
      "Epoch 910/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0197 - accuracy: 0.9991 - val_loss: 0.3259 - val_accuracy: 0.9083\n",
      "Epoch 911/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0196 - accuracy: 0.9991 - val_loss: 0.3260 - val_accuracy: 0.9083\n",
      "Epoch 912/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0196 - accuracy: 0.9991 - val_loss: 0.3262 - val_accuracy: 0.9083\n",
      "Epoch 913/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0195 - accuracy: 0.9991 - val_loss: 0.3255 - val_accuracy: 0.9083\n",
      "Epoch 914/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0194 - accuracy: 0.9991 - val_loss: 0.3261 - val_accuracy: 0.9083\n",
      "Epoch 915/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0193 - accuracy: 0.9991 - val_loss: 0.3269 - val_accuracy: 0.9083\n",
      "Epoch 916/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0192 - accuracy: 0.9991 - val_loss: 0.3263 - val_accuracy: 0.9083\n",
      "Epoch 917/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0191 - accuracy: 0.9991 - val_loss: 0.3264 - val_accuracy: 0.9083\n",
      "Epoch 918/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0190 - accuracy: 0.9991 - val_loss: 0.3263 - val_accuracy: 0.9000\n",
      "Epoch 919/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0190 - accuracy: 0.9991 - val_loss: 0.3265 - val_accuracy: 0.9083\n",
      "Epoch 920/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0189 - accuracy: 0.9991 - val_loss: 0.3276 - val_accuracy: 0.9000\n",
      "Epoch 921/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0187 - accuracy: 0.9991 - val_loss: 0.3268 - val_accuracy: 0.9000\n",
      "Epoch 922/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0187 - accuracy: 0.9991 - val_loss: 0.3264 - val_accuracy: 0.9000\n",
      "Epoch 923/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0186 - accuracy: 0.9991 - val_loss: 0.3271 - val_accuracy: 0.9000\n",
      "Epoch 924/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0185 - accuracy: 0.9991 - val_loss: 0.3278 - val_accuracy: 0.9000\n",
      "Epoch 925/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0184 - accuracy: 0.9991 - val_loss: 0.3282 - val_accuracy: 0.9000\n",
      "Epoch 926/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0183 - accuracy: 0.9991 - val_loss: 0.3276 - val_accuracy: 0.9000\n",
      "Epoch 927/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0182 - accuracy: 0.9991 - val_loss: 0.3278 - val_accuracy: 0.9000\n",
      "Epoch 928/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0181 - accuracy: 0.9991 - val_loss: 0.3281 - val_accuracy: 0.9000\n",
      "Epoch 929/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0180 - accuracy: 0.9991 - val_loss: 0.3282 - val_accuracy: 0.9000\n",
      "Epoch 930/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0180 - accuracy: 0.9991 - val_loss: 0.3288 - val_accuracy: 0.9000\n",
      "Epoch 931/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0178 - accuracy: 0.9991 - val_loss: 0.3290 - val_accuracy: 0.8917\n",
      "Epoch 932/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0177 - accuracy: 0.9991 - val_loss: 0.3289 - val_accuracy: 0.9000\n",
      "Epoch 933/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0177 - accuracy: 0.9991 - val_loss: 0.3271 - val_accuracy: 0.8917\n",
      "Epoch 934/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0175 - accuracy: 0.9991 - val_loss: 0.3282 - val_accuracy: 0.8917\n",
      "Epoch 935/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0175 - accuracy: 0.9991 - val_loss: 0.3286 - val_accuracy: 0.8917\n",
      "Epoch 936/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0174 - accuracy: 0.9991 - val_loss: 0.3296 - val_accuracy: 0.8917\n",
      "Epoch 937/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0173 - accuracy: 0.9991 - val_loss: 0.3294 - val_accuracy: 0.8917\n",
      "Epoch 938/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0172 - accuracy: 0.9991 - val_loss: 0.3295 - val_accuracy: 0.8917\n",
      "Epoch 939/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0171 - accuracy: 0.9991 - val_loss: 0.3288 - val_accuracy: 0.8917\n",
      "Epoch 940/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0170 - accuracy: 0.9991 - val_loss: 0.3285 - val_accuracy: 0.8917\n",
      "Epoch 941/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0169 - accuracy: 0.9991 - val_loss: 0.3300 - val_accuracy: 0.8833\n",
      "Epoch 942/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0168 - accuracy: 0.9991 - val_loss: 0.3303 - val_accuracy: 0.8833\n",
      "Epoch 943/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0167 - accuracy: 0.9991 - val_loss: 0.3318 - val_accuracy: 0.8833\n",
      "Epoch 944/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0166 - accuracy: 0.9991 - val_loss: 0.3296 - val_accuracy: 0.8833\n",
      "Epoch 945/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0166 - accuracy: 0.9991 - val_loss: 0.3301 - val_accuracy: 0.8833\n",
      "Epoch 946/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0165 - accuracy: 0.9991 - val_loss: 0.3320 - val_accuracy: 0.8833\n",
      "Epoch 947/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0164 - accuracy: 0.9991 - val_loss: 0.3315 - val_accuracy: 0.8833\n",
      "Epoch 948/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0163 - accuracy: 0.9991 - val_loss: 0.3315 - val_accuracy: 0.8833\n",
      "Epoch 949/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0162 - accuracy: 0.9991 - val_loss: 0.3316 - val_accuracy: 0.8833\n",
      "Epoch 950/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0161 - accuracy: 0.9991 - val_loss: 0.3312 - val_accuracy: 0.8833\n",
      "Epoch 951/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0160 - accuracy: 0.9991 - val_loss: 0.3339 - val_accuracy: 0.8833\n",
      "Epoch 952/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0159 - accuracy: 0.9991 - val_loss: 0.3329 - val_accuracy: 0.8833\n",
      "Epoch 953/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0158 - accuracy: 0.9991 - val_loss: 0.3326 - val_accuracy: 0.8833\n",
      "Epoch 954/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0157 - accuracy: 0.9991 - val_loss: 0.3322 - val_accuracy: 0.8833\n",
      "Epoch 955/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0157 - accuracy: 0.9991 - val_loss: 0.3338 - val_accuracy: 0.8833\n",
      "Epoch 956/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0155 - accuracy: 0.9991 - val_loss: 0.3342 - val_accuracy: 0.8833\n",
      "Epoch 957/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0155 - accuracy: 0.9991 - val_loss: 0.3348 - val_accuracy: 0.8833\n",
      "Epoch 958/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0154 - accuracy: 0.9991 - val_loss: 0.3346 - val_accuracy: 0.8833\n",
      "Epoch 959/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0153 - accuracy: 0.9991 - val_loss: 0.3347 - val_accuracy: 0.8833\n",
      "Epoch 960/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0152 - accuracy: 0.9991 - val_loss: 0.3356 - val_accuracy: 0.8833\n",
      "Epoch 961/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0151 - accuracy: 0.9991 - val_loss: 0.3357 - val_accuracy: 0.8833\n",
      "Epoch 962/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0150 - accuracy: 0.9991 - val_loss: 0.3361 - val_accuracy: 0.8917\n",
      "Epoch 963/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0150 - accuracy: 0.9991 - val_loss: 0.3359 - val_accuracy: 0.8917\n",
      "Epoch 964/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0148 - accuracy: 0.9991 - val_loss: 0.3360 - val_accuracy: 0.8917\n",
      "Epoch 965/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0148 - accuracy: 0.9991 - val_loss: 0.3379 - val_accuracy: 0.8917\n",
      "Epoch 966/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0147 - accuracy: 0.9991 - val_loss: 0.3370 - val_accuracy: 0.8917\n",
      "Epoch 967/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0146 - accuracy: 0.9991 - val_loss: 0.3376 - val_accuracy: 0.8917\n",
      "Epoch 968/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0145 - accuracy: 0.9991 - val_loss: 0.3380 - val_accuracy: 0.8917\n",
      "Epoch 969/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0144 - accuracy: 0.9991 - val_loss: 0.3382 - val_accuracy: 0.8917\n",
      "Epoch 970/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0144 - accuracy: 0.9991 - val_loss: 0.3399 - val_accuracy: 0.8917\n",
      "Epoch 971/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0143 - accuracy: 0.9991 - val_loss: 0.3398 - val_accuracy: 0.8833\n",
      "Epoch 972/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0142 - accuracy: 0.9991 - val_loss: 0.3393 - val_accuracy: 0.8833\n",
      "Epoch 973/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0141 - accuracy: 0.9991 - val_loss: 0.3397 - val_accuracy: 0.8833\n",
      "Epoch 974/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0141 - accuracy: 0.9991 - val_loss: 0.3413 - val_accuracy: 0.8833\n",
      "Epoch 975/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0140 - accuracy: 0.9991 - val_loss: 0.3422 - val_accuracy: 0.8833\n",
      "Epoch 976/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0139 - accuracy: 0.9991 - val_loss: 0.3416 - val_accuracy: 0.8833\n",
      "Epoch 977/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0139 - accuracy: 0.9991 - val_loss: 0.3434 - val_accuracy: 0.8833\n",
      "Epoch 978/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0138 - accuracy: 0.9991 - val_loss: 0.3425 - val_accuracy: 0.8833\n",
      "Epoch 979/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0137 - accuracy: 0.9991 - val_loss: 0.3431 - val_accuracy: 0.8833\n",
      "Epoch 980/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0137 - accuracy: 0.9991 - val_loss: 0.3442 - val_accuracy: 0.8833\n",
      "Epoch 981/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0136 - accuracy: 0.9991 - val_loss: 0.3455 - val_accuracy: 0.8833\n",
      "Epoch 982/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0135 - accuracy: 0.9991 - val_loss: 0.3458 - val_accuracy: 0.8833\n",
      "Epoch 983/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0135 - accuracy: 0.9991 - val_loss: 0.3467 - val_accuracy: 0.8833\n",
      "Epoch 984/2000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.0134 - accuracy: 0.9991 - val_loss: 0.3466 - val_accuracy: 0.8833\n",
      "Epoch 985/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0133 - accuracy: 0.9991 - val_loss: 0.3468 - val_accuracy: 0.8833\n",
      "Epoch 986/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0133 - accuracy: 0.9991 - val_loss: 0.3490 - val_accuracy: 0.8833\n",
      "Epoch 987/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0132 - accuracy: 0.9991 - val_loss: 0.3485 - val_accuracy: 0.8833\n",
      "Epoch 988/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0131 - accuracy: 0.9991 - val_loss: 0.3502 - val_accuracy: 0.8833\n",
      "Epoch 989/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0131 - accuracy: 0.9991 - val_loss: 0.3499 - val_accuracy: 0.8833\n",
      "Epoch 990/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0130 - accuracy: 0.9991 - val_loss: 0.3510 - val_accuracy: 0.8833\n",
      "Epoch 991/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0130 - accuracy: 0.9991 - val_loss: 0.3522 - val_accuracy: 0.8833\n",
      "Epoch 992/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0129 - accuracy: 0.9991 - val_loss: 0.3521 - val_accuracy: 0.8833\n",
      "Epoch 993/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0128 - accuracy: 0.9991 - val_loss: 0.3526 - val_accuracy: 0.8833\n",
      "Epoch 994/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0128 - accuracy: 0.9991 - val_loss: 0.3549 - val_accuracy: 0.8833\n",
      "Epoch 995/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0127 - accuracy: 0.9991 - val_loss: 0.3532 - val_accuracy: 0.8833\n",
      "Epoch 996/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0127 - accuracy: 0.9991 - val_loss: 0.3554 - val_accuracy: 0.8833\n",
      "Epoch 997/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0127 - accuracy: 0.9991 - val_loss: 0.3553 - val_accuracy: 0.8833\n",
      "Epoch 998/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0126 - accuracy: 0.9991 - val_loss: 0.3557 - val_accuracy: 0.8833\n",
      "Epoch 999/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0125 - accuracy: 0.9991 - val_loss: 0.3583 - val_accuracy: 0.8833\n",
      "Epoch 1000/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0125 - accuracy: 0.9991 - val_loss: 0.3582 - val_accuracy: 0.8833\n",
      "Epoch 1001/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0124 - accuracy: 0.9991 - val_loss: 0.3590 - val_accuracy: 0.8833\n",
      "Epoch 1002/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0124 - accuracy: 0.9991 - val_loss: 0.3592 - val_accuracy: 0.8833\n",
      "Epoch 1003/2000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.0123 - accuracy: 0.9991 - val_loss: 0.3592 - val_accuracy: 0.8833\n",
      "Epoch 1004/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0122 - accuracy: 0.9991 - val_loss: 0.3609 - val_accuracy: 0.8833\n",
      "Epoch 1005/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0122 - accuracy: 0.9991 - val_loss: 0.3626 - val_accuracy: 0.8917\n",
      "Epoch 1006/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0122 - accuracy: 0.9991 - val_loss: 0.3619 - val_accuracy: 0.8833\n",
      "Epoch 1007/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0121 - accuracy: 0.9991 - val_loss: 0.3643 - val_accuracy: 0.8833\n",
      "Epoch 1008/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0121 - accuracy: 0.9991 - val_loss: 0.3640 - val_accuracy: 0.8833\n",
      "Epoch 1009/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0120 - accuracy: 0.9991 - val_loss: 0.3657 - val_accuracy: 0.8917\n",
      "Epoch 1010/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0119 - accuracy: 0.9991 - val_loss: 0.3665 - val_accuracy: 0.8833\n",
      "Epoch 1011/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0119 - accuracy: 0.9991 - val_loss: 0.3685 - val_accuracy: 0.8917\n",
      "Epoch 1012/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0118 - accuracy: 0.9991 - val_loss: 0.3670 - val_accuracy: 0.8917\n",
      "Epoch 1013/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0118 - accuracy: 0.9991 - val_loss: 0.3690 - val_accuracy: 0.8917\n",
      "Epoch 1014/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0117 - accuracy: 0.9991 - val_loss: 0.3684 - val_accuracy: 0.8833\n",
      "Epoch 1015/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0117 - accuracy: 0.9991 - val_loss: 0.3717 - val_accuracy: 0.8917\n",
      "Epoch 1016/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0116 - accuracy: 0.9991 - val_loss: 0.3720 - val_accuracy: 0.8917\n",
      "Epoch 1017/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0116 - accuracy: 0.9991 - val_loss: 0.3731 - val_accuracy: 0.8917\n",
      "Epoch 1018/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0115 - accuracy: 0.9991 - val_loss: 0.3734 - val_accuracy: 0.8833\n",
      "Epoch 1019/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0115 - accuracy: 0.9991 - val_loss: 0.3767 - val_accuracy: 0.8917\n",
      "Epoch 1020/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0114 - accuracy: 0.9991 - val_loss: 0.3750 - val_accuracy: 0.8917\n",
      "Epoch 1021/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0114 - accuracy: 0.9991 - val_loss: 0.3773 - val_accuracy: 0.8917\n",
      "Epoch 1022/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0113 - accuracy: 0.9991 - val_loss: 0.3775 - val_accuracy: 0.8917\n",
      "Epoch 1023/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0113 - accuracy: 0.9991 - val_loss: 0.3791 - val_accuracy: 0.8917\n",
      "Epoch 1024/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0112 - accuracy: 0.9991 - val_loss: 0.3800 - val_accuracy: 0.8917\n",
      "Epoch 1025/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0111 - accuracy: 0.9991 - val_loss: 0.3833 - val_accuracy: 0.8917\n",
      "Epoch 1026/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0111 - accuracy: 0.9991 - val_loss: 0.3819 - val_accuracy: 0.8917\n",
      "Epoch 1027/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.3852 - val_accuracy: 0.8917\n",
      "Epoch 1028/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.3847 - val_accuracy: 0.8917\n",
      "Epoch 1029/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0109 - accuracy: 0.9991 - val_loss: 0.3870 - val_accuracy: 0.8917\n",
      "Epoch 1030/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.3873 - val_accuracy: 0.8917\n",
      "Epoch 1031/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.3888 - val_accuracy: 0.8917\n",
      "Epoch 1032/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.3904 - val_accuracy: 0.8917\n",
      "Epoch 1033/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.3910 - val_accuracy: 0.8833\n",
      "Epoch 1034/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.3940 - val_accuracy: 0.9000\n",
      "Epoch 1035/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.3923 - val_accuracy: 0.9000\n",
      "Epoch 1036/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.3953 - val_accuracy: 0.9000\n",
      "Epoch 1037/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.3957 - val_accuracy: 0.9000\n",
      "Epoch 1038/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.3970 - val_accuracy: 0.9000\n",
      "Epoch 1039/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.3986 - val_accuracy: 0.9000\n",
      "Epoch 1040/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.3991 - val_accuracy: 0.9000\n",
      "Epoch 1041/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.4002 - val_accuracy: 0.9000\n",
      "Epoch 1042/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.3991 - val_accuracy: 0.9000\n",
      "Epoch 1043/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.4020 - val_accuracy: 0.8917\n",
      "Epoch 1044/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.4033 - val_accuracy: 0.9083\n",
      "Epoch 1045/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.4034 - val_accuracy: 0.8917\n",
      "Epoch 1046/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.4047 - val_accuracy: 0.8917\n",
      "Epoch 1047/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.4050 - val_accuracy: 0.9000\n",
      "Epoch 1048/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.4035 - val_accuracy: 0.8917\n",
      "Epoch 1049/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.4047 - val_accuracy: 0.8917\n",
      "Epoch 1050/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.4045 - val_accuracy: 0.8917\n",
      "Epoch 1051/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.4041 - val_accuracy: 0.8917\n",
      "Epoch 1052/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.4023 - val_accuracy: 0.8917\n",
      "Epoch 1053/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.4033 - val_accuracy: 0.8917\n",
      "Epoch 1054/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.4032 - val_accuracy: 0.8917\n",
      "Epoch 1055/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.4028 - val_accuracy: 0.8833\n",
      "Epoch 1056/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.4019 - val_accuracy: 0.8833\n",
      "Epoch 1057/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.4025 - val_accuracy: 0.8833\n",
      "Epoch 1058/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.4008 - val_accuracy: 0.8917\n",
      "Epoch 1059/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.4003 - val_accuracy: 0.8917\n",
      "Epoch 1060/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.4005 - val_accuracy: 0.8917\n",
      "Epoch 1061/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.3993 - val_accuracy: 0.8917\n",
      "Epoch 1062/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.3977 - val_accuracy: 0.8917\n",
      "Epoch 1063/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.3971 - val_accuracy: 0.8917\n",
      "Epoch 1064/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.3956 - val_accuracy: 0.8917\n",
      "Epoch 1065/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.3957 - val_accuracy: 0.8917\n",
      "Epoch 1066/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3933 - val_accuracy: 0.8917\n",
      "Epoch 1067/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3920 - val_accuracy: 0.8917\n",
      "Epoch 1068/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3912 - val_accuracy: 0.8917\n",
      "Epoch 1069/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.3893 - val_accuracy: 0.8917\n",
      "Epoch 1070/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.3878 - val_accuracy: 0.8917\n",
      "Epoch 1071/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.3861 - val_accuracy: 0.8833\n",
      "Epoch 1072/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.3847 - val_accuracy: 0.8833\n",
      "Epoch 1073/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.3829 - val_accuracy: 0.8833\n",
      "Epoch 1074/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.3810 - val_accuracy: 0.8833\n",
      "Epoch 1075/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.3802 - val_accuracy: 0.8833\n",
      "Epoch 1076/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.3794 - val_accuracy: 0.8833\n",
      "Epoch 1077/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.3770 - val_accuracy: 0.8833\n",
      "Epoch 1078/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.3762 - val_accuracy: 0.8833\n",
      "Epoch 1079/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.3747 - val_accuracy: 0.8833\n",
      "Epoch 1080/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.3739 - val_accuracy: 0.8833\n",
      "Epoch 1081/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.3743 - val_accuracy: 0.8917\n",
      "Epoch 1082/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.3715 - val_accuracy: 0.8917\n",
      "Epoch 1083/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.3716 - val_accuracy: 0.8917\n",
      "Epoch 1084/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.3732 - val_accuracy: 0.8917\n",
      "Epoch 1085/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.3717 - val_accuracy: 0.8917\n",
      "Epoch 1086/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.3708 - val_accuracy: 0.8917\n",
      "Epoch 1087/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.3678 - val_accuracy: 0.8917\n",
      "Epoch 1088/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3685 - val_accuracy: 0.8917\n",
      "Epoch 1089/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3684 - val_accuracy: 0.8917\n",
      "Epoch 1090/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3697 - val_accuracy: 0.8917\n",
      "Epoch 1091/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.3688 - val_accuracy: 0.8917\n",
      "Epoch 1092/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.3671 - val_accuracy: 0.8917\n",
      "Epoch 1093/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.3672 - val_accuracy: 0.8917\n",
      "Epoch 1094/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.3676 - val_accuracy: 0.8917\n",
      "Epoch 1095/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.3681 - val_accuracy: 0.8917\n",
      "Epoch 1096/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.3677 - val_accuracy: 0.8917\n",
      "Epoch 1097/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.3675 - val_accuracy: 0.8917\n",
      "Epoch 1098/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.3677 - val_accuracy: 0.9000\n",
      "Epoch 1099/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.3685 - val_accuracy: 0.8917\n",
      "Epoch 1100/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.3698 - val_accuracy: 0.9000\n",
      "Epoch 1101/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.3700 - val_accuracy: 0.9000\n",
      "Epoch 1102/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.3698 - val_accuracy: 0.9000\n",
      "Epoch 1103/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.3685 - val_accuracy: 0.9000\n",
      "Epoch 1104/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3681 - val_accuracy: 0.9000\n",
      "Epoch 1105/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3705 - val_accuracy: 0.9000\n",
      "Epoch 1106/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3723 - val_accuracy: 0.9000\n",
      "Epoch 1107/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3718 - val_accuracy: 0.9000\n",
      "Epoch 1108/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3715 - val_accuracy: 0.9000\n",
      "Epoch 1109/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.3710 - val_accuracy: 0.9000\n",
      "Epoch 1110/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.3718 - val_accuracy: 0.9000\n",
      "Epoch 1111/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.3701 - val_accuracy: 0.9000\n",
      "Epoch 1112/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.3679 - val_accuracy: 0.9000\n",
      "Epoch 1113/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.3703 - val_accuracy: 0.9000\n",
      "Epoch 1114/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.3693 - val_accuracy: 0.9000\n",
      "Epoch 1115/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.3706 - val_accuracy: 0.9000\n",
      "Epoch 1116/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.3680 - val_accuracy: 0.9000\n",
      "Epoch 1117/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.3643 - val_accuracy: 0.9000\n",
      "Epoch 1118/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.3664 - val_accuracy: 0.9000\n",
      "Epoch 1119/2000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3647 - val_accuracy: 0.9083\n",
      "Epoch 1120/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3671 - val_accuracy: 0.9000\n",
      "Epoch 1121/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.3649 - val_accuracy: 0.9083\n",
      "Epoch 1122/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.3680 - val_accuracy: 0.9000\n",
      "Epoch 1123/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.3674 - val_accuracy: 0.9000\n",
      "Epoch 1124/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.3698 - val_accuracy: 0.9000\n",
      "Epoch 1125/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.3690 - val_accuracy: 0.8917\n",
      "Epoch 1126/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.3716 - val_accuracy: 0.8917\n",
      "Epoch 1127/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.3687 - val_accuracy: 0.8917\n",
      "Epoch 1128/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.3689 - val_accuracy: 0.9000\n",
      "Epoch 1129/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.3677 - val_accuracy: 0.9000\n",
      "Epoch 1130/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.3672 - val_accuracy: 0.8917\n",
      "Epoch 1131/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.3650 - val_accuracy: 0.8917\n",
      "Epoch 1132/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.3670 - val_accuracy: 0.8917\n",
      "Epoch 1133/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.3699 - val_accuracy: 0.8833\n",
      "Epoch 1134/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.3859 - val_accuracy: 0.8833\n",
      "Epoch 1135/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.4082 - val_accuracy: 0.8833\n",
      "Epoch 1136/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.4278 - val_accuracy: 0.8833\n",
      "Epoch 1137/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.4114 - val_accuracy: 0.8833\n",
      "Epoch 1138/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.3885 - val_accuracy: 0.8917\n",
      "Epoch 1139/2000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.0156 - accuracy: 0.9981 - val_loss: 0.4010 - val_accuracy: 0.9000\n",
      "Epoch 1140/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0172 - accuracy: 0.9991 - val_loss: 0.3476 - val_accuracy: 0.8917\n",
      "Epoch 1141/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0167 - accuracy: 0.9991 - val_loss: 0.4096 - val_accuracy: 0.9000\n",
      "Epoch 1142/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0132 - accuracy: 0.9991 - val_loss: 0.4066 - val_accuracy: 0.9000\n",
      "Epoch 1143/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.4406 - val_accuracy: 0.9000\n",
      "Epoch 1144/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0135 - accuracy: 0.9991 - val_loss: 0.4076 - val_accuracy: 0.8917\n",
      "Epoch 1145/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0128 - accuracy: 0.9991 - val_loss: 0.4219 - val_accuracy: 0.9083\n",
      "Epoch 1146/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0148 - accuracy: 0.9991 - val_loss: 0.3937 - val_accuracy: 0.9000\n",
      "Epoch 1147/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0144 - accuracy: 0.9991 - val_loss: 0.3922 - val_accuracy: 0.9083\n",
      "Epoch 1148/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.4290 - val_accuracy: 0.9083\n",
      "Epoch 1149/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0144 - accuracy: 0.9991 - val_loss: 0.4760 - val_accuracy: 0.8833\n",
      "Epoch 1150/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0172 - accuracy: 0.9991 - val_loss: 0.4457 - val_accuracy: 0.9083\n",
      "Epoch 1151/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.5064 - val_accuracy: 0.8750\n",
      "Epoch 1152/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0187 - accuracy: 0.9991 - val_loss: 0.5322 - val_accuracy: 0.8750\n",
      "Epoch 1153/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0226 - accuracy: 0.9944 - val_loss: 0.4689 - val_accuracy: 0.8833\n",
      "Epoch 1154/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0182 - accuracy: 0.9972 - val_loss: 0.5581 - val_accuracy: 0.8667\n",
      "Epoch 1155/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0197 - accuracy: 0.9954 - val_loss: 0.5446 - val_accuracy: 0.8667\n",
      "Epoch 1156/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0201 - accuracy: 0.9972 - val_loss: 0.5222 - val_accuracy: 0.8417\n",
      "Epoch 1157/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.4094 - val_accuracy: 0.9000\n",
      "Epoch 1158/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.4062 - val_accuracy: 0.8917\n",
      "Epoch 1159/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0091 - accuracy: 0.9991 - val_loss: 0.4166 - val_accuracy: 0.8917\n",
      "Epoch 1160/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0090 - accuracy: 0.9991 - val_loss: 0.4320 - val_accuracy: 0.9000\n",
      "Epoch 1161/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0087 - accuracy: 0.9991 - val_loss: 0.4348 - val_accuracy: 0.8833\n",
      "Epoch 1162/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0089 - accuracy: 0.9991 - val_loss: 0.4382 - val_accuracy: 0.8833\n",
      "Epoch 1163/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0089 - accuracy: 0.9991 - val_loss: 0.4356 - val_accuracy: 0.8833\n",
      "Epoch 1164/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0091 - accuracy: 0.9991 - val_loss: 0.4378 - val_accuracy: 0.8917\n",
      "Epoch 1165/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0091 - accuracy: 0.9991 - val_loss: 0.4376 - val_accuracy: 0.8917\n",
      "Epoch 1166/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0093 - accuracy: 0.9991 - val_loss: 0.4397 - val_accuracy: 0.8917\n",
      "Epoch 1167/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0092 - accuracy: 0.9991 - val_loss: 0.4391 - val_accuracy: 0.8917\n",
      "Epoch 1168/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0095 - accuracy: 0.9991 - val_loss: 0.4408 - val_accuracy: 0.8917\n",
      "Epoch 1169/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0093 - accuracy: 0.9991 - val_loss: 0.4400 - val_accuracy: 0.9000\n",
      "Epoch 1170/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0096 - accuracy: 0.9991 - val_loss: 0.4421 - val_accuracy: 0.8917\n",
      "Epoch 1171/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0095 - accuracy: 0.9991 - val_loss: 0.4422 - val_accuracy: 0.8833\n",
      "Epoch 1172/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0098 - accuracy: 0.9991 - val_loss: 0.4432 - val_accuracy: 0.8833\n",
      "Epoch 1173/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0097 - accuracy: 0.9991 - val_loss: 0.4431 - val_accuracy: 0.8833\n",
      "Epoch 1174/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0099 - accuracy: 0.9991 - val_loss: 0.4454 - val_accuracy: 0.8833\n",
      "Epoch 1175/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0099 - accuracy: 0.9991 - val_loss: 0.4457 - val_accuracy: 0.8833\n",
      "Epoch 1176/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0100 - accuracy: 0.9991 - val_loss: 0.4469 - val_accuracy: 0.8833\n",
      "Epoch 1177/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0101 - accuracy: 0.9991 - val_loss: 0.4480 - val_accuracy: 0.8833\n",
      "Epoch 1178/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0102 - accuracy: 0.9991 - val_loss: 0.4485 - val_accuracy: 0.8833\n",
      "Epoch 1179/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0104 - accuracy: 0.9991 - val_loss: 0.4512 - val_accuracy: 0.8833\n",
      "Epoch 1180/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0104 - accuracy: 0.9991 - val_loss: 0.4506 - val_accuracy: 0.8833\n",
      "Epoch 1181/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0106 - accuracy: 0.9991 - val_loss: 0.4539 - val_accuracy: 0.8833\n",
      "Epoch 1182/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0106 - accuracy: 0.9991 - val_loss: 0.4554 - val_accuracy: 0.8833\n",
      "Epoch 1183/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0108 - accuracy: 0.9991 - val_loss: 0.4560 - val_accuracy: 0.8833\n",
      "Epoch 1184/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0109 - accuracy: 0.9991 - val_loss: 0.4581 - val_accuracy: 0.8917\n",
      "Epoch 1185/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0110 - accuracy: 0.9991 - val_loss: 0.4593 - val_accuracy: 0.8917\n",
      "Epoch 1186/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0113 - accuracy: 0.9991 - val_loss: 0.4625 - val_accuracy: 0.8917\n",
      "Epoch 1187/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0111 - accuracy: 0.9991 - val_loss: 0.4619 - val_accuracy: 0.8917\n",
      "Epoch 1188/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0117 - accuracy: 0.9991 - val_loss: 0.4664 - val_accuracy: 0.8917\n",
      "Epoch 1189/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0113 - accuracy: 0.9991 - val_loss: 0.4672 - val_accuracy: 0.8917\n",
      "Epoch 1190/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0120 - accuracy: 0.9981 - val_loss: 0.4695 - val_accuracy: 0.8917\n",
      "Epoch 1191/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0118 - accuracy: 0.9981 - val_loss: 0.4725 - val_accuracy: 0.8917\n",
      "Epoch 1192/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0123 - accuracy: 0.9981 - val_loss: 0.4744 - val_accuracy: 0.9000\n",
      "Epoch 1193/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0121 - accuracy: 0.9981 - val_loss: 0.4787 - val_accuracy: 0.9000\n",
      "Epoch 1194/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0126 - accuracy: 0.9981 - val_loss: 0.4815 - val_accuracy: 0.9000\n",
      "Epoch 1195/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0127 - accuracy: 0.9981 - val_loss: 0.4887 - val_accuracy: 0.9000\n",
      "Epoch 1196/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0129 - accuracy: 0.9991 - val_loss: 0.4896 - val_accuracy: 0.9000\n",
      "Epoch 1197/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0131 - accuracy: 0.9991 - val_loss: 0.4963 - val_accuracy: 0.9000\n",
      "Epoch 1198/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0132 - accuracy: 0.9991 - val_loss: 0.5010 - val_accuracy: 0.9000\n",
      "Epoch 1199/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0133 - accuracy: 0.9991 - val_loss: 0.5063 - val_accuracy: 0.9000\n",
      "Epoch 1200/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0134 - accuracy: 0.9991 - val_loss: 0.5114 - val_accuracy: 0.9000\n",
      "Epoch 1201/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0136 - accuracy: 0.9991 - val_loss: 0.5140 - val_accuracy: 0.9000\n",
      "Epoch 1202/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0137 - accuracy: 0.9991 - val_loss: 0.5224 - val_accuracy: 0.9083\n",
      "Epoch 1203/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0139 - accuracy: 0.9991 - val_loss: 0.5208 - val_accuracy: 0.9000\n",
      "Epoch 1204/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0138 - accuracy: 0.9991 - val_loss: 0.5188 - val_accuracy: 0.9167\n",
      "Epoch 1205/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.5155 - val_accuracy: 0.9167\n",
      "Epoch 1206/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.5107 - val_accuracy: 0.9083\n",
      "Epoch 1207/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.5130 - val_accuracy: 0.9083\n",
      "Epoch 1208/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.5102 - val_accuracy: 0.9167\n",
      "Epoch 1209/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.4713 - val_accuracy: 0.9167\n",
      "Epoch 1210/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0170 - accuracy: 0.9972 - val_loss: 0.5010 - val_accuracy: 0.8917\n",
      "Epoch 1211/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0171 - accuracy: 0.9972 - val_loss: 0.4802 - val_accuracy: 0.9000\n",
      "Epoch 1212/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0143 - accuracy: 0.9981 - val_loss: 0.5367 - val_accuracy: 0.9000\n",
      "Epoch 1213/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.5299 - val_accuracy: 0.9000\n",
      "Epoch 1214/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0115 - accuracy: 0.9991 - val_loss: 0.5349 - val_accuracy: 0.8583\n",
      "Epoch 1215/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0126 - accuracy: 0.9981 - val_loss: 0.5333 - val_accuracy: 0.8667\n",
      "Epoch 1216/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.5452 - val_accuracy: 0.8750\n",
      "Epoch 1217/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.5464 - val_accuracy: 0.8750\n",
      "Epoch 1218/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.5446 - val_accuracy: 0.8667\n",
      "Epoch 1219/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.5452 - val_accuracy: 0.8667\n",
      "Epoch 1220/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.5569 - val_accuracy: 0.8583\n",
      "Epoch 1221/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.5631 - val_accuracy: 0.8583\n",
      "Epoch 1222/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.5652 - val_accuracy: 0.8667\n",
      "Epoch 1223/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.5687 - val_accuracy: 0.8667\n",
      "Epoch 1224/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.5694 - val_accuracy: 0.8667\n",
      "Epoch 1225/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.5722 - val_accuracy: 0.8667\n",
      "Epoch 1226/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.5736 - val_accuracy: 0.8583\n",
      "Epoch 1227/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.5744 - val_accuracy: 0.8583\n",
      "Epoch 1228/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.5771 - val_accuracy: 0.8583\n",
      "Epoch 1229/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.5769 - val_accuracy: 0.8583\n",
      "Epoch 1230/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.5803 - val_accuracy: 0.8667\n",
      "Epoch 1231/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.5805 - val_accuracy: 0.8667\n",
      "Epoch 1232/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.5842 - val_accuracy: 0.8667\n",
      "Epoch 1233/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.5838 - val_accuracy: 0.8667\n",
      "Epoch 1234/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.5873 - val_accuracy: 0.8667\n",
      "Epoch 1235/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.5941 - val_accuracy: 0.8583\n",
      "Epoch 1236/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.6011 - val_accuracy: 0.8500\n",
      "Epoch 1237/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.6027 - val_accuracy: 0.8417\n",
      "Epoch 1238/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.6173 - val_accuracy: 0.8417\n",
      "Epoch 1239/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.6206 - val_accuracy: 0.8417\n",
      "Epoch 1240/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.6258 - val_accuracy: 0.8417\n",
      "Epoch 1241/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.6344 - val_accuracy: 0.8417\n",
      "Epoch 1242/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.6432 - val_accuracy: 0.8417\n",
      "Epoch 1243/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.6450 - val_accuracy: 0.8417\n",
      "Epoch 1244/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.6505 - val_accuracy: 0.8417\n",
      "Epoch 1245/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.6520 - val_accuracy: 0.8417\n",
      "Epoch 1246/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.6565 - val_accuracy: 0.8500\n",
      "Epoch 1247/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.6558 - val_accuracy: 0.8583\n",
      "Epoch 1248/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0142 - accuracy: 0.9991 - val_loss: 0.6502 - val_accuracy: 0.8583\n",
      "Epoch 1249/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0149 - accuracy: 0.9991 - val_loss: 0.6486 - val_accuracy: 0.8583\n",
      "Epoch 1250/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0156 - accuracy: 0.9991 - val_loss: 0.6426 - val_accuracy: 0.8750\n",
      "Epoch 1251/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0163 - accuracy: 0.9981 - val_loss: 0.6289 - val_accuracy: 0.8750\n",
      "Epoch 1252/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0176 - accuracy: 0.9991 - val_loss: 0.6208 - val_accuracy: 0.8750\n",
      "Epoch 1253/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0191 - accuracy: 0.9981 - val_loss: 0.6199 - val_accuracy: 0.8833\n",
      "Epoch 1254/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0193 - accuracy: 0.9972 - val_loss: 0.6247 - val_accuracy: 0.8750\n",
      "Epoch 1255/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0204 - accuracy: 0.9963 - val_loss: 0.6081 - val_accuracy: 0.8750\n",
      "Epoch 1256/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0219 - accuracy: 0.9954 - val_loss: 0.6194 - val_accuracy: 0.8750\n",
      "Epoch 1257/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0219 - accuracy: 0.9954 - val_loss: 0.6163 - val_accuracy: 0.8750\n",
      "Epoch 1258/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0241 - accuracy: 0.9935 - val_loss: 0.6155 - val_accuracy: 0.8750\n",
      "Epoch 1259/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0246 - accuracy: 0.9926 - val_loss: 0.6211 - val_accuracy: 0.8750\n",
      "Epoch 1260/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0256 - accuracy: 0.9926 - val_loss: 0.6194 - val_accuracy: 0.8667\n",
      "Epoch 1261/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0276 - accuracy: 0.9926 - val_loss: 0.6255 - val_accuracy: 0.8583\n",
      "Epoch 1262/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0281 - accuracy: 0.9926 - val_loss: 0.6226 - val_accuracy: 0.8583\n",
      "Epoch 1263/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0293 - accuracy: 0.9917 - val_loss: 0.6295 - val_accuracy: 0.8583\n",
      "Epoch 1264/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0306 - accuracy: 0.9917 - val_loss: 0.6320 - val_accuracy: 0.8583\n",
      "Epoch 1265/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0317 - accuracy: 0.9898 - val_loss: 0.6339 - val_accuracy: 0.8583\n",
      "Epoch 1266/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0330 - accuracy: 0.9898 - val_loss: 0.6324 - val_accuracy: 0.8667\n",
      "Epoch 1267/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0346 - accuracy: 0.9889 - val_loss: 0.6367 - val_accuracy: 0.8750\n",
      "Epoch 1268/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0363 - accuracy: 0.9898 - val_loss: 0.6399 - val_accuracy: 0.8750\n",
      "Epoch 1269/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0388 - accuracy: 0.9889 - val_loss: 0.6428 - val_accuracy: 0.8750\n",
      "Epoch 1270/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0396 - accuracy: 0.9889 - val_loss: 0.6466 - val_accuracy: 0.8833\n",
      "Epoch 1271/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0414 - accuracy: 0.9861 - val_loss: 0.6730 - val_accuracy: 0.8667\n",
      "Epoch 1272/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0433 - accuracy: 0.9843 - val_loss: 0.6772 - val_accuracy: 0.8583\n",
      "Epoch 1273/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0444 - accuracy: 0.9843 - val_loss: 0.7548 - val_accuracy: 0.8667\n",
      "Epoch 1274/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0451 - accuracy: 0.9806 - val_loss: 0.7649 - val_accuracy: 0.8667\n",
      "Epoch 1275/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0497 - accuracy: 0.9787 - val_loss: 0.7318 - val_accuracy: 0.8500\n",
      "Epoch 1276/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0523 - accuracy: 0.9796 - val_loss: 0.7002 - val_accuracy: 0.8667\n",
      "Epoch 1277/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0552 - accuracy: 0.9731 - val_loss: 0.6601 - val_accuracy: 0.8583\n",
      "Epoch 1278/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0453 - accuracy: 0.9796 - val_loss: 0.5212 - val_accuracy: 0.8833\n",
      "Epoch 1279/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0454 - accuracy: 0.9843 - val_loss: 0.4992 - val_accuracy: 0.8917\n",
      "Epoch 1280/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0486 - accuracy: 0.9833 - val_loss: 0.4712 - val_accuracy: 0.8750\n",
      "Epoch 1281/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0510 - accuracy: 0.9796 - val_loss: 0.5126 - val_accuracy: 0.8833\n",
      "Epoch 1282/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0516 - accuracy: 0.9806 - val_loss: 0.5752 - val_accuracy: 0.8583\n",
      "Epoch 1283/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0519 - accuracy: 0.9806 - val_loss: 0.5209 - val_accuracy: 0.8583\n",
      "Epoch 1284/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0415 - accuracy: 0.9833 - val_loss: 0.5521 - val_accuracy: 0.8833\n",
      "Epoch 1285/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0354 - accuracy: 0.9880 - val_loss: 0.5209 - val_accuracy: 0.8667\n",
      "Epoch 1286/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0350 - accuracy: 0.9870 - val_loss: 0.4956 - val_accuracy: 0.8667\n",
      "Epoch 1287/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0408 - accuracy: 0.9806 - val_loss: 0.5288 - val_accuracy: 0.8583\n",
      "Epoch 1288/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0425 - accuracy: 0.9833 - val_loss: 0.5065 - val_accuracy: 0.8750\n",
      "Epoch 1289/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0472 - accuracy: 0.9806 - val_loss: 0.5098 - val_accuracy: 0.8750\n",
      "Epoch 1290/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0512 - accuracy: 0.9778 - val_loss: 0.4940 - val_accuracy: 0.8750\n",
      "Epoch 1291/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0551 - accuracy: 0.9750 - val_loss: 0.4848 - val_accuracy: 0.8667\n",
      "Epoch 1292/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0597 - accuracy: 0.9741 - val_loss: 0.4820 - val_accuracy: 0.8667\n",
      "Epoch 1293/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0638 - accuracy: 0.9759 - val_loss: 0.4830 - val_accuracy: 0.8583\n",
      "Epoch 1294/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0683 - accuracy: 0.9769 - val_loss: 0.5106 - val_accuracy: 0.8417\n",
      "Epoch 1295/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0787 - accuracy: 0.9722 - val_loss: 0.5471 - val_accuracy: 0.8417\n",
      "Epoch 1296/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0842 - accuracy: 0.9685 - val_loss: 0.6060 - val_accuracy: 0.8417\n",
      "Epoch 1297/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0765 - accuracy: 0.9694 - val_loss: 0.6492 - val_accuracy: 0.8417\n",
      "Epoch 1298/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0617 - accuracy: 0.9796 - val_loss: 0.6677 - val_accuracy: 0.8250\n",
      "Epoch 1299/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0412 - accuracy: 0.9861 - val_loss: 0.5051 - val_accuracy: 0.8500\n",
      "Epoch 1300/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0164 - accuracy: 0.9935 - val_loss: 0.5211 - val_accuracy: 0.8750\n",
      "Epoch 1301/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.5256 - val_accuracy: 0.8667\n",
      "Epoch 1302/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0079 - accuracy: 0.9991 - val_loss: 0.5001 - val_accuracy: 0.8833\n",
      "Epoch 1303/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.5046 - val_accuracy: 0.8667\n",
      "Epoch 1304/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.4948 - val_accuracy: 0.8750\n",
      "Epoch 1305/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.4954 - val_accuracy: 0.8750\n",
      "Epoch 1306/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.4930 - val_accuracy: 0.8750\n",
      "Epoch 1307/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.4915 - val_accuracy: 0.8750\n",
      "Epoch 1308/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.4904 - val_accuracy: 0.8750\n",
      "Epoch 1309/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.4891 - val_accuracy: 0.8750\n",
      "Epoch 1310/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.4878 - val_accuracy: 0.8750\n",
      "Epoch 1311/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.4867 - val_accuracy: 0.8667\n",
      "Epoch 1312/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.4857 - val_accuracy: 0.8667\n",
      "Epoch 1313/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.4847 - val_accuracy: 0.8667\n",
      "Epoch 1314/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.4835 - val_accuracy: 0.8667\n",
      "Epoch 1315/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.4826 - val_accuracy: 0.8667\n",
      "Epoch 1316/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.4817 - val_accuracy: 0.8667\n",
      "Epoch 1317/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.4806 - val_accuracy: 0.8667\n",
      "Epoch 1318/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.4798 - val_accuracy: 0.8667\n",
      "Epoch 1319/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.4789 - val_accuracy: 0.8667\n",
      "Epoch 1320/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.4782 - val_accuracy: 0.8667\n",
      "Epoch 1321/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.4773 - val_accuracy: 0.8667\n",
      "Epoch 1322/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.4767 - val_accuracy: 0.8667\n",
      "Epoch 1323/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.4760 - val_accuracy: 0.8667\n",
      "Epoch 1324/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.4755 - val_accuracy: 0.8667\n",
      "Epoch 1325/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.4748 - val_accuracy: 0.8667\n",
      "Epoch 1326/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.4743 - val_accuracy: 0.8667\n",
      "Epoch 1327/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.4737 - val_accuracy: 0.8667\n",
      "Epoch 1328/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.4733 - val_accuracy: 0.8667\n",
      "Epoch 1329/2000\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.4726 - val_accuracy: 0.8667\n",
      "Epoch 1330/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.4721 - val_accuracy: 0.8667\n",
      "Epoch 1331/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.4717 - val_accuracy: 0.8667\n",
      "Epoch 1332/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.4713 - val_accuracy: 0.8667\n",
      "Epoch 1333/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.4706 - val_accuracy: 0.8667\n",
      "Epoch 1334/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.4703 - val_accuracy: 0.8667\n",
      "Epoch 1335/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.4699 - val_accuracy: 0.8667\n",
      "Epoch 1336/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4694 - val_accuracy: 0.8667\n",
      "Epoch 1337/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4689 - val_accuracy: 0.8667\n",
      "Epoch 1338/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4686 - val_accuracy: 0.8667\n",
      "Epoch 1339/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4682 - val_accuracy: 0.8667\n",
      "Epoch 1340/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4679 - val_accuracy: 0.8667\n",
      "Epoch 1341/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.4673 - val_accuracy: 0.8667\n",
      "Epoch 1342/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.4670 - val_accuracy: 0.8667\n",
      "Epoch 1343/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.4670 - val_accuracy: 0.8667\n",
      "Epoch 1344/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.4666 - val_accuracy: 0.8667\n",
      "Epoch 1345/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.4661 - val_accuracy: 0.8667\n",
      "Epoch 1346/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4659 - val_accuracy: 0.8667\n",
      "Epoch 1347/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4659 - val_accuracy: 0.8667\n",
      "Epoch 1348/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4655 - val_accuracy: 0.8667\n",
      "Epoch 1349/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4652 - val_accuracy: 0.8667\n",
      "Epoch 1350/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4650 - val_accuracy: 0.8667\n",
      "Epoch 1351/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4649 - val_accuracy: 0.8667\n",
      "Epoch 1352/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.4647 - val_accuracy: 0.8667\n",
      "Epoch 1353/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.4644 - val_accuracy: 0.8667\n",
      "Epoch 1354/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.4644 - val_accuracy: 0.8667\n",
      "Epoch 1355/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.4642 - val_accuracy: 0.8667\n",
      "Epoch 1356/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.4641 - val_accuracy: 0.8667\n",
      "Epoch 1357/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.4640 - val_accuracy: 0.8667\n",
      "Epoch 1358/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4637 - val_accuracy: 0.8667\n",
      "Epoch 1359/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4637 - val_accuracy: 0.8667\n",
      "Epoch 1360/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4636 - val_accuracy: 0.8667\n",
      "Epoch 1361/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4635 - val_accuracy: 0.8667\n",
      "Epoch 1362/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4634 - val_accuracy: 0.8667\n",
      "Epoch 1363/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4632 - val_accuracy: 0.8667\n",
      "Epoch 1364/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4631 - val_accuracy: 0.8667\n",
      "Epoch 1365/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.4632 - val_accuracy: 0.8667\n",
      "Epoch 1366/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.4629 - val_accuracy: 0.8667\n",
      "Epoch 1367/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.4629 - val_accuracy: 0.8667\n",
      "Epoch 1368/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.4628 - val_accuracy: 0.8667\n",
      "Epoch 1369/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.4626 - val_accuracy: 0.8667\n",
      "Epoch 1370/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.4625 - val_accuracy: 0.8667\n",
      "Epoch 1371/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.4626 - val_accuracy: 0.8667\n",
      "Epoch 1372/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4625 - val_accuracy: 0.8667\n",
      "Epoch 1373/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4623 - val_accuracy: 0.8667\n",
      "Epoch 1374/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4623 - val_accuracy: 0.8667\n",
      "Epoch 1375/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4622 - val_accuracy: 0.8667\n",
      "Epoch 1376/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4624 - val_accuracy: 0.8667\n",
      "Epoch 1377/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4620 - val_accuracy: 0.8667\n",
      "Epoch 1378/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4621 - val_accuracy: 0.8667\n",
      "Epoch 1379/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4620 - val_accuracy: 0.8667\n",
      "Epoch 1380/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4619 - val_accuracy: 0.8667\n",
      "Epoch 1381/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4619 - val_accuracy: 0.8667\n",
      "Epoch 1382/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4617 - val_accuracy: 0.8667\n",
      "Epoch 1383/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4620 - val_accuracy: 0.8667\n",
      "Epoch 1384/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4616 - val_accuracy: 0.8667\n",
      "Epoch 1385/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4618 - val_accuracy: 0.8667\n",
      "Epoch 1386/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4618 - val_accuracy: 0.8667\n",
      "Epoch 1387/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4616 - val_accuracy: 0.8667\n",
      "Epoch 1388/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4618 - val_accuracy: 0.8667\n",
      "Epoch 1389/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4615 - val_accuracy: 0.8667\n",
      "Epoch 1390/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4615 - val_accuracy: 0.8667\n",
      "Epoch 1391/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4614 - val_accuracy: 0.8667\n",
      "Epoch 1392/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4615 - val_accuracy: 0.8667\n",
      "Epoch 1393/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4616 - val_accuracy: 0.8667\n",
      "Epoch 1394/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4613 - val_accuracy: 0.8667\n",
      "Epoch 1395/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4613 - val_accuracy: 0.8667\n",
      "Epoch 1396/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4613 - val_accuracy: 0.8667\n",
      "Epoch 1397/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.4613 - val_accuracy: 0.8667\n",
      "Epoch 1398/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.4610 - val_accuracy: 0.8667\n",
      "Epoch 1399/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.4613 - val_accuracy: 0.8667\n",
      "Epoch 1400/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.4609 - val_accuracy: 0.8667\n",
      "Epoch 1401/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.4611 - val_accuracy: 0.8667\n",
      "Epoch 1402/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.4609 - val_accuracy: 0.8667\n",
      "Epoch 1403/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.4609 - val_accuracy: 0.8667\n",
      "Epoch 1404/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.4610 - val_accuracy: 0.8667\n",
      "Epoch 1405/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.4609 - val_accuracy: 0.8667\n",
      "Epoch 1406/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4607 - val_accuracy: 0.8667\n",
      "Epoch 1407/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4609 - val_accuracy: 0.8667\n",
      "Epoch 1408/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4607 - val_accuracy: 0.8667\n",
      "Epoch 1409/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4606 - val_accuracy: 0.8667\n",
      "Epoch 1410/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4607 - val_accuracy: 0.8667\n",
      "Epoch 1411/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4605 - val_accuracy: 0.8667\n",
      "Epoch 1412/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4605 - val_accuracy: 0.8667\n",
      "Epoch 1413/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4605 - val_accuracy: 0.8667\n",
      "Epoch 1414/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4603 - val_accuracy: 0.8667\n",
      "Epoch 1415/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4602 - val_accuracy: 0.8667\n",
      "Epoch 1416/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4601 - val_accuracy: 0.8667\n",
      "Epoch 1417/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4603 - val_accuracy: 0.8667\n",
      "Epoch 1418/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4601 - val_accuracy: 0.8667\n",
      "Epoch 1419/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4600 - val_accuracy: 0.8667\n",
      "Epoch 1420/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4601 - val_accuracy: 0.8667\n",
      "Epoch 1421/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4599 - val_accuracy: 0.8667\n",
      "Epoch 1422/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4600 - val_accuracy: 0.8667\n",
      "Epoch 1423/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4600 - val_accuracy: 0.8667\n",
      "Epoch 1424/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4599 - val_accuracy: 0.8667\n",
      "Epoch 1425/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4598 - val_accuracy: 0.8667\n",
      "Epoch 1426/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4599 - val_accuracy: 0.8667\n",
      "Epoch 1427/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.8667\n",
      "Epoch 1428/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.8667\n",
      "Epoch 1429/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4599 - val_accuracy: 0.8667\n",
      "Epoch 1430/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.8667\n",
      "Epoch 1431/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.8667\n",
      "Epoch 1432/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4598 - val_accuracy: 0.8667\n",
      "Epoch 1433/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.8667\n",
      "Epoch 1434/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.8667\n",
      "Epoch 1435/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.8667\n",
      "Epoch 1436/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.8667\n",
      "Epoch 1437/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.8667\n",
      "Epoch 1438/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4599 - val_accuracy: 0.8667\n",
      "Epoch 1439/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4599 - val_accuracy: 0.8667\n",
      "Epoch 1440/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.8667\n",
      "Epoch 1441/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4598 - val_accuracy: 0.8667\n",
      "Epoch 1442/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4596 - val_accuracy: 0.8667\n",
      "Epoch 1443/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4594 - val_accuracy: 0.8667\n",
      "Epoch 1444/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4593 - val_accuracy: 0.8667\n",
      "Epoch 1445/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4594 - val_accuracy: 0.8667\n",
      "Epoch 1446/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4591 - val_accuracy: 0.8667\n",
      "Epoch 1447/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4590 - val_accuracy: 0.8667\n",
      "Epoch 1448/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4590 - val_accuracy: 0.8667\n",
      "Epoch 1449/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4590 - val_accuracy: 0.8667\n",
      "Epoch 1450/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4587 - val_accuracy: 0.8667\n",
      "Epoch 1451/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4585 - val_accuracy: 0.8667\n",
      "Epoch 1452/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4587 - val_accuracy: 0.8667\n",
      "Epoch 1453/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4584 - val_accuracy: 0.8667\n",
      "Epoch 1454/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4584 - val_accuracy: 0.8667\n",
      "Epoch 1455/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4582 - val_accuracy: 0.8667\n",
      "Epoch 1456/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4583 - val_accuracy: 0.8667\n",
      "Epoch 1457/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4581 - val_accuracy: 0.8667\n",
      "Epoch 1458/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4581 - val_accuracy: 0.8667\n",
      "Epoch 1459/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4580 - val_accuracy: 0.8667\n",
      "Epoch 1460/2000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4581 - val_accuracy: 0.8667\n",
      "Epoch 1461/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4579 - val_accuracy: 0.8667\n",
      "Epoch 1462/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4578 - val_accuracy: 0.8667\n",
      "Epoch 1463/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4579 - val_accuracy: 0.8667\n",
      "Epoch 1464/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4577 - val_accuracy: 0.8667\n",
      "Epoch 1465/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4577 - val_accuracy: 0.8667\n",
      "Epoch 1466/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4575 - val_accuracy: 0.8667\n",
      "Epoch 1467/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4576 - val_accuracy: 0.8667\n",
      "Epoch 1468/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4575 - val_accuracy: 0.8667\n",
      "Epoch 1469/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4573 - val_accuracy: 0.8667\n",
      "Epoch 1470/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4572 - val_accuracy: 0.8667\n",
      "Epoch 1471/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4570 - val_accuracy: 0.8667\n",
      "Epoch 1472/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4571 - val_accuracy: 0.8667\n",
      "Epoch 1473/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4568 - val_accuracy: 0.8667\n",
      "Epoch 1474/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4568 - val_accuracy: 0.8667\n",
      "Epoch 1475/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4566 - val_accuracy: 0.8667\n",
      "Epoch 1476/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4568 - val_accuracy: 0.8667\n",
      "Epoch 1477/2000\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4565 - val_accuracy: 0.8667\n",
      "Epoch 1478/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4565 - val_accuracy: 0.8667\n",
      "Epoch 1479/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4563 - val_accuracy: 0.8667\n",
      "Epoch 1480/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4564 - val_accuracy: 0.8667\n",
      "Epoch 1481/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4562 - val_accuracy: 0.8667\n",
      "Epoch 1482/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4559 - val_accuracy: 0.8667\n",
      "Epoch 1483/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4558 - val_accuracy: 0.8667\n",
      "Epoch 1484/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4560 - val_accuracy: 0.8667\n",
      "Epoch 1485/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4557 - val_accuracy: 0.8667\n",
      "Epoch 1486/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4555 - val_accuracy: 0.8667\n",
      "Epoch 1487/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4558 - val_accuracy: 0.8667\n",
      "Epoch 1488/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4553 - val_accuracy: 0.8667\n",
      "Epoch 1489/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4551 - val_accuracy: 0.8667\n",
      "Epoch 1490/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4551 - val_accuracy: 0.8667\n",
      "Epoch 1491/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4550 - val_accuracy: 0.8667\n",
      "Epoch 1492/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4549 - val_accuracy: 0.8667\n",
      "Epoch 1493/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4546 - val_accuracy: 0.8667\n",
      "Epoch 1494/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4547 - val_accuracy: 0.8667\n",
      "Epoch 1495/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4545 - val_accuracy: 0.8667\n",
      "Epoch 1496/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4543 - val_accuracy: 0.8667\n",
      "Epoch 1497/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4544 - val_accuracy: 0.8667\n",
      "Epoch 1498/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4543 - val_accuracy: 0.8667\n",
      "Epoch 1499/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4542 - val_accuracy: 0.8667\n",
      "Epoch 1500/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4543 - val_accuracy: 0.8667\n",
      "Epoch 1501/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4541 - val_accuracy: 0.8667\n",
      "Epoch 1502/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4541 - val_accuracy: 0.8667\n",
      "Epoch 1503/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4540 - val_accuracy: 0.8667\n",
      "Epoch 1504/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4540 - val_accuracy: 0.8667\n",
      "Epoch 1505/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4541 - val_accuracy: 0.8667\n",
      "Epoch 1506/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4538 - val_accuracy: 0.8667\n",
      "Epoch 1507/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4537 - val_accuracy: 0.8667\n",
      "Epoch 1508/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4534 - val_accuracy: 0.8667\n",
      "Epoch 1509/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4539 - val_accuracy: 0.8667\n",
      "Epoch 1510/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4533 - val_accuracy: 0.8667\n",
      "Epoch 1511/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4535 - val_accuracy: 0.8667\n",
      "Epoch 1512/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4529 - val_accuracy: 0.8667\n",
      "Epoch 1513/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4532 - val_accuracy: 0.8667\n",
      "Epoch 1514/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4529 - val_accuracy: 0.8667\n",
      "Epoch 1515/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4527 - val_accuracy: 0.8667\n",
      "Epoch 1516/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4528 - val_accuracy: 0.8667\n",
      "Epoch 1517/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4525 - val_accuracy: 0.8667\n",
      "Epoch 1518/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4524 - val_accuracy: 0.8667\n",
      "Epoch 1519/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4520 - val_accuracy: 0.8667\n",
      "Epoch 1520/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4526 - val_accuracy: 0.8667\n",
      "Epoch 1521/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4517 - val_accuracy: 0.8667\n",
      "Epoch 1522/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4522 - val_accuracy: 0.8667\n",
      "Epoch 1523/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4517 - val_accuracy: 0.8667\n",
      "Epoch 1524/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4520 - val_accuracy: 0.8667\n",
      "Epoch 1525/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4517 - val_accuracy: 0.8667\n",
      "Epoch 1526/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4518 - val_accuracy: 0.8667\n",
      "Epoch 1527/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4513 - val_accuracy: 0.8667\n",
      "Epoch 1528/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4516 - val_accuracy: 0.8667\n",
      "Epoch 1529/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4516 - val_accuracy: 0.8667\n",
      "Epoch 1530/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4511 - val_accuracy: 0.8667\n",
      "Epoch 1531/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4513 - val_accuracy: 0.8667\n",
      "Epoch 1532/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4515 - val_accuracy: 0.8667\n",
      "Epoch 1533/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4506 - val_accuracy: 0.8667\n",
      "Epoch 1534/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4507 - val_accuracy: 0.8667\n",
      "Epoch 1535/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4508 - val_accuracy: 0.8667\n",
      "Epoch 1536/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4506 - val_accuracy: 0.8667\n",
      "Epoch 1537/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4506 - val_accuracy: 0.8667\n",
      "Epoch 1538/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4503 - val_accuracy: 0.8667\n",
      "Epoch 1539/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4506 - val_accuracy: 0.8667\n",
      "Epoch 1540/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4507 - val_accuracy: 0.8667\n",
      "Epoch 1541/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4498 - val_accuracy: 0.8667\n",
      "Epoch 1542/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4505 - val_accuracy: 0.8667\n",
      "Epoch 1543/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4499 - val_accuracy: 0.8667\n",
      "Epoch 1544/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4499 - val_accuracy: 0.8667\n",
      "Epoch 1545/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4499 - val_accuracy: 0.8667\n",
      "Epoch 1546/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4498 - val_accuracy: 0.8667\n",
      "Epoch 1547/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4501 - val_accuracy: 0.8667\n",
      "Epoch 1548/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4492 - val_accuracy: 0.8667\n",
      "Epoch 1549/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4494 - val_accuracy: 0.8667\n",
      "Epoch 1550/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4497 - val_accuracy: 0.8667\n",
      "Epoch 1551/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4492 - val_accuracy: 0.8667\n",
      "Epoch 1552/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4495 - val_accuracy: 0.8667\n",
      "Epoch 1553/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4488 - val_accuracy: 0.8667\n",
      "Epoch 1554/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4488 - val_accuracy: 0.8667\n",
      "Epoch 1555/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4489 - val_accuracy: 0.8667\n",
      "Epoch 1556/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4485 - val_accuracy: 0.8667\n",
      "Epoch 1557/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4480 - val_accuracy: 0.8667\n",
      "Epoch 1558/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4485 - val_accuracy: 0.8667\n",
      "Epoch 1559/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4480 - val_accuracy: 0.8667\n",
      "Epoch 1560/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4487 - val_accuracy: 0.8667\n",
      "Epoch 1561/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4476 - val_accuracy: 0.8667\n",
      "Epoch 1562/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4480 - val_accuracy: 0.8667\n",
      "Epoch 1563/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4475 - val_accuracy: 0.8667\n",
      "Epoch 1564/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4480 - val_accuracy: 0.8667\n",
      "Epoch 1565/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4477 - val_accuracy: 0.8667\n",
      "Epoch 1566/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4476 - val_accuracy: 0.8667\n",
      "Epoch 1567/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4471 - val_accuracy: 0.8667\n",
      "Epoch 1568/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4477 - val_accuracy: 0.8667\n",
      "Epoch 1569/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4478 - val_accuracy: 0.8667\n",
      "Epoch 1570/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4478 - val_accuracy: 0.8667\n",
      "Epoch 1571/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4475 - val_accuracy: 0.8667\n",
      "Epoch 1572/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4476 - val_accuracy: 0.8667\n",
      "Epoch 1573/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4472 - val_accuracy: 0.8667\n",
      "Epoch 1574/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4478 - val_accuracy: 0.8667\n",
      "Epoch 1575/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4478 - val_accuracy: 0.8667\n",
      "Epoch 1576/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4477 - val_accuracy: 0.8667\n",
      "Epoch 1577/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4474 - val_accuracy: 0.8667\n",
      "Epoch 1578/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4477 - val_accuracy: 0.8667\n",
      "Epoch 1579/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4475 - val_accuracy: 0.8667\n",
      "Epoch 1580/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4476 - val_accuracy: 0.8667\n",
      "Epoch 1581/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4479 - val_accuracy: 0.8667\n",
      "Epoch 1582/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4474 - val_accuracy: 0.8667\n",
      "Epoch 1583/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4475 - val_accuracy: 0.8667\n",
      "Epoch 1584/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4475 - val_accuracy: 0.8667\n",
      "Epoch 1585/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4472 - val_accuracy: 0.8667\n",
      "Epoch 1586/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4474 - val_accuracy: 0.8667\n",
      "Epoch 1587/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4475 - val_accuracy: 0.8667\n",
      "Epoch 1588/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4475 - val_accuracy: 0.8667\n",
      "Epoch 1589/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4473 - val_accuracy: 0.8667\n",
      "Epoch 1590/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4474 - val_accuracy: 0.8667\n",
      "Epoch 1591/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4474 - val_accuracy: 0.8667\n",
      "Epoch 1592/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4470 - val_accuracy: 0.8667\n",
      "Epoch 1593/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4472 - val_accuracy: 0.8667\n",
      "Epoch 1594/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4471 - val_accuracy: 0.8667\n",
      "Epoch 1595/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4473 - val_accuracy: 0.8667\n",
      "Epoch 1596/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4476 - val_accuracy: 0.8667\n",
      "Epoch 1597/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4467 - val_accuracy: 0.8667\n",
      "Epoch 1598/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4473 - val_accuracy: 0.8667\n",
      "Epoch 1599/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4471 - val_accuracy: 0.8667\n",
      "Epoch 1600/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4477 - val_accuracy: 0.8667\n",
      "Epoch 1601/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4475 - val_accuracy: 0.8667\n",
      "Epoch 1602/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4472 - val_accuracy: 0.8667\n",
      "Epoch 1603/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4470 - val_accuracy: 0.8667\n",
      "Epoch 1604/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4476 - val_accuracy: 0.8667\n",
      "Epoch 1605/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4474 - val_accuracy: 0.8667\n",
      "Epoch 1606/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4470 - val_accuracy: 0.8667\n",
      "Epoch 1607/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4474 - val_accuracy: 0.8667\n",
      "Epoch 1608/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4476 - val_accuracy: 0.8667\n",
      "Epoch 1609/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4475 - val_accuracy: 0.8667\n",
      "Epoch 1610/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4478 - val_accuracy: 0.8667\n",
      "Epoch 1611/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4472 - val_accuracy: 0.8667\n",
      "Epoch 1612/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4475 - val_accuracy: 0.8667\n",
      "Epoch 1613/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4480 - val_accuracy: 0.8667\n",
      "Epoch 1614/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4479 - val_accuracy: 0.8667\n",
      "Epoch 1615/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4476 - val_accuracy: 0.8667\n",
      "Epoch 1616/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4478 - val_accuracy: 0.8667\n",
      "Epoch 1617/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4482 - val_accuracy: 0.8667\n",
      "Epoch 1618/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4473 - val_accuracy: 0.8667\n",
      "Epoch 1619/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4482 - val_accuracy: 0.8667\n",
      "Epoch 1620/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4483 - val_accuracy: 0.8667\n",
      "Epoch 1621/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4482 - val_accuracy: 0.8750\n",
      "Epoch 1622/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4488 - val_accuracy: 0.8750\n",
      "Epoch 1623/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4482 - val_accuracy: 0.8750\n",
      "Epoch 1624/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4483 - val_accuracy: 0.8750\n",
      "Epoch 1625/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4489 - val_accuracy: 0.8750\n",
      "Epoch 1626/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4492 - val_accuracy: 0.8750\n",
      "Epoch 1627/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4488 - val_accuracy: 0.8750\n",
      "Epoch 1628/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4494 - val_accuracy: 0.8750\n",
      "Epoch 1629/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4485 - val_accuracy: 0.8750\n",
      "Epoch 1630/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4490 - val_accuracy: 0.8750\n",
      "Epoch 1631/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4490 - val_accuracy: 0.8750\n",
      "Epoch 1632/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4489 - val_accuracy: 0.8750\n",
      "Epoch 1633/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4495 - val_accuracy: 0.8750\n",
      "Epoch 1634/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4505 - val_accuracy: 0.8750\n",
      "Epoch 1635/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4495 - val_accuracy: 0.8750\n",
      "Epoch 1636/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4496 - val_accuracy: 0.8750\n",
      "Epoch 1637/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4499 - val_accuracy: 0.8750\n",
      "Epoch 1638/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4507 - val_accuracy: 0.8750\n",
      "Epoch 1639/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4502 - val_accuracy: 0.8750\n",
      "Epoch 1640/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4515 - val_accuracy: 0.8750\n",
      "Epoch 1641/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4511 - val_accuracy: 0.8750\n",
      "Epoch 1642/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4506 - val_accuracy: 0.8750\n",
      "Epoch 1643/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4512 - val_accuracy: 0.8750\n",
      "Epoch 1644/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4508 - val_accuracy: 0.8750\n",
      "Epoch 1645/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4518 - val_accuracy: 0.8750\n",
      "Epoch 1646/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4517 - val_accuracy: 0.8750\n",
      "Epoch 1647/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4518 - val_accuracy: 0.8750\n",
      "Epoch 1648/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4520 - val_accuracy: 0.8750\n",
      "Epoch 1649/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4530 - val_accuracy: 0.8750\n",
      "Epoch 1650/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4531 - val_accuracy: 0.8750\n",
      "Epoch 1651/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4524 - val_accuracy: 0.8750\n",
      "Epoch 1652/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4532 - val_accuracy: 0.8750\n",
      "Epoch 1653/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4536 - val_accuracy: 0.8750\n",
      "Epoch 1654/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4541 - val_accuracy: 0.8750\n",
      "Epoch 1655/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4538 - val_accuracy: 0.8750\n",
      "Epoch 1656/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4543 - val_accuracy: 0.8750\n",
      "Epoch 1657/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4552 - val_accuracy: 0.8750\n",
      "Epoch 1658/2000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4549 - val_accuracy: 0.8750\n",
      "Epoch 1659/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4543 - val_accuracy: 0.8750\n",
      "Epoch 1660/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4553 - val_accuracy: 0.8750\n",
      "Epoch 1661/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4564 - val_accuracy: 0.8750\n",
      "Epoch 1662/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4552 - val_accuracy: 0.8750\n",
      "Epoch 1663/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4556 - val_accuracy: 0.8750\n",
      "Epoch 1664/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4565 - val_accuracy: 0.8750\n",
      "Epoch 1665/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4573 - val_accuracy: 0.8750\n",
      "Epoch 1666/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4576 - val_accuracy: 0.8750\n",
      "Epoch 1667/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4577 - val_accuracy: 0.8750\n",
      "Epoch 1668/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4575 - val_accuracy: 0.8750\n",
      "Epoch 1669/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4585 - val_accuracy: 0.8750\n",
      "Epoch 1670/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4580 - val_accuracy: 0.8750\n",
      "Epoch 1671/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4591 - val_accuracy: 0.8750\n",
      "Epoch 1672/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4594 - val_accuracy: 0.8750\n",
      "Epoch 1673/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4601 - val_accuracy: 0.8750\n",
      "Epoch 1674/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.8833\n",
      "Epoch 1675/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.8750\n",
      "Epoch 1676/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4602 - val_accuracy: 0.8833\n",
      "Epoch 1677/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4603 - val_accuracy: 0.8750\n",
      "Epoch 1678/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 9.9761e-04 - accuracy: 1.0000 - val_loss: 0.4610 - val_accuracy: 0.8833\n",
      "Epoch 1679/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 9.9194e-04 - accuracy: 1.0000 - val_loss: 0.4607 - val_accuracy: 0.8833\n",
      "Epoch 1680/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 9.8578e-04 - accuracy: 1.0000 - val_loss: 0.4609 - val_accuracy: 0.8833\n",
      "Epoch 1681/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 9.7687e-04 - accuracy: 1.0000 - val_loss: 0.4611 - val_accuracy: 0.8833\n",
      "Epoch 1682/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 9.7148e-04 - accuracy: 1.0000 - val_loss: 0.4622 - val_accuracy: 0.8833\n",
      "Epoch 1683/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 9.6702e-04 - accuracy: 1.0000 - val_loss: 0.4624 - val_accuracy: 0.8833\n",
      "Epoch 1684/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 9.6156e-04 - accuracy: 1.0000 - val_loss: 0.4608 - val_accuracy: 0.8833\n",
      "Epoch 1685/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 9.5171e-04 - accuracy: 1.0000 - val_loss: 0.4626 - val_accuracy: 0.8833\n",
      "Epoch 1686/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 9.4820e-04 - accuracy: 1.0000 - val_loss: 0.4622 - val_accuracy: 0.8833\n",
      "Epoch 1687/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 9.4348e-04 - accuracy: 1.0000 - val_loss: 0.4629 - val_accuracy: 0.8833\n",
      "Epoch 1688/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 9.3674e-04 - accuracy: 1.0000 - val_loss: 0.4621 - val_accuracy: 0.8833\n",
      "Epoch 1689/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 9.2885e-04 - accuracy: 1.0000 - val_loss: 0.4634 - val_accuracy: 0.8833\n",
      "Epoch 1690/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 9.2500e-04 - accuracy: 1.0000 - val_loss: 0.4633 - val_accuracy: 0.8833\n",
      "Epoch 1691/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 9.1937e-04 - accuracy: 1.0000 - val_loss: 0.4631 - val_accuracy: 0.8833\n",
      "Epoch 1692/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 9.1039e-04 - accuracy: 1.0000 - val_loss: 0.4640 - val_accuracy: 0.8833\n",
      "Epoch 1693/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 9.0913e-04 - accuracy: 1.0000 - val_loss: 0.4648 - val_accuracy: 0.8833\n",
      "Epoch 1694/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 9.0102e-04 - accuracy: 1.0000 - val_loss: 0.4637 - val_accuracy: 0.8833\n",
      "Epoch 1695/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 8.9386e-04 - accuracy: 1.0000 - val_loss: 0.4646 - val_accuracy: 0.8833\n",
      "Epoch 1696/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.9165e-04 - accuracy: 1.0000 - val_loss: 0.4643 - val_accuracy: 0.8833\n",
      "Epoch 1697/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 8.8385e-04 - accuracy: 1.0000 - val_loss: 0.4655 - val_accuracy: 0.8833\n",
      "Epoch 1698/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 8.7909e-04 - accuracy: 1.0000 - val_loss: 0.4642 - val_accuracy: 0.8833\n",
      "Epoch 1699/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.7149e-04 - accuracy: 1.0000 - val_loss: 0.4646 - val_accuracy: 0.8833\n",
      "Epoch 1700/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 8.6702e-04 - accuracy: 1.0000 - val_loss: 0.4660 - val_accuracy: 0.8833\n",
      "Epoch 1701/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 8.6249e-04 - accuracy: 1.0000 - val_loss: 0.4646 - val_accuracy: 0.8833\n",
      "Epoch 1702/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 8.5626e-04 - accuracy: 1.0000 - val_loss: 0.4654 - val_accuracy: 0.8833\n",
      "Epoch 1703/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 8.5024e-04 - accuracy: 1.0000 - val_loss: 0.4654 - val_accuracy: 0.8833\n",
      "Epoch 1704/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 8.4504e-04 - accuracy: 1.0000 - val_loss: 0.4668 - val_accuracy: 0.8833\n",
      "Epoch 1705/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.4001e-04 - accuracy: 1.0000 - val_loss: 0.4665 - val_accuracy: 0.8833\n",
      "Epoch 1706/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.3326e-04 - accuracy: 1.0000 - val_loss: 0.4666 - val_accuracy: 0.8833\n",
      "Epoch 1707/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.2986e-04 - accuracy: 1.0000 - val_loss: 0.4665 - val_accuracy: 0.8833\n",
      "Epoch 1708/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.2382e-04 - accuracy: 1.0000 - val_loss: 0.4670 - val_accuracy: 0.8917\n",
      "Epoch 1709/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 8.1828e-04 - accuracy: 1.0000 - val_loss: 0.4665 - val_accuracy: 0.8917\n",
      "Epoch 1710/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.1279e-04 - accuracy: 1.0000 - val_loss: 0.4680 - val_accuracy: 0.8917\n",
      "Epoch 1711/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 8.1073e-04 - accuracy: 1.0000 - val_loss: 0.4660 - val_accuracy: 0.8917\n",
      "Epoch 1712/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 8.0167e-04 - accuracy: 1.0000 - val_loss: 0.4668 - val_accuracy: 0.8917\n",
      "Epoch 1713/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.9855e-04 - accuracy: 1.0000 - val_loss: 0.4680 - val_accuracy: 0.8917\n",
      "Epoch 1714/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 7.9326e-04 - accuracy: 1.0000 - val_loss: 0.4680 - val_accuracy: 0.8917\n",
      "Epoch 1715/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 7.8899e-04 - accuracy: 1.0000 - val_loss: 0.4668 - val_accuracy: 0.8917\n",
      "Epoch 1716/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 7.8247e-04 - accuracy: 1.0000 - val_loss: 0.4676 - val_accuracy: 0.8917\n",
      "Epoch 1717/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.7967e-04 - accuracy: 1.0000 - val_loss: 0.4684 - val_accuracy: 0.8917\n",
      "Epoch 1718/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.7368e-04 - accuracy: 1.0000 - val_loss: 0.4678 - val_accuracy: 0.8917\n",
      "Epoch 1719/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.6701e-04 - accuracy: 1.0000 - val_loss: 0.4670 - val_accuracy: 0.8917\n",
      "Epoch 1720/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 7.6415e-04 - accuracy: 1.0000 - val_loss: 0.4683 - val_accuracy: 0.8917\n",
      "Epoch 1721/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 7.6091e-04 - accuracy: 1.0000 - val_loss: 0.4686 - val_accuracy: 0.8917\n",
      "Epoch 1722/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.5324e-04 - accuracy: 1.0000 - val_loss: 0.4669 - val_accuracy: 0.8917\n",
      "Epoch 1723/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.4888e-04 - accuracy: 1.0000 - val_loss: 0.4682 - val_accuracy: 0.8917\n",
      "Epoch 1724/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 7.4636e-04 - accuracy: 1.0000 - val_loss: 0.4693 - val_accuracy: 0.8917\n",
      "Epoch 1725/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.4096e-04 - accuracy: 1.0000 - val_loss: 0.4677 - val_accuracy: 0.8917\n",
      "Epoch 1726/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.3306e-04 - accuracy: 1.0000 - val_loss: 0.4692 - val_accuracy: 0.8917\n",
      "Epoch 1727/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.3264e-04 - accuracy: 1.0000 - val_loss: 0.4691 - val_accuracy: 0.8917\n",
      "Epoch 1728/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 7.2673e-04 - accuracy: 1.0000 - val_loss: 0.4687 - val_accuracy: 0.8917\n",
      "Epoch 1729/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.2197e-04 - accuracy: 1.0000 - val_loss: 0.4693 - val_accuracy: 0.8917\n",
      "Epoch 1730/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 7.1720e-04 - accuracy: 1.0000 - val_loss: 0.4694 - val_accuracy: 0.8917\n",
      "Epoch 1731/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.1343e-04 - accuracy: 1.0000 - val_loss: 0.4696 - val_accuracy: 0.8917\n",
      "Epoch 1732/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.0754e-04 - accuracy: 1.0000 - val_loss: 0.4698 - val_accuracy: 0.8917\n",
      "Epoch 1733/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.0563e-04 - accuracy: 1.0000 - val_loss: 0.4703 - val_accuracy: 0.8917\n",
      "Epoch 1734/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.9912e-04 - accuracy: 1.0000 - val_loss: 0.4694 - val_accuracy: 0.8917\n",
      "Epoch 1735/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.9575e-04 - accuracy: 1.0000 - val_loss: 0.4705 - val_accuracy: 0.8917\n",
      "Epoch 1736/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 6.9186e-04 - accuracy: 1.0000 - val_loss: 0.4715 - val_accuracy: 0.8917\n",
      "Epoch 1737/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.8849e-04 - accuracy: 1.0000 - val_loss: 0.4695 - val_accuracy: 0.8917\n",
      "Epoch 1738/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.8178e-04 - accuracy: 1.0000 - val_loss: 0.4706 - val_accuracy: 0.8917\n",
      "Epoch 1739/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.8086e-04 - accuracy: 1.0000 - val_loss: 0.4715 - val_accuracy: 0.8833\n",
      "Epoch 1740/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.7527e-04 - accuracy: 1.0000 - val_loss: 0.4709 - val_accuracy: 0.8917\n",
      "Epoch 1741/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.7073e-04 - accuracy: 1.0000 - val_loss: 0.4714 - val_accuracy: 0.8917\n",
      "Epoch 1742/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.6960e-04 - accuracy: 1.0000 - val_loss: 0.4720 - val_accuracy: 0.8833\n",
      "Epoch 1743/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.6291e-04 - accuracy: 1.0000 - val_loss: 0.4717 - val_accuracy: 0.8917\n",
      "Epoch 1744/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.5925e-04 - accuracy: 1.0000 - val_loss: 0.4712 - val_accuracy: 0.8917\n",
      "Epoch 1745/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 6.5656e-04 - accuracy: 1.0000 - val_loss: 0.4724 - val_accuracy: 0.8917\n",
      "Epoch 1746/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.5194e-04 - accuracy: 1.0000 - val_loss: 0.4728 - val_accuracy: 0.8917\n",
      "Epoch 1747/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.4691e-04 - accuracy: 1.0000 - val_loss: 0.4718 - val_accuracy: 0.8917\n",
      "Epoch 1748/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.4506e-04 - accuracy: 1.0000 - val_loss: 0.4737 - val_accuracy: 0.8833\n",
      "Epoch 1749/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.4014e-04 - accuracy: 1.0000 - val_loss: 0.4724 - val_accuracy: 0.8917\n",
      "Epoch 1750/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.3600e-04 - accuracy: 1.0000 - val_loss: 0.4726 - val_accuracy: 0.8833\n",
      "Epoch 1751/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.3333e-04 - accuracy: 1.0000 - val_loss: 0.4737 - val_accuracy: 0.8917\n",
      "Epoch 1752/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.2792e-04 - accuracy: 1.0000 - val_loss: 0.4722 - val_accuracy: 0.8917\n",
      "Epoch 1753/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.2358e-04 - accuracy: 1.0000 - val_loss: 0.4728 - val_accuracy: 0.8833\n",
      "Epoch 1754/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.2357e-04 - accuracy: 1.0000 - val_loss: 0.4737 - val_accuracy: 0.8917\n",
      "Epoch 1755/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.1628e-04 - accuracy: 1.0000 - val_loss: 0.4725 - val_accuracy: 0.8917\n",
      "Epoch 1756/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.1528e-04 - accuracy: 1.0000 - val_loss: 0.4729 - val_accuracy: 0.8833\n",
      "Epoch 1757/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.0848e-04 - accuracy: 1.0000 - val_loss: 0.4734 - val_accuracy: 0.8917\n",
      "Epoch 1758/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.0841e-04 - accuracy: 1.0000 - val_loss: 0.4739 - val_accuracy: 0.8833\n",
      "Epoch 1759/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 6.0326e-04 - accuracy: 1.0000 - val_loss: 0.4730 - val_accuracy: 0.8917\n",
      "Epoch 1760/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.9940e-04 - accuracy: 1.0000 - val_loss: 0.4741 - val_accuracy: 0.8917\n",
      "Epoch 1761/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.9760e-04 - accuracy: 1.0000 - val_loss: 0.4740 - val_accuracy: 0.8833\n",
      "Epoch 1762/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 5.9251e-04 - accuracy: 1.0000 - val_loss: 0.4736 - val_accuracy: 0.8917\n",
      "Epoch 1763/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.8891e-04 - accuracy: 1.0000 - val_loss: 0.4748 - val_accuracy: 0.8917\n",
      "Epoch 1764/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.8718e-04 - accuracy: 1.0000 - val_loss: 0.4744 - val_accuracy: 0.8833\n",
      "Epoch 1765/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.8246e-04 - accuracy: 1.0000 - val_loss: 0.4742 - val_accuracy: 0.8917\n",
      "Epoch 1766/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.7982e-04 - accuracy: 1.0000 - val_loss: 0.4750 - val_accuracy: 0.8917\n",
      "Epoch 1767/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.7607e-04 - accuracy: 1.0000 - val_loss: 0.4747 - val_accuracy: 0.8833\n",
      "Epoch 1768/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.7405e-04 - accuracy: 1.0000 - val_loss: 0.4754 - val_accuracy: 0.8917\n",
      "Epoch 1769/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 5.7008e-04 - accuracy: 1.0000 - val_loss: 0.4744 - val_accuracy: 0.8833\n",
      "Epoch 1770/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.6616e-04 - accuracy: 1.0000 - val_loss: 0.4746 - val_accuracy: 0.8917\n",
      "Epoch 1771/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.6472e-04 - accuracy: 1.0000 - val_loss: 0.4758 - val_accuracy: 0.8833\n",
      "Epoch 1772/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.6063e-04 - accuracy: 1.0000 - val_loss: 0.4761 - val_accuracy: 0.8917\n",
      "Epoch 1773/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 5.5888e-04 - accuracy: 1.0000 - val_loss: 0.4757 - val_accuracy: 0.8917\n",
      "Epoch 1774/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 5.5420e-04 - accuracy: 1.0000 - val_loss: 0.4750 - val_accuracy: 0.8917\n",
      "Epoch 1775/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.5160e-04 - accuracy: 1.0000 - val_loss: 0.4767 - val_accuracy: 0.8917\n",
      "Epoch 1776/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.5113e-04 - accuracy: 1.0000 - val_loss: 0.4763 - val_accuracy: 0.8833\n",
      "Epoch 1777/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.4569e-04 - accuracy: 1.0000 - val_loss: 0.4758 - val_accuracy: 0.8917\n",
      "Epoch 1778/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.4472e-04 - accuracy: 1.0000 - val_loss: 0.4747 - val_accuracy: 0.8917\n",
      "Epoch 1779/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.4010e-04 - accuracy: 1.0000 - val_loss: 0.4765 - val_accuracy: 0.8917\n",
      "Epoch 1780/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 5.4181e-04 - accuracy: 1.0000 - val_loss: 0.4771 - val_accuracy: 0.8917\n",
      "Epoch 1781/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 5.3341e-04 - accuracy: 1.0000 - val_loss: 0.4758 - val_accuracy: 0.8917\n",
      "Epoch 1782/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 5.3548e-04 - accuracy: 1.0000 - val_loss: 0.4770 - val_accuracy: 0.8917\n",
      "Epoch 1783/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 5.3010e-04 - accuracy: 1.0000 - val_loss: 0.4764 - val_accuracy: 0.8833\n",
      "Epoch 1784/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.3006e-04 - accuracy: 1.0000 - val_loss: 0.4766 - val_accuracy: 0.8917\n",
      "Epoch 1785/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.2673e-04 - accuracy: 1.0000 - val_loss: 0.4780 - val_accuracy: 0.8917\n",
      "Epoch 1786/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.2379e-04 - accuracy: 1.0000 - val_loss: 0.4771 - val_accuracy: 0.8917\n",
      "Epoch 1787/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 5.2109e-04 - accuracy: 1.0000 - val_loss: 0.4780 - val_accuracy: 0.8917\n",
      "Epoch 1788/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.1876e-04 - accuracy: 1.0000 - val_loss: 0.4782 - val_accuracy: 0.8833\n",
      "Epoch 1789/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.1649e-04 - accuracy: 1.0000 - val_loss: 0.4782 - val_accuracy: 0.8917\n",
      "Epoch 1790/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 5.1508e-04 - accuracy: 1.0000 - val_loss: 0.4773 - val_accuracy: 0.8833\n",
      "Epoch 1791/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 5.1092e-04 - accuracy: 1.0000 - val_loss: 0.4773 - val_accuracy: 0.8917\n",
      "Epoch 1792/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 5.0988e-04 - accuracy: 1.0000 - val_loss: 0.4783 - val_accuracy: 0.8917\n",
      "Epoch 1793/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 5.0691e-04 - accuracy: 1.0000 - val_loss: 0.4788 - val_accuracy: 0.8833\n",
      "Epoch 1794/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 5.0507e-04 - accuracy: 1.0000 - val_loss: 0.4780 - val_accuracy: 0.8917\n",
      "Epoch 1795/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.0240e-04 - accuracy: 1.0000 - val_loss: 0.4775 - val_accuracy: 0.8833\n",
      "Epoch 1796/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.0061e-04 - accuracy: 1.0000 - val_loss: 0.4783 - val_accuracy: 0.8917\n",
      "Epoch 1797/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 4.9956e-04 - accuracy: 1.0000 - val_loss: 0.4772 - val_accuracy: 0.8833\n",
      "Epoch 1798/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 4.9540e-04 - accuracy: 1.0000 - val_loss: 0.4770 - val_accuracy: 0.8833\n",
      "Epoch 1799/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.9547e-04 - accuracy: 1.0000 - val_loss: 0.4789 - val_accuracy: 0.8833\n",
      "Epoch 1800/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.9261e-04 - accuracy: 1.0000 - val_loss: 0.4782 - val_accuracy: 0.8833\n",
      "Epoch 1801/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.8900e-04 - accuracy: 1.0000 - val_loss: 0.4778 - val_accuracy: 0.8833\n",
      "Epoch 1802/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.8794e-04 - accuracy: 1.0000 - val_loss: 0.4768 - val_accuracy: 0.8833\n",
      "Epoch 1803/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.8537e-04 - accuracy: 1.0000 - val_loss: 0.4767 - val_accuracy: 0.8833\n",
      "Epoch 1804/2000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 4.8440e-04 - accuracy: 1.0000 - val_loss: 0.4763 - val_accuracy: 0.8917\n",
      "Epoch 1805/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.8257e-04 - accuracy: 1.0000 - val_loss: 0.4754 - val_accuracy: 0.8833\n",
      "Epoch 1806/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.8027e-04 - accuracy: 1.0000 - val_loss: 0.4747 - val_accuracy: 0.8833\n",
      "Epoch 1807/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.7943e-04 - accuracy: 1.0000 - val_loss: 0.4764 - val_accuracy: 0.8833\n",
      "Epoch 1808/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.7716e-04 - accuracy: 1.0000 - val_loss: 0.4756 - val_accuracy: 0.8917\n",
      "Epoch 1809/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 4.7443e-04 - accuracy: 1.0000 - val_loss: 0.4738 - val_accuracy: 0.8917\n",
      "Epoch 1810/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.7374e-04 - accuracy: 1.0000 - val_loss: 0.4732 - val_accuracy: 0.8917\n",
      "Epoch 1811/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.7180e-04 - accuracy: 1.0000 - val_loss: 0.4732 - val_accuracy: 0.8917\n",
      "Epoch 1812/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.7074e-04 - accuracy: 1.0000 - val_loss: 0.4728 - val_accuracy: 0.8917\n",
      "Epoch 1813/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.6891e-04 - accuracy: 1.0000 - val_loss: 0.4719 - val_accuracy: 0.8917\n",
      "Epoch 1814/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.6811e-04 - accuracy: 1.0000 - val_loss: 0.4725 - val_accuracy: 0.8917\n",
      "Epoch 1815/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.6609e-04 - accuracy: 1.0000 - val_loss: 0.4721 - val_accuracy: 0.8917\n",
      "Epoch 1816/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.6571e-04 - accuracy: 1.0000 - val_loss: 0.4712 - val_accuracy: 0.8917\n",
      "Epoch 1817/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 4.6391e-04 - accuracy: 1.0000 - val_loss: 0.4709 - val_accuracy: 0.8917\n",
      "Epoch 1818/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.6185e-04 - accuracy: 1.0000 - val_loss: 0.4704 - val_accuracy: 0.8917\n",
      "Epoch 1819/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.6093e-04 - accuracy: 1.0000 - val_loss: 0.4700 - val_accuracy: 0.8917\n",
      "Epoch 1820/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.6067e-04 - accuracy: 1.0000 - val_loss: 0.4700 - val_accuracy: 0.8917\n",
      "Epoch 1821/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.5864e-04 - accuracy: 1.0000 - val_loss: 0.4695 - val_accuracy: 0.8917\n",
      "Epoch 1822/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.5793e-04 - accuracy: 1.0000 - val_loss: 0.4694 - val_accuracy: 0.8917\n",
      "Epoch 1823/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.5594e-04 - accuracy: 1.0000 - val_loss: 0.4687 - val_accuracy: 0.8917\n",
      "Epoch 1824/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.5577e-04 - accuracy: 1.0000 - val_loss: 0.4683 - val_accuracy: 0.8917\n",
      "Epoch 1825/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.5344e-04 - accuracy: 1.0000 - val_loss: 0.4667 - val_accuracy: 0.9000\n",
      "Epoch 1826/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.5348e-04 - accuracy: 1.0000 - val_loss: 0.4662 - val_accuracy: 0.8917\n",
      "Epoch 1827/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.5472e-04 - accuracy: 1.0000 - val_loss: 0.4645 - val_accuracy: 0.8917\n",
      "Epoch 1828/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.5165e-04 - accuracy: 1.0000 - val_loss: 0.4656 - val_accuracy: 0.8917\n",
      "Epoch 1829/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.5199e-04 - accuracy: 1.0000 - val_loss: 0.4653 - val_accuracy: 0.8917\n",
      "Epoch 1830/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.5031e-04 - accuracy: 1.0000 - val_loss: 0.4642 - val_accuracy: 0.8917\n",
      "Epoch 1831/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.5019e-04 - accuracy: 1.0000 - val_loss: 0.4644 - val_accuracy: 0.8917\n",
      "Epoch 1832/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.4930e-04 - accuracy: 1.0000 - val_loss: 0.4630 - val_accuracy: 0.9000\n",
      "Epoch 1833/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.4991e-04 - accuracy: 1.0000 - val_loss: 0.4625 - val_accuracy: 0.8917\n",
      "Epoch 1834/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.4886e-04 - accuracy: 1.0000 - val_loss: 0.4632 - val_accuracy: 0.8917\n",
      "Epoch 1835/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.4770e-04 - accuracy: 1.0000 - val_loss: 0.4610 - val_accuracy: 0.9000\n",
      "Epoch 1836/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.4970e-04 - accuracy: 1.0000 - val_loss: 0.4605 - val_accuracy: 0.8917\n",
      "Epoch 1837/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.4744e-04 - accuracy: 1.0000 - val_loss: 0.4607 - val_accuracy: 0.9000\n",
      "Epoch 1838/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.5019e-04 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.9000\n",
      "Epoch 1839/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.4700e-04 - accuracy: 1.0000 - val_loss: 0.4610 - val_accuracy: 0.9000\n",
      "Epoch 1840/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 4.4741e-04 - accuracy: 1.0000 - val_loss: 0.4579 - val_accuracy: 0.9083\n",
      "Epoch 1841/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 4.4855e-04 - accuracy: 1.0000 - val_loss: 0.4594 - val_accuracy: 0.9000\n",
      "Epoch 1842/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.4990e-04 - accuracy: 1.0000 - val_loss: 0.4563 - val_accuracy: 0.9083\n",
      "Epoch 1843/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.4730e-04 - accuracy: 1.0000 - val_loss: 0.4588 - val_accuracy: 0.9000\n",
      "Epoch 1844/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.4858e-04 - accuracy: 1.0000 - val_loss: 0.4558 - val_accuracy: 0.9083\n",
      "Epoch 1845/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.4996e-04 - accuracy: 1.0000 - val_loss: 0.4581 - val_accuracy: 0.9000\n",
      "Epoch 1846/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.4817e-04 - accuracy: 1.0000 - val_loss: 0.4548 - val_accuracy: 0.9083\n",
      "Epoch 1847/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.4975e-04 - accuracy: 1.0000 - val_loss: 0.4565 - val_accuracy: 0.9000\n",
      "Epoch 1848/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.5108e-04 - accuracy: 1.0000 - val_loss: 0.4546 - val_accuracy: 0.9083\n",
      "Epoch 1849/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.4988e-04 - accuracy: 1.0000 - val_loss: 0.4564 - val_accuracy: 0.9000\n",
      "Epoch 1850/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.5107e-04 - accuracy: 1.0000 - val_loss: 0.4561 - val_accuracy: 0.9000\n",
      "Epoch 1851/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.5191e-04 - accuracy: 1.0000 - val_loss: 0.4551 - val_accuracy: 0.9000\n",
      "Epoch 1852/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.5382e-04 - accuracy: 1.0000 - val_loss: 0.4551 - val_accuracy: 0.9000\n",
      "Epoch 1853/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.5465e-04 - accuracy: 1.0000 - val_loss: 0.4559 - val_accuracy: 0.9000\n",
      "Epoch 1854/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.5672e-04 - accuracy: 1.0000 - val_loss: 0.4528 - val_accuracy: 0.9083\n",
      "Epoch 1855/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.5779e-04 - accuracy: 1.0000 - val_loss: 0.4541 - val_accuracy: 0.9083\n",
      "Epoch 1856/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 4.5727e-04 - accuracy: 1.0000 - val_loss: 0.4545 - val_accuracy: 0.9083\n",
      "Epoch 1857/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.6113e-04 - accuracy: 1.0000 - val_loss: 0.4536 - val_accuracy: 0.9083\n",
      "Epoch 1858/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.6152e-04 - accuracy: 1.0000 - val_loss: 0.4541 - val_accuracy: 0.9083\n",
      "Epoch 1859/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.6041e-04 - accuracy: 1.0000 - val_loss: 0.4535 - val_accuracy: 0.9083\n",
      "Epoch 1860/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.6634e-04 - accuracy: 1.0000 - val_loss: 0.4528 - val_accuracy: 0.9083\n",
      "Epoch 1861/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.6654e-04 - accuracy: 1.0000 - val_loss: 0.4528 - val_accuracy: 0.9083\n",
      "Epoch 1862/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.6566e-04 - accuracy: 1.0000 - val_loss: 0.4530 - val_accuracy: 0.9083\n",
      "Epoch 1863/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.7413e-04 - accuracy: 1.0000 - val_loss: 0.4536 - val_accuracy: 0.9000\n",
      "Epoch 1864/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.7090e-04 - accuracy: 1.0000 - val_loss: 0.4532 - val_accuracy: 0.9000\n",
      "Epoch 1865/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.7386e-04 - accuracy: 1.0000 - val_loss: 0.4533 - val_accuracy: 0.9000\n",
      "Epoch 1866/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.7660e-04 - accuracy: 1.0000 - val_loss: 0.4527 - val_accuracy: 0.9000\n",
      "Epoch 1867/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.7918e-04 - accuracy: 1.0000 - val_loss: 0.4534 - val_accuracy: 0.9000\n",
      "Epoch 1868/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.8381e-04 - accuracy: 1.0000 - val_loss: 0.4537 - val_accuracy: 0.9000\n",
      "Epoch 1869/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.8199e-04 - accuracy: 1.0000 - val_loss: 0.4555 - val_accuracy: 0.9000\n",
      "Epoch 1870/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.8750e-04 - accuracy: 1.0000 - val_loss: 0.4552 - val_accuracy: 0.8917\n",
      "Epoch 1871/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.9769e-04 - accuracy: 1.0000 - val_loss: 0.4556 - val_accuracy: 0.9000\n",
      "Epoch 1872/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.8400e-04 - accuracy: 1.0000 - val_loss: 0.4580 - val_accuracy: 0.8917\n",
      "Epoch 1873/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.9963e-04 - accuracy: 1.0000 - val_loss: 0.4591 - val_accuracy: 0.8917\n",
      "Epoch 1874/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.9845e-04 - accuracy: 1.0000 - val_loss: 0.4579 - val_accuracy: 0.9000\n",
      "Epoch 1875/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 5.1439e-04 - accuracy: 1.0000 - val_loss: 0.4579 - val_accuracy: 0.9000\n",
      "Epoch 1876/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 4.8664e-04 - accuracy: 1.0000 - val_loss: 0.4604 - val_accuracy: 0.8917\n",
      "Epoch 1877/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.3569e-04 - accuracy: 1.0000 - val_loss: 0.4575 - val_accuracy: 0.9000\n",
      "Epoch 1878/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 4.8369e-04 - accuracy: 1.0000 - val_loss: 0.4631 - val_accuracy: 0.8917\n",
      "Epoch 1879/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.4411e-04 - accuracy: 1.0000 - val_loss: 0.4602 - val_accuracy: 0.8917\n",
      "Epoch 1880/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 4.9102e-04 - accuracy: 1.0000 - val_loss: 0.4608 - val_accuracy: 0.8917\n",
      "Epoch 1881/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.6540e-04 - accuracy: 1.0000 - val_loss: 0.4602 - val_accuracy: 0.9000\n",
      "Epoch 1882/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.8911e-04 - accuracy: 1.0000 - val_loss: 0.4669 - val_accuracy: 0.8750\n",
      "Epoch 1883/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 5.7244e-04 - accuracy: 1.0000 - val_loss: 0.4660 - val_accuracy: 0.8917\n",
      "Epoch 1884/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 4.9534e-04 - accuracy: 1.0000 - val_loss: 0.4628 - val_accuracy: 0.8833\n",
      "Epoch 1885/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.9762e-04 - accuracy: 1.0000 - val_loss: 0.4637 - val_accuracy: 0.8917\n",
      "Epoch 1886/2000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 5.2533e-04 - accuracy: 1.0000 - val_loss: 0.4713 - val_accuracy: 0.8750\n",
      "Epoch 1887/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.7074e-04 - accuracy: 1.0000 - val_loss: 0.4708 - val_accuracy: 0.8917\n",
      "Epoch 1888/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.3210e-04 - accuracy: 1.0000 - val_loss: 0.4674 - val_accuracy: 0.8833\n",
      "Epoch 1889/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.2724e-04 - accuracy: 1.0000 - val_loss: 0.4700 - val_accuracy: 0.8917\n",
      "Epoch 1890/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.3977e-04 - accuracy: 1.0000 - val_loss: 0.5085 - val_accuracy: 0.8917\n",
      "Epoch 1891/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.8970e-04 - accuracy: 1.0000 - val_loss: 0.4328 - val_accuracy: 0.9000\n",
      "Epoch 1892/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 9.9926e-04 - accuracy: 1.0000 - val_loss: 0.5846 - val_accuracy: 0.8667\n",
      "Epoch 1893/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2347 - accuracy: 0.9417 - val_loss: 1.0588 - val_accuracy: 0.8083\n",
      "Epoch 1894/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.1257 - accuracy: 0.9611 - val_loss: 0.8316 - val_accuracy: 0.8583\n",
      "Epoch 1895/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0148 - accuracy: 0.9944 - val_loss: 0.4872 - val_accuracy: 0.9000\n",
      "Epoch 1896/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.6759 - val_accuracy: 0.8500\n",
      "Epoch 1897/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5039 - val_accuracy: 0.8917\n",
      "Epoch 1898/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.9022e-04 - accuracy: 1.0000 - val_loss: 0.4968 - val_accuracy: 0.8917\n",
      "Epoch 1899/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.4013e-04 - accuracy: 1.0000 - val_loss: 0.5160 - val_accuracy: 0.8917\n",
      "Epoch 1900/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.5410e-04 - accuracy: 1.0000 - val_loss: 0.5295 - val_accuracy: 0.8833\n",
      "Epoch 1901/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.3652e-04 - accuracy: 1.0000 - val_loss: 0.5337 - val_accuracy: 0.8833\n",
      "Epoch 1902/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 4.2736e-04 - accuracy: 1.0000 - val_loss: 0.5342 - val_accuracy: 0.8833\n",
      "Epoch 1903/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.1956e-04 - accuracy: 1.0000 - val_loss: 0.5339 - val_accuracy: 0.8833\n",
      "Epoch 1904/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 4.1291e-04 - accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 0.8833\n",
      "Epoch 1905/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.0728e-04 - accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 0.8833\n",
      "Epoch 1906/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 4.0215e-04 - accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 0.8833\n",
      "Epoch 1907/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.9738e-04 - accuracy: 1.0000 - val_loss: 0.5336 - val_accuracy: 0.8833\n",
      "Epoch 1908/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.9286e-04 - accuracy: 1.0000 - val_loss: 0.5337 - val_accuracy: 0.8833\n",
      "Epoch 1909/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.8860e-04 - accuracy: 1.0000 - val_loss: 0.5337 - val_accuracy: 0.8833\n",
      "Epoch 1910/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 3.8462e-04 - accuracy: 1.0000 - val_loss: 0.5337 - val_accuracy: 0.8833\n",
      "Epoch 1911/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 3.8084e-04 - accuracy: 1.0000 - val_loss: 0.5337 - val_accuracy: 0.8833\n",
      "Epoch 1912/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 3.7728e-04 - accuracy: 1.0000 - val_loss: 0.5336 - val_accuracy: 0.8833\n",
      "Epoch 1913/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 3.7389e-04 - accuracy: 1.0000 - val_loss: 0.5336 - val_accuracy: 0.8833\n",
      "Epoch 1914/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 3.7061e-04 - accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 0.8833\n",
      "Epoch 1915/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 3.6747e-04 - accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 0.8833\n",
      "Epoch 1916/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 3.6444e-04 - accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 0.8833\n",
      "Epoch 1917/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 3.6148e-04 - accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 0.8833\n",
      "Epoch 1918/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.5867e-04 - accuracy: 1.0000 - val_loss: 0.5334 - val_accuracy: 0.8833\n",
      "Epoch 1919/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.5594e-04 - accuracy: 1.0000 - val_loss: 0.5334 - val_accuracy: 0.8833\n",
      "Epoch 1920/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.5330e-04 - accuracy: 1.0000 - val_loss: 0.5334 - val_accuracy: 0.8833\n",
      "Epoch 1921/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 3.5072e-04 - accuracy: 1.0000 - val_loss: 0.5333 - val_accuracy: 0.8833\n",
      "Epoch 1922/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 3.4823e-04 - accuracy: 1.0000 - val_loss: 0.5333 - val_accuracy: 0.8833\n",
      "Epoch 1923/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.4581e-04 - accuracy: 1.0000 - val_loss: 0.5333 - val_accuracy: 0.8833\n",
      "Epoch 1924/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.4341e-04 - accuracy: 1.0000 - val_loss: 0.5333 - val_accuracy: 0.8833\n",
      "Epoch 1925/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 3.4108e-04 - accuracy: 1.0000 - val_loss: 0.5333 - val_accuracy: 0.8833\n",
      "Epoch 1926/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 3.3881e-04 - accuracy: 1.0000 - val_loss: 0.5334 - val_accuracy: 0.8833\n",
      "Epoch 1927/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.3661e-04 - accuracy: 1.0000 - val_loss: 0.5333 - val_accuracy: 0.8833\n",
      "Epoch 1928/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.3446e-04 - accuracy: 1.0000 - val_loss: 0.5333 - val_accuracy: 0.8833\n",
      "Epoch 1929/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.3235e-04 - accuracy: 1.0000 - val_loss: 0.5332 - val_accuracy: 0.8833\n",
      "Epoch 1930/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 3.3029e-04 - accuracy: 1.0000 - val_loss: 0.5332 - val_accuracy: 0.8833\n",
      "Epoch 1931/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 3.2828e-04 - accuracy: 1.0000 - val_loss: 0.5332 - val_accuracy: 0.8833\n",
      "Epoch 1932/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.2631e-04 - accuracy: 1.0000 - val_loss: 0.5331 - val_accuracy: 0.8833\n",
      "Epoch 1933/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.2440e-04 - accuracy: 1.0000 - val_loss: 0.5331 - val_accuracy: 0.8833\n",
      "Epoch 1934/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.2253e-04 - accuracy: 1.0000 - val_loss: 0.5331 - val_accuracy: 0.8833\n",
      "Epoch 1935/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.2070e-04 - accuracy: 1.0000 - val_loss: 0.5330 - val_accuracy: 0.8750\n",
      "Epoch 1936/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 3.1891e-04 - accuracy: 1.0000 - val_loss: 0.5330 - val_accuracy: 0.8750\n",
      "Epoch 1937/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 3.1715e-04 - accuracy: 1.0000 - val_loss: 0.5329 - val_accuracy: 0.8750\n",
      "Epoch 1938/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 3.1545e-04 - accuracy: 1.0000 - val_loss: 0.5328 - val_accuracy: 0.8750\n",
      "Epoch 1939/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 3.1378e-04 - accuracy: 1.0000 - val_loss: 0.5327 - val_accuracy: 0.8750\n",
      "Epoch 1940/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 3.1214e-04 - accuracy: 1.0000 - val_loss: 0.5326 - val_accuracy: 0.8750\n",
      "Epoch 1941/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 3.1053e-04 - accuracy: 1.0000 - val_loss: 0.5326 - val_accuracy: 0.8750\n",
      "Epoch 1942/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 3.0895e-04 - accuracy: 1.0000 - val_loss: 0.5325 - val_accuracy: 0.8750\n",
      "Epoch 1943/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 3.0740e-04 - accuracy: 1.0000 - val_loss: 0.5325 - val_accuracy: 0.8750\n",
      "Epoch 1944/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 3.0587e-04 - accuracy: 1.0000 - val_loss: 0.5324 - val_accuracy: 0.8750\n",
      "Epoch 1945/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 3.0439e-04 - accuracy: 1.0000 - val_loss: 0.5324 - val_accuracy: 0.8750\n",
      "Epoch 1946/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 3.0293e-04 - accuracy: 1.0000 - val_loss: 0.5323 - val_accuracy: 0.8750\n",
      "Epoch 1947/2000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 3.0149e-04 - accuracy: 1.0000 - val_loss: 0.5323 - val_accuracy: 0.8750\n",
      "Epoch 1948/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 3.0008e-04 - accuracy: 1.0000 - val_loss: 0.5322 - val_accuracy: 0.8750\n",
      "Epoch 1949/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 2.9870e-04 - accuracy: 1.0000 - val_loss: 0.5322 - val_accuracy: 0.8750\n",
      "Epoch 1950/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 2.9734e-04 - accuracy: 1.0000 - val_loss: 0.5322 - val_accuracy: 0.8750\n",
      "Epoch 1951/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.9599e-04 - accuracy: 1.0000 - val_loss: 0.5321 - val_accuracy: 0.8750\n",
      "Epoch 1952/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.9469e-04 - accuracy: 1.0000 - val_loss: 0.5321 - val_accuracy: 0.8750\n",
      "Epoch 1953/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.9342e-04 - accuracy: 1.0000 - val_loss: 0.5321 - val_accuracy: 0.8750\n",
      "Epoch 1954/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 2.9218e-04 - accuracy: 1.0000 - val_loss: 0.5321 - val_accuracy: 0.8750\n",
      "Epoch 1955/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.9095e-04 - accuracy: 1.0000 - val_loss: 0.5321 - val_accuracy: 0.8750\n",
      "Epoch 1956/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.8975e-04 - accuracy: 1.0000 - val_loss: 0.5320 - val_accuracy: 0.8750\n",
      "Epoch 1957/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.8856e-04 - accuracy: 1.0000 - val_loss: 0.5320 - val_accuracy: 0.8750\n",
      "Epoch 1958/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.8740e-04 - accuracy: 1.0000 - val_loss: 0.5320 - val_accuracy: 0.8750\n",
      "Epoch 1959/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.8626e-04 - accuracy: 1.0000 - val_loss: 0.5320 - val_accuracy: 0.8750\n",
      "Epoch 1960/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.8514e-04 - accuracy: 1.0000 - val_loss: 0.5319 - val_accuracy: 0.8750\n",
      "Epoch 1961/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.8405e-04 - accuracy: 1.0000 - val_loss: 0.5319 - val_accuracy: 0.8750\n",
      "Epoch 1962/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.8297e-04 - accuracy: 1.0000 - val_loss: 0.5319 - val_accuracy: 0.8750\n",
      "Epoch 1963/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.8187e-04 - accuracy: 1.0000 - val_loss: 0.5319 - val_accuracy: 0.8750\n",
      "Epoch 1964/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 2.8079e-04 - accuracy: 1.0000 - val_loss: 0.5319 - val_accuracy: 0.8750\n",
      "Epoch 1965/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.7972e-04 - accuracy: 1.0000 - val_loss: 0.5319 - val_accuracy: 0.8750\n",
      "Epoch 1966/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 2.7867e-04 - accuracy: 1.0000 - val_loss: 0.5319 - val_accuracy: 0.8750\n",
      "Epoch 1967/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 2.7763e-04 - accuracy: 1.0000 - val_loss: 0.5318 - val_accuracy: 0.8750\n",
      "Epoch 1968/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.7659e-04 - accuracy: 1.0000 - val_loss: 0.5318 - val_accuracy: 0.8750\n",
      "Epoch 1969/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.7558e-04 - accuracy: 1.0000 - val_loss: 0.5318 - val_accuracy: 0.8750\n",
      "Epoch 1970/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.7456e-04 - accuracy: 1.0000 - val_loss: 0.5318 - val_accuracy: 0.8750\n",
      "Epoch 1971/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.7353e-04 - accuracy: 1.0000 - val_loss: 0.5318 - val_accuracy: 0.8750\n",
      "Epoch 1972/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 2.7253e-04 - accuracy: 1.0000 - val_loss: 0.5318 - val_accuracy: 0.8750\n",
      "Epoch 1973/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.7154e-04 - accuracy: 1.0000 - val_loss: 0.5318 - val_accuracy: 0.8750\n",
      "Epoch 1974/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.7059e-04 - accuracy: 1.0000 - val_loss: 0.5317 - val_accuracy: 0.8750\n",
      "Epoch 1975/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 2.6963e-04 - accuracy: 1.0000 - val_loss: 0.5317 - val_accuracy: 0.8750\n",
      "Epoch 1976/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 2.6869e-04 - accuracy: 1.0000 - val_loss: 0.5317 - val_accuracy: 0.8750\n",
      "Epoch 1977/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.6776e-04 - accuracy: 1.0000 - val_loss: 0.5316 - val_accuracy: 0.8750\n",
      "Epoch 1978/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.6684e-04 - accuracy: 1.0000 - val_loss: 0.5315 - val_accuracy: 0.8750\n",
      "Epoch 1979/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 2.6593e-04 - accuracy: 1.0000 - val_loss: 0.5315 - val_accuracy: 0.8750\n",
      "Epoch 1980/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 2.6505e-04 - accuracy: 1.0000 - val_loss: 0.5314 - val_accuracy: 0.8750\n",
      "Epoch 1981/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.6415e-04 - accuracy: 1.0000 - val_loss: 0.5314 - val_accuracy: 0.8750\n",
      "Epoch 1982/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 2.6328e-04 - accuracy: 1.0000 - val_loss: 0.5313 - val_accuracy: 0.8750\n",
      "Epoch 1983/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.6243e-04 - accuracy: 1.0000 - val_loss: 0.5313 - val_accuracy: 0.8750\n",
      "Epoch 1984/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.6158e-04 - accuracy: 1.0000 - val_loss: 0.5312 - val_accuracy: 0.8750\n",
      "Epoch 1985/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.6075e-04 - accuracy: 1.0000 - val_loss: 0.5312 - val_accuracy: 0.8750\n",
      "Epoch 1986/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.5992e-04 - accuracy: 1.0000 - val_loss: 0.5311 - val_accuracy: 0.8750\n",
      "Epoch 1987/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.5908e-04 - accuracy: 1.0000 - val_loss: 0.5311 - val_accuracy: 0.8750\n",
      "Epoch 1988/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.5826e-04 - accuracy: 1.0000 - val_loss: 0.5310 - val_accuracy: 0.8750\n",
      "Epoch 1989/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 2.5747e-04 - accuracy: 1.0000 - val_loss: 0.5309 - val_accuracy: 0.8750\n",
      "Epoch 1990/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.5668e-04 - accuracy: 1.0000 - val_loss: 0.5308 - val_accuracy: 0.8750\n",
      "Epoch 1991/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.5591e-04 - accuracy: 1.0000 - val_loss: 0.5308 - val_accuracy: 0.8750\n",
      "Epoch 1992/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.5514e-04 - accuracy: 1.0000 - val_loss: 0.5307 - val_accuracy: 0.8750\n",
      "Epoch 1993/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.5439e-04 - accuracy: 1.0000 - val_loss: 0.5306 - val_accuracy: 0.8750\n",
      "Epoch 1994/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.5364e-04 - accuracy: 1.0000 - val_loss: 0.5305 - val_accuracy: 0.8750\n",
      "Epoch 1995/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.5290e-04 - accuracy: 1.0000 - val_loss: 0.5305 - val_accuracy: 0.8750\n",
      "Epoch 1996/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.5217e-04 - accuracy: 1.0000 - val_loss: 0.5304 - val_accuracy: 0.8750\n",
      "Epoch 1997/2000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 2.5144e-04 - accuracy: 1.0000 - val_loss: 0.5303 - val_accuracy: 0.8750\n",
      "Epoch 1998/2000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 2.5073e-04 - accuracy: 1.0000 - val_loss: 0.5302 - val_accuracy: 0.8750\n",
      "Epoch 1999/2000\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 2.5000e-04 - accuracy: 1.0000 - val_loss: 0.5301 - val_accuracy: 0.8750\n",
      "Epoch 2000/2000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 2.4932e-04 - accuracy: 1.0000 - val_loss: 0.5300 - val_accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(64)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(64)\n",
    "history = conv_model.fit(train_dataset, epochs=2000, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - History Object \n",
    "\n",
    "The history object is an output of the `.fit()` operation, and provides a record of all the loss and metric values in memory. It's stored as a dictionary that you can retrieve at `history.history`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.8019572496414185,\n",
       "  1.7925302982330322,\n",
       "  1.7898133993148804,\n",
       "  1.7874634265899658,\n",
       "  1.7851066589355469,\n",
       "  1.782565951347351,\n",
       "  1.7798997163772583,\n",
       "  1.7768115997314453,\n",
       "  1.7730380296707153,\n",
       "  1.7666505575180054,\n",
       "  1.7605409622192383,\n",
       "  1.7530797719955444,\n",
       "  1.7442282438278198,\n",
       "  1.733672857284546,\n",
       "  1.7210496664047241,\n",
       "  1.7062945365905762,\n",
       "  1.689037561416626,\n",
       "  1.6698002815246582,\n",
       "  1.6462005376815796,\n",
       "  1.6188411712646484,\n",
       "  1.588638186454773,\n",
       "  1.5558120012283325,\n",
       "  1.5202975273132324,\n",
       "  1.4850893020629883,\n",
       "  1.4495270252227783,\n",
       "  1.414390206336975,\n",
       "  1.3780497312545776,\n",
       "  1.3432353734970093,\n",
       "  1.3085461854934692,\n",
       "  1.273001790046692,\n",
       "  1.239656686782837,\n",
       "  1.2076281309127808,\n",
       "  1.176575779914856,\n",
       "  1.146544337272644,\n",
       "  1.1174259185791016,\n",
       "  1.0905604362487793,\n",
       "  1.0633649826049805,\n",
       "  1.0382777452468872,\n",
       "  1.0131025314331055,\n",
       "  0.9898427128791809,\n",
       "  0.9681819081306458,\n",
       "  0.9469761252403259,\n",
       "  0.9267241954803467,\n",
       "  0.906883716583252,\n",
       "  0.8871070742607117,\n",
       "  0.8690906167030334,\n",
       "  0.8508061766624451,\n",
       "  0.8334395885467529,\n",
       "  0.8181033134460449,\n",
       "  0.8038227558135986,\n",
       "  0.7902641296386719,\n",
       "  0.7758132219314575,\n",
       "  0.7634511590003967,\n",
       "  0.7506417036056519,\n",
       "  0.7388023734092712,\n",
       "  0.7275434136390686,\n",
       "  0.7164029479026794,\n",
       "  0.7059649229049683,\n",
       "  0.6958460807800293,\n",
       "  0.6861391663551331,\n",
       "  0.6770042181015015,\n",
       "  0.6680349707603455,\n",
       "  0.6590743064880371,\n",
       "  0.6512780785560608,\n",
       "  0.6421347260475159,\n",
       "  0.6353893280029297,\n",
       "  0.627069890499115,\n",
       "  0.6201883554458618,\n",
       "  0.612515926361084,\n",
       "  0.6060794591903687,\n",
       "  0.5987035036087036,\n",
       "  0.5926454663276672,\n",
       "  0.5856319069862366,\n",
       "  0.5799920558929443,\n",
       "  0.5733441710472107,\n",
       "  0.5682551860809326,\n",
       "  0.5619490742683411,\n",
       "  0.5561496019363403,\n",
       "  0.5508589148521423,\n",
       "  0.5446705222129822,\n",
       "  0.5393725633621216,\n",
       "  0.5347859859466553,\n",
       "  0.5291869044303894,\n",
       "  0.5247085690498352,\n",
       "  0.5197907090187073,\n",
       "  0.5148293972015381,\n",
       "  0.5106025338172913,\n",
       "  0.5057439804077148,\n",
       "  0.5015224814414978,\n",
       "  0.49715226888656616,\n",
       "  0.49265843629837036,\n",
       "  0.4890094995498657,\n",
       "  0.4848458170890808,\n",
       "  0.48089221119880676,\n",
       "  0.4774450361728668,\n",
       "  0.4732626676559448,\n",
       "  0.4698977470397949,\n",
       "  0.46674132347106934,\n",
       "  0.4628604054450989,\n",
       "  0.45960405468940735,\n",
       "  0.4564904272556305,\n",
       "  0.4528331160545349,\n",
       "  0.4496549963951111,\n",
       "  0.44668352603912354,\n",
       "  0.44329750537872314,\n",
       "  0.4399164617061615,\n",
       "  0.4371967315673828,\n",
       "  0.43412142992019653,\n",
       "  0.43069764971733093,\n",
       "  0.4280332922935486,\n",
       "  0.42521432042121887,\n",
       "  0.42172136902809143,\n",
       "  0.4192793369293213,\n",
       "  0.41630128026008606,\n",
       "  0.41373345255851746,\n",
       "  0.41094207763671875,\n",
       "  0.4080077111721039,\n",
       "  0.4056623578071594,\n",
       "  0.4032687544822693,\n",
       "  0.40052294731140137,\n",
       "  0.39821183681488037,\n",
       "  0.39577174186706543,\n",
       "  0.3933714032173157,\n",
       "  0.39091020822525024,\n",
       "  0.38890624046325684,\n",
       "  0.38642221689224243,\n",
       "  0.3839629292488098,\n",
       "  0.3822152316570282,\n",
       "  0.37987446784973145,\n",
       "  0.37777575850486755,\n",
       "  0.3755938410758972,\n",
       "  0.37339287996292114,\n",
       "  0.3714660704135895,\n",
       "  0.36969882249832153,\n",
       "  0.3673153519630432,\n",
       "  0.36554956436157227,\n",
       "  0.3636782467365265,\n",
       "  0.36146390438079834,\n",
       "  0.35924190282821655,\n",
       "  0.3573916554450989,\n",
       "  0.35533618927001953,\n",
       "  0.3535986542701721,\n",
       "  0.35195717215538025,\n",
       "  0.3497839868068695,\n",
       "  0.34800827503204346,\n",
       "  0.34625640511512756,\n",
       "  0.34438356757164,\n",
       "  0.34252098202705383,\n",
       "  0.34078294038772583,\n",
       "  0.33900803327560425,\n",
       "  0.33752554655075073,\n",
       "  0.33574503660202026,\n",
       "  0.334168016910553,\n",
       "  0.3325405716896057,\n",
       "  0.3309257924556732,\n",
       "  0.3293490409851074,\n",
       "  0.3276077210903168,\n",
       "  0.326113224029541,\n",
       "  0.32454532384872437,\n",
       "  0.3229885995388031,\n",
       "  0.32160040736198425,\n",
       "  0.32012295722961426,\n",
       "  0.31857267022132874,\n",
       "  0.3172074556350708,\n",
       "  0.31562578678131104,\n",
       "  0.3142701983451843,\n",
       "  0.31271475553512573,\n",
       "  0.3114655315876007,\n",
       "  0.3101208209991455,\n",
       "  0.3087446689605713,\n",
       "  0.3071748912334442,\n",
       "  0.3058905303478241,\n",
       "  0.30466192960739136,\n",
       "  0.3033812940120697,\n",
       "  0.3019886016845703,\n",
       "  0.30084773898124695,\n",
       "  0.2994103729724884,\n",
       "  0.2982947528362274,\n",
       "  0.2968701720237732,\n",
       "  0.29558512568473816,\n",
       "  0.29436901211738586,\n",
       "  0.29309096932411194,\n",
       "  0.2920674979686737,\n",
       "  0.29050788283348083,\n",
       "  0.289601594209671,\n",
       "  0.28824782371520996,\n",
       "  0.287232905626297,\n",
       "  0.28593915700912476,\n",
       "  0.2847302556037903,\n",
       "  0.28356489539146423,\n",
       "  0.2824043333530426,\n",
       "  0.28134390711784363,\n",
       "  0.2801189422607422,\n",
       "  0.279112309217453,\n",
       "  0.2779178321361542,\n",
       "  0.2768107056617737,\n",
       "  0.27570921182632446,\n",
       "  0.27462100982666016,\n",
       "  0.27358385920524597,\n",
       "  0.2725885212421417,\n",
       "  0.27147403359413147,\n",
       "  0.2704143822193146,\n",
       "  0.26938122510910034,\n",
       "  0.26846128702163696,\n",
       "  0.26750069856643677,\n",
       "  0.26654547452926636,\n",
       "  0.26558807492256165,\n",
       "  0.2646706700325012,\n",
       "  0.2637440860271454,\n",
       "  0.26289620995521545,\n",
       "  0.2618923783302307,\n",
       "  0.26089540123939514,\n",
       "  0.26000770926475525,\n",
       "  0.2591462731361389,\n",
       "  0.2583068013191223,\n",
       "  0.2574605941772461,\n",
       "  0.2564791738986969,\n",
       "  0.25578340888023376,\n",
       "  0.25478479266166687,\n",
       "  0.2540438175201416,\n",
       "  0.25310349464416504,\n",
       "  0.25229471921920776,\n",
       "  0.2513972818851471,\n",
       "  0.25048789381980896,\n",
       "  0.24957537651062012,\n",
       "  0.24865694344043732,\n",
       "  0.2476840615272522,\n",
       "  0.24679172039031982,\n",
       "  0.24608024954795837,\n",
       "  0.24503985047340393,\n",
       "  0.24394774436950684,\n",
       "  0.24294394254684448,\n",
       "  0.242242231965065,\n",
       "  0.24129121005535126,\n",
       "  0.24036136269569397,\n",
       "  0.23945552110671997,\n",
       "  0.23843061923980713,\n",
       "  0.2374759465456009,\n",
       "  0.236599862575531,\n",
       "  0.23569224774837494,\n",
       "  0.23466886579990387,\n",
       "  0.2339441031217575,\n",
       "  0.2330550104379654,\n",
       "  0.23206381499767303,\n",
       "  0.2311968058347702,\n",
       "  0.23028787970542908,\n",
       "  0.22933627665042877,\n",
       "  0.22859197854995728,\n",
       "  0.22767570614814758,\n",
       "  0.22678831219673157,\n",
       "  0.22585496306419373,\n",
       "  0.22494634985923767,\n",
       "  0.2241005152463913,\n",
       "  0.22323288023471832,\n",
       "  0.22215884923934937,\n",
       "  0.22127975523471832,\n",
       "  0.22031764686107635,\n",
       "  0.2193724662065506,\n",
       "  0.2184411585330963,\n",
       "  0.21752366423606873,\n",
       "  0.21648329496383667,\n",
       "  0.2154291868209839,\n",
       "  0.21454983949661255,\n",
       "  0.2137395292520523,\n",
       "  0.21293894946575165,\n",
       "  0.21196973323822021,\n",
       "  0.2110101729631424,\n",
       "  0.2100668102502823,\n",
       "  0.20925968885421753,\n",
       "  0.20830339193344116,\n",
       "  0.2074359953403473,\n",
       "  0.2066352367401123,\n",
       "  0.20582599937915802,\n",
       "  0.205009326338768,\n",
       "  0.20427265763282776,\n",
       "  0.20345832407474518,\n",
       "  0.20269447565078735,\n",
       "  0.20205622911453247,\n",
       "  0.2012263834476471,\n",
       "  0.20047730207443237,\n",
       "  0.19965921342372894,\n",
       "  0.19900479912757874,\n",
       "  0.1982647180557251,\n",
       "  0.19753773510456085,\n",
       "  0.19679421186447144,\n",
       "  0.1961519867181778,\n",
       "  0.19539247453212738,\n",
       "  0.1947360783815384,\n",
       "  0.19409584999084473,\n",
       "  0.1934480369091034,\n",
       "  0.19277068972587585,\n",
       "  0.19208046793937683,\n",
       "  0.1915026158094406,\n",
       "  0.19090448319911957,\n",
       "  0.190261110663414,\n",
       "  0.18958507478237152,\n",
       "  0.18896086513996124,\n",
       "  0.18832199275493622,\n",
       "  0.1877155601978302,\n",
       "  0.18708571791648865,\n",
       "  0.186476469039917,\n",
       "  0.18595898151397705,\n",
       "  0.1852990835905075,\n",
       "  0.1847490519285202,\n",
       "  0.18417249619960785,\n",
       "  0.1836109310388565,\n",
       "  0.1830592155456543,\n",
       "  0.18251743912696838,\n",
       "  0.18201369047164917,\n",
       "  0.18141841888427734,\n",
       "  0.18084853887557983,\n",
       "  0.18043656647205353,\n",
       "  0.17997905611991882,\n",
       "  0.17942066490650177,\n",
       "  0.17894668877124786,\n",
       "  0.17855548858642578,\n",
       "  0.1780644953250885,\n",
       "  0.17759130895137787,\n",
       "  0.17719267308712006,\n",
       "  0.17668789625167847,\n",
       "  0.17620304226875305,\n",
       "  0.1758403480052948,\n",
       "  0.17536845803260803,\n",
       "  0.17483122646808624,\n",
       "  0.1744113713502884,\n",
       "  0.1739315688610077,\n",
       "  0.17344075441360474,\n",
       "  0.17317765951156616,\n",
       "  0.1726684421300888,\n",
       "  0.17223045229911804,\n",
       "  0.17170383036136627,\n",
       "  0.17140047252178192,\n",
       "  0.1710759997367859,\n",
       "  0.17060425877571106,\n",
       "  0.17026330530643463,\n",
       "  0.16983145475387573,\n",
       "  0.1695413589477539,\n",
       "  0.16912102699279785,\n",
       "  0.16874545812606812,\n",
       "  0.16833552718162537,\n",
       "  0.16800221800804138,\n",
       "  0.16762876510620117,\n",
       "  0.16729792952537537,\n",
       "  0.1669035702943802,\n",
       "  0.1665855050086975,\n",
       "  0.16623379290103912,\n",
       "  0.16579069197177887,\n",
       "  0.16550570726394653,\n",
       "  0.1650538146495819,\n",
       "  0.16466599702835083,\n",
       "  0.16429947316646576,\n",
       "  0.16394644975662231,\n",
       "  0.16353869438171387,\n",
       "  0.1632038652896881,\n",
       "  0.1628137230873108,\n",
       "  0.16250044107437134,\n",
       "  0.16218215227127075,\n",
       "  0.16174855828285217,\n",
       "  0.1614643782377243,\n",
       "  0.16102920472621918,\n",
       "  0.16072531044483185,\n",
       "  0.16027817130088806,\n",
       "  0.15994535386562347,\n",
       "  0.1596134752035141,\n",
       "  0.15920957922935486,\n",
       "  0.1587226837873459,\n",
       "  0.1583564132452011,\n",
       "  0.15797056257724762,\n",
       "  0.15754568576812744,\n",
       "  0.1571236103773117,\n",
       "  0.15671861171722412,\n",
       "  0.1563221514225006,\n",
       "  0.15603254735469818,\n",
       "  0.15542203187942505,\n",
       "  0.1550656408071518,\n",
       "  0.1546313762664795,\n",
       "  0.15422338247299194,\n",
       "  0.15370585024356842,\n",
       "  0.1531810760498047,\n",
       "  0.15281838178634644,\n",
       "  0.15231269598007202,\n",
       "  0.15187276899814606,\n",
       "  0.15142835676670074,\n",
       "  0.15104204416275024,\n",
       "  0.15067468583583832,\n",
       "  0.15023083984851837,\n",
       "  0.14976465702056885,\n",
       "  0.14941151440143585,\n",
       "  0.14891810715198517,\n",
       "  0.14835461974143982,\n",
       "  0.14788314700126648,\n",
       "  0.14743070304393768,\n",
       "  0.14702734351158142,\n",
       "  0.1466846615076065,\n",
       "  0.14609496295452118,\n",
       "  0.14567962288856506,\n",
       "  0.1451255828142166,\n",
       "  0.1445804387331009,\n",
       "  0.14425621926784515,\n",
       "  0.1437397599220276,\n",
       "  0.143158957362175,\n",
       "  0.1429077386856079,\n",
       "  0.14215001463890076,\n",
       "  0.14167720079421997,\n",
       "  0.1413453370332718,\n",
       "  0.1406305581331253,\n",
       "  0.1400083303451538,\n",
       "  0.1397562026977539,\n",
       "  0.13900157809257507,\n",
       "  0.13858598470687866,\n",
       "  0.13806776702404022,\n",
       "  0.13749366998672485,\n",
       "  0.13694238662719727,\n",
       "  0.1364150047302246,\n",
       "  0.13581708073616028,\n",
       "  0.13516950607299805,\n",
       "  0.13469861447811127,\n",
       "  0.13419564068317413,\n",
       "  0.13352911174297333,\n",
       "  0.13304324448108673,\n",
       "  0.13250449299812317,\n",
       "  0.13194262981414795,\n",
       "  0.1314973533153534,\n",
       "  0.13090555369853973,\n",
       "  0.1304561346769333,\n",
       "  0.1299639195203781,\n",
       "  0.12943394482135773,\n",
       "  0.12894590198993683,\n",
       "  0.12848973274230957,\n",
       "  0.12792591750621796,\n",
       "  0.12745019793510437,\n",
       "  0.1270371824502945,\n",
       "  0.12647601962089539,\n",
       "  0.12601007521152496,\n",
       "  0.12552334368228912,\n",
       "  0.1250273585319519,\n",
       "  0.12451742589473724,\n",
       "  0.1240149736404419,\n",
       "  0.12357210367918015,\n",
       "  0.12307841330766678,\n",
       "  0.12264455109834671,\n",
       "  0.1221366599202156,\n",
       "  0.12164372950792313,\n",
       "  0.12113595008850098,\n",
       "  0.12059011310338974,\n",
       "  0.12023550271987915,\n",
       "  0.11970517039299011,\n",
       "  0.11922624707221985,\n",
       "  0.11873415857553482,\n",
       "  0.11833813786506653,\n",
       "  0.11787956207990646,\n",
       "  0.1174091026186943,\n",
       "  0.11702129989862442,\n",
       "  0.11664987355470657,\n",
       "  0.11620902270078659,\n",
       "  0.11580634862184525,\n",
       "  0.11542332917451859,\n",
       "  0.11499399691820145,\n",
       "  0.11456481367349625,\n",
       "  0.1142173781991005,\n",
       "  0.11387572437524796,\n",
       "  0.11339201778173447,\n",
       "  0.11298960447311401,\n",
       "  0.1126319169998169,\n",
       "  0.11225026100873947,\n",
       "  0.11187505722045898,\n",
       "  0.11139050871133804,\n",
       "  0.11100108921527863,\n",
       "  0.11059767752885818,\n",
       "  0.11014196276664734,\n",
       "  0.1096915528178215,\n",
       "  0.10930513590574265,\n",
       "  0.10891450196504593,\n",
       "  0.10855105519294739,\n",
       "  0.10822726786136627,\n",
       "  0.10780435055494308,\n",
       "  0.10739900171756744,\n",
       "  0.10704640299081802,\n",
       "  0.1065821647644043,\n",
       "  0.10623720288276672,\n",
       "  0.10578811913728714,\n",
       "  0.1054147332906723,\n",
       "  0.1050715371966362,\n",
       "  0.10465694963932037,\n",
       "  0.10430482029914856,\n",
       "  0.10390790551900864,\n",
       "  0.10348784178495407,\n",
       "  0.10317722707986832,\n",
       "  0.10284754633903503,\n",
       "  0.10251515358686447,\n",
       "  0.10209603607654572,\n",
       "  0.10167089104652405,\n",
       "  0.1013801246881485,\n",
       "  0.10098077356815338,\n",
       "  0.10062341392040253,\n",
       "  0.10031413286924362,\n",
       "  0.09995737671852112,\n",
       "  0.0995936468243599,\n",
       "  0.09928151965141296,\n",
       "  0.09900189936161041,\n",
       "  0.0986253172159195,\n",
       "  0.09833705425262451,\n",
       "  0.09797953069210052,\n",
       "  0.09764251112937927,\n",
       "  0.09735757112503052,\n",
       "  0.09699547290802002,\n",
       "  0.0966704860329628,\n",
       "  0.09639204293489456,\n",
       "  0.09611490368843079,\n",
       "  0.09575112164020538,\n",
       "  0.09545406699180603,\n",
       "  0.09508320689201355,\n",
       "  0.09482459723949432,\n",
       "  0.09460937976837158,\n",
       "  0.09425855427980423,\n",
       "  0.09393314272165298,\n",
       "  0.09360276162624359,\n",
       "  0.09330815821886063,\n",
       "  0.09294396638870239,\n",
       "  0.09267201274633408,\n",
       "  0.09233060479164124,\n",
       "  0.09204765409231186,\n",
       "  0.0917181521654129,\n",
       "  0.09139100462198257,\n",
       "  0.09111335873603821,\n",
       "  0.09083159267902374,\n",
       "  0.09055064618587494,\n",
       "  0.09020879864692688,\n",
       "  0.08985292911529541,\n",
       "  0.0896022841334343,\n",
       "  0.08923251181840897,\n",
       "  0.08895459771156311,\n",
       "  0.08865558356046677,\n",
       "  0.08833745867013931,\n",
       "  0.08802630752325058,\n",
       "  0.08774124830961227,\n",
       "  0.0873737633228302,\n",
       "  0.08708351105451584,\n",
       "  0.08677707612514496,\n",
       "  0.0865299180150032,\n",
       "  0.08614938706159592,\n",
       "  0.08583693951368332,\n",
       "  0.08555050939321518,\n",
       "  0.08528263121843338,\n",
       "  0.08494805544614792,\n",
       "  0.08463136106729507,\n",
       "  0.08436387777328491,\n",
       "  0.0840427428483963,\n",
       "  0.0837780088186264,\n",
       "  0.08349071443080902,\n",
       "  0.08315062522888184,\n",
       "  0.08287711441516876,\n",
       "  0.08258949220180511,\n",
       "  0.08224977552890778,\n",
       "  0.08198052644729614,\n",
       "  0.08172831684350967,\n",
       "  0.08146098256111145,\n",
       "  0.08114967495203018,\n",
       "  0.08081737905740738,\n",
       "  0.08054181188344955,\n",
       "  0.0802423432469368,\n",
       "  0.07991821318864822,\n",
       "  0.07968755066394806,\n",
       "  0.07937338948249817,\n",
       "  0.0791165828704834,\n",
       "  0.07887323945760727,\n",
       "  0.07856012880802155,\n",
       "  0.07824280112981796,\n",
       "  0.07797618955373764,\n",
       "  0.07769649475812912,\n",
       "  0.07751373201608658,\n",
       "  0.07723098993301392,\n",
       "  0.07693787664175034,\n",
       "  0.07668787240982056,\n",
       "  0.07646016031503677,\n",
       "  0.07617371529340744,\n",
       "  0.07589132338762283,\n",
       "  0.07567410171031952,\n",
       "  0.07540953159332275,\n",
       "  0.07514782249927521,\n",
       "  0.07488363981246948,\n",
       "  0.07463069260120392,\n",
       "  0.07441095262765884,\n",
       "  0.07417817413806915,\n",
       "  0.07389075309038162,\n",
       "  0.07355339080095291,\n",
       "  0.07337239384651184,\n",
       "  0.07308221608400345,\n",
       "  0.07280085235834122,\n",
       "  0.07261653989553452,\n",
       "  0.0724240317940712,\n",
       "  0.07210438698530197,\n",
       "  0.07186032086610794,\n",
       "  0.07164943963289261,\n",
       "  0.07138613611459732,\n",
       "  0.07108361274003983,\n",
       "  0.07084406167268753,\n",
       "  0.07052413374185562,\n",
       "  0.07030333578586578,\n",
       "  0.07013855129480362,\n",
       "  0.06982837617397308,\n",
       "  0.06955166906118393,\n",
       "  0.06936898082494736,\n",
       "  0.0690702497959137,\n",
       "  0.06877470761537552,\n",
       "  0.06857892125844955,\n",
       "  0.06829602271318436,\n",
       "  0.0680585652589798,\n",
       "  0.06775305420160294,\n",
       "  0.06754202395677567,\n",
       "  0.0672590434551239,\n",
       "  0.06699255108833313,\n",
       "  0.06676903367042542,\n",
       "  0.0664948895573616,\n",
       "  0.06627306342124939,\n",
       "  0.06601086258888245,\n",
       "  0.06578817963600159,\n",
       "  0.06557759642601013,\n",
       "  0.06524171680212021,\n",
       "  0.0650024265050888,\n",
       "  0.06482003629207611,\n",
       "  0.06451932340860367,\n",
       "  0.06428246200084686,\n",
       "  0.06399320811033249,\n",
       "  0.06375490128993988,\n",
       "  0.06356186419725418,\n",
       "  0.06322945654392242,\n",
       "  0.06302063912153244,\n",
       "  0.06272774189710617,\n",
       "  0.06254155188798904,\n",
       "  0.06229216232895851,\n",
       "  0.06209729611873627,\n",
       "  0.06185678020119667,\n",
       "  0.061627987772226334,\n",
       "  0.06136822700500488,\n",
       "  0.061146702617406845,\n",
       "  0.060922227799892426,\n",
       "  0.06065991148352623,\n",
       "  0.060403041541576385,\n",
       "  0.06023271381855011,\n",
       "  0.05996768921613693,\n",
       "  0.059817079454660416,\n",
       "  0.05953415110707283,\n",
       "  0.05929950252175331,\n",
       "  0.05907243490219116,\n",
       "  0.05889575928449631,\n",
       "  0.05862898752093315,\n",
       "  0.05839540436863899,\n",
       "  0.05819910764694214,\n",
       "  0.05794161558151245,\n",
       "  0.05775342136621475,\n",
       "  0.05748710036277771,\n",
       "  0.05723130330443382,\n",
       "  0.05702318623661995,\n",
       "  0.056796845048666,\n",
       "  0.05652496963739395,\n",
       "  0.05635243281722069,\n",
       "  0.05606060475111008,\n",
       "  0.055917587131261826,\n",
       "  0.05571029335260391,\n",
       "  0.05550503730773926,\n",
       "  0.05527404695749283,\n",
       "  0.055020708590745926,\n",
       "  0.05482581630349159,\n",
       "  0.05458766222000122,\n",
       "  0.05438863858580589,\n",
       "  0.05415065959095955,\n",
       "  0.05399244278669357,\n",
       "  0.05373472347855568,\n",
       "  0.053607892245054245,\n",
       "  0.05338139459490776,\n",
       "  0.053183455020189285,\n",
       "  0.05301300436258316,\n",
       "  0.05282830446958542,\n",
       "  0.05256189405918121,\n",
       "  0.05242614075541496,\n",
       "  0.05215469002723694,\n",
       "  0.05203070864081383,\n",
       "  0.05183793231844902,\n",
       "  0.05165640637278557,\n",
       "  0.05149015411734581,\n",
       "  0.051299531012773514,\n",
       "  0.051140472292900085,\n",
       "  0.05092586576938629,\n",
       "  0.05073561519384384,\n",
       "  0.05059481039643288,\n",
       "  0.05046023800969124,\n",
       "  0.05022844299674034,\n",
       "  0.050035279244184494,\n",
       "  0.04986770823597908,\n",
       "  0.04961743205785751,\n",
       "  0.049502886831760406,\n",
       "  0.04932011663913727,\n",
       "  0.04913336783647537,\n",
       "  0.04893164336681366,\n",
       "  0.04875461757183075,\n",
       "  0.04863791912794113,\n",
       "  0.04847588762640953,\n",
       "  0.04833951219916344,\n",
       "  0.04811687767505646,\n",
       "  0.04798753187060356,\n",
       "  0.04773158207535744,\n",
       "  0.04759470373392105,\n",
       "  0.047476865351200104,\n",
       "  0.047272805124521255,\n",
       "  0.047147203236818314,\n",
       "  0.04695332050323486,\n",
       "  0.04682081937789917,\n",
       "  0.04658610001206398,\n",
       "  0.046440791338682175,\n",
       "  0.04623348265886307,\n",
       "  0.04611717164516449,\n",
       "  0.04599669575691223,\n",
       "  0.04578307643532753,\n",
       "  0.045640978962183,\n",
       "  0.045401185750961304,\n",
       "  0.04532789811491966,\n",
       "  0.0452011339366436,\n",
       "  0.04496845230460167,\n",
       "  0.044854238629341125,\n",
       "  0.04463975504040718,\n",
       "  0.044513970613479614,\n",
       "  0.04436754435300827,\n",
       "  0.04422279819846153,\n",
       "  0.04406763240695,\n",
       "  0.04389015957713127,\n",
       "  0.04376774653792381,\n",
       "  0.043585002422332764,\n",
       "  0.04342398792505264,\n",
       "  0.04326144605875015,\n",
       "  0.043113451451063156,\n",
       "  0.04295194149017334,\n",
       "  0.04285266250371933,\n",
       "  0.04264964535832405,\n",
       "  0.042490534484386444,\n",
       "  0.0423465371131897,\n",
       "  0.042217135429382324,\n",
       "  0.042051754891872406,\n",
       "  0.041892439126968384,\n",
       "  0.04170142859220505,\n",
       "  0.04163708537817001,\n",
       "  0.04148509353399277,\n",
       "  0.041309237480163574,\n",
       "  0.04109714925289154,\n",
       "  0.04095283895730972,\n",
       "  0.040805622935295105,\n",
       "  0.0406477265059948,\n",
       "  0.04056451469659805,\n",
       "  0.04038180783390999,\n",
       "  0.04024304822087288,\n",
       "  0.04006181284785271,\n",
       "  0.03989534080028534,\n",
       "  0.039783645421266556,\n",
       "  0.03962177038192749,\n",
       "  0.03946368768811226,\n",
       "  0.039367616176605225,\n",
       "  0.03918539732694626,\n",
       "  0.03905229642987251,\n",
       "  0.038883060216903687,\n",
       "  0.038788069039583206,\n",
       "  0.03865315765142441,\n",
       "  0.03849654272198677,\n",
       "  0.03838714584708214,\n",
       "  0.0382048599421978,\n",
       "  0.038118381053209305,\n",
       "  0.03792882710695267,\n",
       "  0.03779539093375206,\n",
       "  0.03760622441768646,\n",
       "  0.037445951253175735,\n",
       "  0.03733225166797638,\n",
       "  0.03717958182096481,\n",
       "  0.037058085203170776,\n",
       "  0.03693464770913124,\n",
       "  0.036820877343416214,\n",
       "  0.03666755557060242,\n",
       "  0.036531712859869,\n",
       "  0.03638856112957001,\n",
       "  0.036245256662368774,\n",
       "  0.03614600747823715,\n",
       "  0.03600716590881348,\n",
       "  0.03587675839662552,\n",
       "  0.035681284964084625,\n",
       "  0.035519152879714966,\n",
       "  0.03547777235507965,\n",
       "  0.03534615412354469,\n",
       "  0.03517404943704605,\n",
       "  0.0350690558552742,\n",
       "  0.0349283367395401,\n",
       "  0.034897446632385254,\n",
       "  0.03473855182528496,\n",
       "  0.0346941277384758,\n",
       "  0.0345480851829052,\n",
       "  0.03435775637626648,\n",
       "  0.03427053242921829,\n",
       "  0.03416036441922188,\n",
       "  0.03393828496336937,\n",
       "  0.033897846937179565,\n",
       "  0.03368241339921951,\n",
       "  0.03362909331917763,\n",
       "  0.033468861132860184,\n",
       "  0.033352505415678024,\n",
       "  0.03322404995560646,\n",
       "  0.03311038389801979,\n",
       "  0.032981012016534805,\n",
       "  0.03281066194176674,\n",
       "  0.03265954554080963,\n",
       "  0.03255132585763931,\n",
       "  0.03228215500712395,\n",
       "  0.032220132648944855,\n",
       "  0.03207549825310707,\n",
       "  0.0319395586848259,\n",
       "  0.03170425072312355,\n",
       "  0.03166002407670021,\n",
       "  0.03147398307919502,\n",
       "  0.03131803870201111,\n",
       "  0.031125754117965698,\n",
       "  0.03102731704711914,\n",
       "  0.030845236033201218,\n",
       "  0.030711336061358452,\n",
       "  0.030613206326961517,\n",
       "  0.030428200960159302,\n",
       "  0.03028779849410057,\n",
       "  0.030095156282186508,\n",
       "  0.030050231143832207,\n",
       "  0.029854753986001015,\n",
       "  0.02968817763030529,\n",
       "  0.029526058584451675,\n",
       "  0.029361002147197723,\n",
       "  0.029263107106089592,\n",
       "  0.029068177565932274,\n",
       "  0.028935527428984642,\n",
       "  0.02875826507806778,\n",
       "  0.02863089181482792,\n",
       "  0.028428571298718452,\n",
       "  0.028347620740532875,\n",
       "  0.028152598068118095,\n",
       "  0.02796912007033825,\n",
       "  0.02786247245967388,\n",
       "  0.02769659459590912,\n",
       "  0.027544153854250908,\n",
       "  0.02745867148041725,\n",
       "  0.027271706610918045,\n",
       "  0.027132773771882057,\n",
       "  0.027027208358049393,\n",
       "  0.026867199689149857,\n",
       "  0.026696525514125824,\n",
       "  0.026570873335003853,\n",
       "  0.026466606184840202,\n",
       "  0.02630918100476265,\n",
       "  0.02614680863916874,\n",
       "  0.026093943044543266,\n",
       "  0.02594657614827156,\n",
       "  0.025774706155061722,\n",
       "  0.025678513571619987,\n",
       "  0.02550918236374855,\n",
       "  0.025369344279170036,\n",
       "  0.02529052458703518,\n",
       "  0.025160573422908783,\n",
       "  0.025013107806444168,\n",
       "  0.024903887882828712,\n",
       "  0.024784235283732414,\n",
       "  0.024671265855431557,\n",
       "  0.024576159194111824,\n",
       "  0.024461999535560608,\n",
       "  0.02431156486272812,\n",
       "  0.024218089878559113,\n",
       "  0.0240629892796278,\n",
       "  0.02395222894847393,\n",
       "  0.02384987100958824,\n",
       "  0.02369728684425354,\n",
       "  0.023623699322342873,\n",
       "  0.023453930392861366,\n",
       "  0.02335771545767784,\n",
       "  0.02328599989414215,\n",
       "  0.023133616894483566,\n",
       "  0.023041019216179848,\n",
       "  0.022933129221200943,\n",
       "  0.022824883460998535,\n",
       "  0.02273913472890854,\n",
       "  0.022615689784288406,\n",
       "  0.022511739283800125,\n",
       "  0.02240362949669361,\n",
       "  0.02232978492975235,\n",
       "  0.022211192175745964,\n",
       "  0.022113634273409843,\n",
       "  0.022006560117006302,\n",
       "  0.021895989775657654,\n",
       "  0.021809669211506844,\n",
       "  0.021725153550505638,\n",
       "  0.021589092910289764,\n",
       "  0.02152121067047119,\n",
       "  0.02140616439282894,\n",
       "  0.02130294032394886,\n",
       "  0.02120070345699787,\n",
       "  0.021101869642734528,\n",
       "  0.021035771816968918,\n",
       "  0.020932525396347046,\n",
       "  0.02087625302374363,\n",
       "  0.020735476166009903,\n",
       "  0.02066907472908497,\n",
       "  0.02057933248579502,\n",
       "  0.020426280796527863,\n",
       "  0.020391028374433517,\n",
       "  0.020304877310991287,\n",
       "  0.020198529586195946,\n",
       "  0.02012980356812477,\n",
       "  0.019998770207166672,\n",
       "  0.019932353869080544,\n",
       "  0.01985306479036808,\n",
       "  0.019713008776307106,\n",
       "  0.019637737423181534,\n",
       "  0.019577903673052788,\n",
       "  0.01946706511080265,\n",
       "  0.01939353160560131,\n",
       "  0.019334783777594566,\n",
       "  0.019212335348129272,\n",
       "  0.019129004329442978,\n",
       "  0.01898912526667118,\n",
       "  0.018964730203151703,\n",
       "  0.018863754346966743,\n",
       "  0.018709147348999977,\n",
       "  0.018651029095053673,\n",
       "  0.01857812888920307,\n",
       "  0.01847858354449272,\n",
       "  0.018434975296258926,\n",
       "  0.01829625479876995,\n",
       "  0.01819981448352337,\n",
       "  0.01811501383781433,\n",
       "  0.018007783219218254,\n",
       "  0.01797010377049446,\n",
       "  0.017829393967986107,\n",
       "  0.017712339758872986,\n",
       "  0.017674636095762253,\n",
       "  0.017539653927087784,\n",
       "  0.01751522719860077,\n",
       "  0.017371537163853645,\n",
       "  0.017269758507609367,\n",
       "  0.01719648204743862,\n",
       "  0.017136234790086746,\n",
       "  0.016997184604406357,\n",
       "  0.01689504086971283,\n",
       "  0.016820063814520836,\n",
       "  0.01674383133649826,\n",
       "  0.0166410394012928,\n",
       "  0.01655423641204834,\n",
       "  0.016465798020362854,\n",
       "  0.01636572554707527,\n",
       "  0.01629454456269741,\n",
       "  0.01617247797548771,\n",
       "  0.016118694096803665,\n",
       "  0.016009673476219177,\n",
       "  0.015905970707535744,\n",
       "  0.015838060528039932,\n",
       "  0.01571720466017723,\n",
       "  0.01567750610411167,\n",
       "  0.015527977608144283,\n",
       "  0.015463809482753277,\n",
       "  0.015362130478024483,\n",
       "  0.015288055874407291,\n",
       "  0.015202894806861877,\n",
       "  0.015119580551981926,\n",
       "  0.015011397190392017,\n",
       "  0.014991476200520992,\n",
       "  0.01482667401432991,\n",
       "  0.014788903295993805,\n",
       "  0.014664387330412865,\n",
       "  0.014632640406489372,\n",
       "  0.014528647996485233,\n",
       "  0.01444273442029953,\n",
       "  0.014376971870660782,\n",
       "  0.014287563972175121,\n",
       "  0.014199156314134598,\n",
       "  0.014149789698421955,\n",
       "  0.01406870037317276,\n",
       "  0.013991347514092922,\n",
       "  0.013911763206124306,\n",
       "  0.013865133747458458,\n",
       "  0.013801402412354946,\n",
       "  0.013737667351961136,\n",
       "  0.013654470443725586,\n",
       "  0.013591891154646873,\n",
       "  0.013515865430235863,\n",
       "  0.013456576503813267,\n",
       "  0.013415482826530933,\n",
       "  0.013333245180547237,\n",
       "  0.013281834311783314,\n",
       "  0.013208010233938694,\n",
       "  0.013138391077518463,\n",
       "  0.013100980781018734,\n",
       "  0.013044433668255806,\n",
       "  0.013000342063605785,\n",
       "  0.012914617545902729,\n",
       "  0.01283930242061615,\n",
       "  0.012844203040003777,\n",
       "  0.012715333141386509,\n",
       "  0.012692186050117016,\n",
       "  0.01265681441873312,\n",
       "  0.01260423008352518,\n",
       "  0.012518411502242088,\n",
       "  0.012458727695047855,\n",
       "  ...],\n",
       " 'accuracy': [0.14074073731899261,\n",
       "  0.1935185194015503,\n",
       "  0.18333333730697632,\n",
       "  0.1666666716337204,\n",
       "  0.18703703582286835,\n",
       "  0.19629628956317902,\n",
       "  0.21574074029922485,\n",
       "  0.21759259700775146,\n",
       "  0.2361111044883728,\n",
       "  0.27407407760620117,\n",
       "  0.2759259343147278,\n",
       "  0.3157407343387604,\n",
       "  0.3444444537162781,\n",
       "  0.3712962865829468,\n",
       "  0.38333332538604736,\n",
       "  0.39907407760620117,\n",
       "  0.41111111640930176,\n",
       "  0.4148148000240326,\n",
       "  0.4342592656612396,\n",
       "  0.44999998807907104,\n",
       "  0.4481481611728668,\n",
       "  0.45370370149612427,\n",
       "  0.48055556416511536,\n",
       "  0.4851851761341095,\n",
       "  0.4990740716457367,\n",
       "  0.510185182094574,\n",
       "  0.5166666507720947,\n",
       "  0.5388888716697693,\n",
       "  0.5481481552124023,\n",
       "  0.5611110925674438,\n",
       "  0.5768518447875977,\n",
       "  0.5907407402992249,\n",
       "  0.5916666388511658,\n",
       "  0.6018518805503845,\n",
       "  0.6259258985519409,\n",
       "  0.6388888955116272,\n",
       "  0.6509259343147278,\n",
       "  0.6574074029922485,\n",
       "  0.675000011920929,\n",
       "  0.6805555820465088,\n",
       "  0.6805555820465088,\n",
       "  0.6833333373069763,\n",
       "  0.6925926208496094,\n",
       "  0.7083333134651184,\n",
       "  0.7138888835906982,\n",
       "  0.7240740656852722,\n",
       "  0.7287036776542664,\n",
       "  0.7342592477798462,\n",
       "  0.7361111044883728,\n",
       "  0.7416666746139526,\n",
       "  0.7435185313224792,\n",
       "  0.75,\n",
       "  0.7509258985519409,\n",
       "  0.7555555701255798,\n",
       "  0.7592592835426331,\n",
       "  0.7629629373550415,\n",
       "  0.7722222208976746,\n",
       "  0.7777777910232544,\n",
       "  0.7814815044403076,\n",
       "  0.7861111164093018,\n",
       "  0.7888888716697693,\n",
       "  0.7925925850868225,\n",
       "  0.7953703999519348,\n",
       "  0.7981481552124023,\n",
       "  0.8018518686294556,\n",
       "  0.800000011920929,\n",
       "  0.8064814805984497,\n",
       "  0.8074073791503906,\n",
       "  0.8083333373069763,\n",
       "  0.8074073791503906,\n",
       "  0.8111110925674438,\n",
       "  0.8120370507240295,\n",
       "  0.8138889074325562,\n",
       "  0.8138889074325562,\n",
       "  0.8157407641410828,\n",
       "  0.8194444179534912,\n",
       "  0.8185185194015503,\n",
       "  0.8203703761100769,\n",
       "  0.8212962746620178,\n",
       "  0.824999988079071,\n",
       "  0.8277778029441833,\n",
       "  0.8305555582046509,\n",
       "  0.8342592716217041,\n",
       "  0.8342592716217041,\n",
       "  0.8342592716217041,\n",
       "  0.8370370268821716,\n",
       "  0.8379629850387573,\n",
       "  0.8388888835906982,\n",
       "  0.8425925970077515,\n",
       "  0.8435184955596924,\n",
       "  0.845370352268219,\n",
       "  0.8481481671333313,\n",
       "  0.8537036776542664,\n",
       "  0.8509259223937988,\n",
       "  0.8537036776542664,\n",
       "  0.8574073910713196,\n",
       "  0.855555534362793,\n",
       "  0.8564814925193787,\n",
       "  0.8592592477798462,\n",
       "  0.8583333492279053,\n",
       "  0.8592592477798462,\n",
       "  0.8629629611968994,\n",
       "  0.864814817905426,\n",
       "  0.864814817905426,\n",
       "  0.8694444298744202,\n",
       "  0.8675925731658936,\n",
       "  0.8675925731658936,\n",
       "  0.8685185313224792,\n",
       "  0.8712962865829468,\n",
       "  0.8694444298744202,\n",
       "  0.8703703880310059,\n",
       "  0.8712962865829468,\n",
       "  0.8694444298744202,\n",
       "  0.8731481432914734,\n",
       "  0.8722222447395325,\n",
       "  0.8722222447395325,\n",
       "  0.8740741014480591,\n",
       "  0.8768518567085266,\n",
       "  0.8759258985519409,\n",
       "  0.8787037134170532,\n",
       "  0.8787037134170532,\n",
       "  0.8805555701255798,\n",
       "  0.8814814686775208,\n",
       "  0.8824074268341064,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8842592835426331,\n",
       "  0.8870370388031006,\n",
       "  0.8861111402511597,\n",
       "  0.8870370388031006,\n",
       "  0.8861111402511597,\n",
       "  0.8870370388031006,\n",
       "  0.8879629373550415,\n",
       "  0.8879629373550415,\n",
       "  0.8888888955116272,\n",
       "  0.8907407522201538,\n",
       "  0.8907407522201538,\n",
       "  0.8888888955116272,\n",
       "  0.8925926089286804,\n",
       "  0.8925926089286804,\n",
       "  0.8925926089286804,\n",
       "  0.8925926089286804,\n",
       "  0.894444465637207,\n",
       "  0.894444465637207,\n",
       "  0.8962963223457336,\n",
       "  0.8972222208976746,\n",
       "  0.8981481194496155,\n",
       "  0.9009259343147278,\n",
       "  0.8990740776062012,\n",
       "  0.9027777910232544,\n",
       "  0.8990740776062012,\n",
       "  0.9018518328666687,\n",
       "  0.9037036895751953,\n",
       "  0.904629647731781,\n",
       "  0.9064815044403076,\n",
       "  0.9064815044403076,\n",
       "  0.9074074029922485,\n",
       "  0.9083333611488342,\n",
       "  0.9074074029922485,\n",
       "  0.9074074029922485,\n",
       "  0.9074074029922485,\n",
       "  0.9092592597007751,\n",
       "  0.9092592597007751,\n",
       "  0.9101851582527161,\n",
       "  0.9101851582527161,\n",
       "  0.9120370149612427,\n",
       "  0.9101851582527161,\n",
       "  0.9111111164093018,\n",
       "  0.9120370149612427,\n",
       "  0.9129629731178284,\n",
       "  0.9129629731178284,\n",
       "  0.9138888716697693,\n",
       "  0.9129629731178284,\n",
       "  0.9138888716697693,\n",
       "  0.9129629731178284,\n",
       "  0.9138888716697693,\n",
       "  0.9138888716697693,\n",
       "  0.9138888716697693,\n",
       "  0.914814829826355,\n",
       "  0.9157407283782959,\n",
       "  0.9157407283782959,\n",
       "  0.9157407283782959,\n",
       "  0.9157407283782959,\n",
       "  0.9166666865348816,\n",
       "  0.9157407283782959,\n",
       "  0.9175925850868225,\n",
       "  0.9157407283782959,\n",
       "  0.9157407283782959,\n",
       "  0.9175925850868225,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9157407283782959,\n",
       "  0.9157407283782959,\n",
       "  0.9157407283782959,\n",
       "  0.9157407283782959,\n",
       "  0.9166666865348816,\n",
       "  0.9175925850868225,\n",
       "  0.9175925850868225,\n",
       "  0.9175925850868225,\n",
       "  0.9175925850868225,\n",
       "  0.9175925850868225,\n",
       "  0.9185185432434082,\n",
       "  0.9175925850868225,\n",
       "  0.9185185432434082,\n",
       "  0.9203703999519348,\n",
       "  0.9203703999519348,\n",
       "  0.9222221970558167,\n",
       "  0.9203703999519348,\n",
       "  0.9222221970558167,\n",
       "  0.9222221970558167,\n",
       "  0.9222221970558167,\n",
       "  0.9231481552124023,\n",
       "  0.9231481552124023,\n",
       "  0.9240740537643433,\n",
       "  0.9222221970558167,\n",
       "  0.9240740537643433,\n",
       "  0.9259259104728699,\n",
       "  0.9240740537643433,\n",
       "  0.925000011920929,\n",
       "  0.925000011920929,\n",
       "  0.925000011920929,\n",
       "  0.9240740537643433,\n",
       "  0.9240740537643433,\n",
       "  0.9240740537643433,\n",
       "  0.9259259104728699,\n",
       "  0.925000011920929,\n",
       "  0.9268518686294556,\n",
       "  0.9259259104728699,\n",
       "  0.9268518686294556,\n",
       "  0.9277777671813965,\n",
       "  0.9277777671813965,\n",
       "  0.9277777671813965,\n",
       "  0.9277777671813965,\n",
       "  0.9277777671813965,\n",
       "  0.9296296238899231,\n",
       "  0.9296296238899231,\n",
       "  0.9305555820465088,\n",
       "  0.9296296238899231,\n",
       "  0.9305555820465088,\n",
       "  0.9296296238899231,\n",
       "  0.9324073791503906,\n",
       "  0.9324073791503906,\n",
       "  0.9314814805984497,\n",
       "  0.9324073791503906,\n",
       "  0.9324073791503906,\n",
       "  0.9333333373069763,\n",
       "  0.9342592358589172,\n",
       "  0.9342592358589172,\n",
       "  0.9342592358589172,\n",
       "  0.9342592358589172,\n",
       "  0.9342592358589172,\n",
       "  0.9342592358589172,\n",
       "  0.9351851940155029,\n",
       "  0.9361110925674438,\n",
       "  0.9361110925674438,\n",
       "  0.9361110925674438,\n",
       "  0.9361110925674438,\n",
       "  0.9361110925674438,\n",
       "  0.9361110925674438,\n",
       "  0.9351851940155029,\n",
       "  0.9351851940155029,\n",
       "  0.9342592358589172,\n",
       "  0.9342592358589172,\n",
       "  0.9342592358589172,\n",
       "  0.9342592358589172,\n",
       "  0.9333333373069763,\n",
       "  0.9351851940155029,\n",
       "  0.9351851940155029,\n",
       "  0.9370370507240295,\n",
       "  0.9370370507240295,\n",
       "  0.9388889074325562,\n",
       "  0.9388889074325562,\n",
       "  0.9388889074325562,\n",
       "  0.9388889074325562,\n",
       "  0.9388889074325562,\n",
       "  0.9388889074325562,\n",
       "  0.9388889074325562,\n",
       "  0.9379629492759705,\n",
       "  0.9398148059844971,\n",
       "  0.9398148059844971,\n",
       "  0.9398148059844971,\n",
       "  0.9398148059844971,\n",
       "  0.9398148059844971,\n",
       "  0.9407407641410828,\n",
       "  0.9407407641410828,\n",
       "  0.9416666626930237,\n",
       "  0.9416666626930237,\n",
       "  0.9407407641410828,\n",
       "  0.9416666626930237,\n",
       "  0.9398148059844971,\n",
       "  0.9407407641410828,\n",
       "  0.9398148059844971,\n",
       "  0.9398148059844971,\n",
       "  0.9398148059844971,\n",
       "  0.9398148059844971,\n",
       "  0.9398148059844971,\n",
       "  0.9398148059844971,\n",
       "  0.9398148059844971,\n",
       "  0.9407407641410828,\n",
       "  0.9398148059844971,\n",
       "  0.9398148059844971,\n",
       "  0.9398148059844971,\n",
       "  0.9398148059844971,\n",
       "  0.9407407641410828,\n",
       "  0.9407407641410828,\n",
       "  0.9416666626930237,\n",
       "  0.9416666626930237,\n",
       "  0.9425926208496094,\n",
       "  0.9407407641410828,\n",
       "  0.9416666626930237,\n",
       "  0.9416666626930237,\n",
       "  0.9407407641410828,\n",
       "  0.9407407641410828,\n",
       "  0.9416666626930237,\n",
       "  0.9416666626930237,\n",
       "  0.9407407641410828,\n",
       "  0.9407407641410828,\n",
       "  0.9398148059844971,\n",
       "  0.9407407641410828,\n",
       "  0.9407407641410828,\n",
       "  0.9407407641410828,\n",
       "  0.9407407641410828,\n",
       "  0.9407407641410828,\n",
       "  0.9407407641410828,\n",
       "  0.9407407641410828,\n",
       "  0.9416666626930237,\n",
       "  0.9407407641410828,\n",
       "  0.9425926208496094,\n",
       "  0.9416666626930237,\n",
       "  0.9425926208496094,\n",
       "  0.9425926208496094,\n",
       "  0.9416666626930237,\n",
       "  0.9425926208496094,\n",
       "  0.9425926208496094,\n",
       "  0.9425926208496094,\n",
       "  0.9425926208496094,\n",
       "  0.9416666626930237,\n",
       "  0.9425926208496094,\n",
       "  0.9416666626930237,\n",
       "  0.9435185194015503,\n",
       "  0.9444444179534912,\n",
       "  0.9453703761100769,\n",
       "  0.9444444179534912,\n",
       "  0.9444444179534912,\n",
       "  0.9444444179534912,\n",
       "  0.9444444179534912,\n",
       "  0.9444444179534912,\n",
       "  0.9444444179534912,\n",
       "  0.9444444179534912,\n",
       "  0.9462962746620178,\n",
       "  0.9444444179534912,\n",
       "  0.9453703761100769,\n",
       "  0.9462962746620178,\n",
       "  0.9462962746620178,\n",
       "  0.9462962746620178,\n",
       "  0.9462962746620178,\n",
       "  0.9462962746620178,\n",
       "  0.9462962746620178,\n",
       "  0.9462962746620178,\n",
       "  0.9462962746620178,\n",
       "  0.9462962746620178,\n",
       "  0.9462962746620178,\n",
       "  0.9472222328186035,\n",
       "  0.9472222328186035,\n",
       "  0.9472222328186035,\n",
       "  0.9481481313705444,\n",
       "  0.9481481313705444,\n",
       "  0.949999988079071,\n",
       "  0.9490740895271301,\n",
       "  0.949999988079071,\n",
       "  0.949999988079071,\n",
       "  0.949999988079071,\n",
       "  0.949999988079071,\n",
       "  0.9509259462356567,\n",
       "  0.949999988079071,\n",
       "  0.9509259462356567,\n",
       "  0.9509259462356567,\n",
       "  0.9518518447875977,\n",
       "  0.9518518447875977,\n",
       "  0.9518518447875977,\n",
       "  0.9518518447875977,\n",
       "  0.9518518447875977,\n",
       "  0.9509259462356567,\n",
       "  0.9518518447875977,\n",
       "  0.9518518447875977,\n",
       "  0.9509259462356567,\n",
       "  0.9518518447875977,\n",
       "  0.9518518447875977,\n",
       "  0.9527778029441833,\n",
       "  0.9537037014961243,\n",
       "  0.9537037014961243,\n",
       "  0.9537037014961243,\n",
       "  0.9537037014961243,\n",
       "  0.9555555582046509,\n",
       "  0.9555555582046509,\n",
       "  0.9564814567565918,\n",
       "  0.9564814567565918,\n",
       "  0.9564814567565918,\n",
       "  0.9555555582046509,\n",
       "  0.9564814567565918,\n",
       "  0.9564814567565918,\n",
       "  0.9564814567565918,\n",
       "  0.9564814567565918,\n",
       "  0.9583333134651184,\n",
       "  0.9583333134651184,\n",
       "  0.9583333134651184,\n",
       "  0.9583333134651184,\n",
       "  0.9583333134651184,\n",
       "  0.9592592716217041,\n",
       "  0.9592592716217041,\n",
       "  0.960185170173645,\n",
       "  0.960185170173645,\n",
       "  0.960185170173645,\n",
       "  0.9592592716217041,\n",
       "  0.960185170173645,\n",
       "  0.9592592716217041,\n",
       "  0.960185170173645,\n",
       "  0.9592592716217041,\n",
       "  0.960185170173645,\n",
       "  0.9592592716217041,\n",
       "  0.960185170173645,\n",
       "  0.960185170173645,\n",
       "  0.9611111283302307,\n",
       "  0.960185170173645,\n",
       "  0.9620370268821716,\n",
       "  0.9620370268821716,\n",
       "  0.9620370268821716,\n",
       "  0.9629629850387573,\n",
       "  0.9629629850387573,\n",
       "  0.9611111283302307,\n",
       "  0.9620370268821716,\n",
       "  0.9620370268821716,\n",
       "  0.9620370268821716,\n",
       "  0.9620370268821716,\n",
       "  0.9620370268821716,\n",
       "  0.9620370268821716,\n",
       "  0.9620370268821716,\n",
       "  0.9629629850387573,\n",
       "  0.9629629850387573,\n",
       "  0.9638888835906982,\n",
       "  0.9629629850387573,\n",
       "  0.9638888835906982,\n",
       "  0.9648148417472839,\n",
       "  0.9648148417472839,\n",
       "  0.9648148417472839,\n",
       "  0.9648148417472839,\n",
       "  0.9657407402992249,\n",
       "  0.9666666388511658,\n",
       "  0.9666666388511658,\n",
       "  0.9666666388511658,\n",
       "  0.9675925970077515,\n",
       "  0.9666666388511658,\n",
       "  0.9675925970077515,\n",
       "  0.9685184955596924,\n",
       "  0.9666666388511658,\n",
       "  0.9685184955596924,\n",
       "  0.9685184955596924,\n",
       "  0.9685184955596924,\n",
       "  0.9675925970077515,\n",
       "  0.9685184955596924,\n",
       "  0.9694444537162781,\n",
       "  0.9685184955596924,\n",
       "  0.9694444537162781,\n",
       "  0.9712963104248047,\n",
       "  0.9712963104248047,\n",
       "  0.9712963104248047,\n",
       "  0.9712963104248047,\n",
       "  0.9722222089767456,\n",
       "  0.9712963104248047,\n",
       "  0.9731481671333313,\n",
       "  0.9722222089767456,\n",
       "  0.9731481671333313,\n",
       "  0.9731481671333313,\n",
       "  0.9731481671333313,\n",
       "  0.9731481671333313,\n",
       "  0.9731481671333313,\n",
       "  0.9731481671333313,\n",
       "  0.9740740656852722,\n",
       "  0.9740740656852722,\n",
       "  0.9740740656852722,\n",
       "  0.9750000238418579,\n",
       "  0.9740740656852722,\n",
       "  0.9750000238418579,\n",
       "  0.9750000238418579,\n",
       "  0.9750000238418579,\n",
       "  0.9750000238418579,\n",
       "  0.9768518805503845,\n",
       "  0.9768518805503845,\n",
       "  0.9768518805503845,\n",
       "  0.9768518805503845,\n",
       "  0.9768518805503845,\n",
       "  0.9777777791023254,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9777777791023254,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.9787036776542664,\n",
       "  0.980555534362793,\n",
       "  0.979629635810852,\n",
       "  0.980555534362793,\n",
       "  0.980555534362793,\n",
       "  0.980555534362793,\n",
       "  0.980555534362793,\n",
       "  0.980555534362793,\n",
       "  0.9814814925193787,\n",
       "  0.9814814925193787,\n",
       "  0.9814814925193787,\n",
       "  0.9814814925193787,\n",
       "  0.9824073910713196,\n",
       "  0.9824073910713196,\n",
       "  0.9824073910713196,\n",
       "  0.9833333492279053,\n",
       "  0.9833333492279053,\n",
       "  0.9842592477798462,\n",
       "  0.9833333492279053,\n",
       "  0.9842592477798462,\n",
       "  0.9842592477798462,\n",
       "  0.9842592477798462,\n",
       "  0.9842592477798462,\n",
       "  0.9842592477798462,\n",
       "  0.9842592477798462,\n",
       "  0.9842592477798462,\n",
       "  0.9842592477798462,\n",
       "  0.9842592477798462,\n",
       "  0.9861111044883728,\n",
       "  0.9842592477798462,\n",
       "  0.9842592477798462,\n",
       "  0.9851852059364319,\n",
       "  0.9861111044883728,\n",
       "  0.9851852059364319,\n",
       "  0.9851852059364319,\n",
       "  0.9861111044883728,\n",
       "  0.9861111044883728,\n",
       "  0.9861111044883728,\n",
       "  0.9861111044883728,\n",
       "  0.9870370626449585,\n",
       "  0.9861111044883728,\n",
       "  0.9861111044883728,\n",
       "  0.9870370626449585,\n",
       "  0.9870370626449585,\n",
       "  0.9861111044883728,\n",
       "  0.9861111044883728,\n",
       "  0.9870370626449585,\n",
       "  0.9879629611968994,\n",
       "  0.9879629611968994,\n",
       "  0.9870370626449585,\n",
       "  0.9879629611968994,\n",
       "  0.9879629611968994,\n",
       "  0.9879629611968994,\n",
       "  0.9879629611968994,\n",
       "  0.9879629611968994,\n",
       "  0.9879629611968994,\n",
       "  0.9888888597488403,\n",
       "  0.9888888597488403,\n",
       "  0.9888888597488403,\n",
       "  0.9888888597488403,\n",
       "  0.9888888597488403,\n",
       "  0.989814817905426,\n",
       "  0.9888888597488403,\n",
       "  0.9888888597488403,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.989814817905426,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9907407164573669,\n",
       "  0.9925925731658936,\n",
       "  0.9916666746139526,\n",
       "  0.9916666746139526,\n",
       "  0.9916666746139526,\n",
       "  0.9925925731658936,\n",
       "  0.9925925731658936,\n",
       "  0.9925925731658936,\n",
       "  0.9944444298744202,\n",
       "  0.9925925731658936,\n",
       "  0.9925925731658936,\n",
       "  0.9935185313224792,\n",
       "  0.9925925731658936,\n",
       "  0.9935185313224792,\n",
       "  0.9944444298744202,\n",
       "  0.9944444298744202,\n",
       "  0.9935185313224792,\n",
       "  0.9944444298744202,\n",
       "  0.9935185313224792,\n",
       "  0.9944444298744202,\n",
       "  0.9944444298744202,\n",
       "  0.9944444298744202,\n",
       "  0.9935185313224792,\n",
       "  0.9944444298744202,\n",
       "  0.9944444298744202,\n",
       "  0.9944444298744202,\n",
       "  0.9944444298744202,\n",
       "  0.9944444298744202,\n",
       "  0.9944444298744202,\n",
       "  0.9953703880310059,\n",
       "  0.9944444298744202,\n",
       "  0.9944444298744202,\n",
       "  0.9953703880310059,\n",
       "  0.9944444298744202,\n",
       "  0.9953703880310059,\n",
       "  0.9944444298744202,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9953703880310059,\n",
       "  0.9962962865829468,\n",
       "  0.9953703880310059,\n",
       "  0.9962962865829468,\n",
       "  0.9962962865829468,\n",
       "  0.9972222447395325,\n",
       "  0.9962962865829468,\n",
       "  0.9972222447395325,\n",
       "  0.9962962865829468,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9981481432914734,\n",
       "  0.9972222447395325,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9981481432914734,\n",
       "  0.9972222447395325,\n",
       "  0.9981481432914734,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9972222447395325,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9981481432914734,\n",
       "  0.9990741014480591,\n",
       "  0.9981481432914734,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  0.9990741014480591,\n",
       "  ...],\n",
       " 'val_loss': [1.7939677238464355,\n",
       "  1.7922638654708862,\n",
       "  1.789137601852417,\n",
       "  1.7867103815078735,\n",
       "  1.7840802669525146,\n",
       "  1.781585454940796,\n",
       "  1.7792423963546753,\n",
       "  1.7761458158493042,\n",
       "  1.7707641124725342,\n",
       "  1.7642359733581543,\n",
       "  1.7578071355819702,\n",
       "  1.750478982925415,\n",
       "  1.7401978969573975,\n",
       "  1.7274224758148193,\n",
       "  1.7123652696609497,\n",
       "  1.6961435079574585,\n",
       "  1.677833914756775,\n",
       "  1.6568435430526733,\n",
       "  1.6350497007369995,\n",
       "  1.6081500053405762,\n",
       "  1.5766364336013794,\n",
       "  1.5427480936050415,\n",
       "  1.5103636980056763,\n",
       "  1.476349949836731,\n",
       "  1.4430729150772095,\n",
       "  1.4116816520690918,\n",
       "  1.3785932064056396,\n",
       "  1.348730444908142,\n",
       "  1.3164353370666504,\n",
       "  1.285866379737854,\n",
       "  1.2557175159454346,\n",
       "  1.227718710899353,\n",
       "  1.2006281614303589,\n",
       "  1.1745383739471436,\n",
       "  1.1508755683898926,\n",
       "  1.1262229681015015,\n",
       "  1.1043522357940674,\n",
       "  1.080094575881958,\n",
       "  1.0596750974655151,\n",
       "  1.0417325496673584,\n",
       "  1.0205451250076294,\n",
       "  1.0041316747665405,\n",
       "  0.9871823787689209,\n",
       "  0.9709148406982422,\n",
       "  0.9502333402633667,\n",
       "  0.9318201541900635,\n",
       "  0.914212167263031,\n",
       "  0.90059494972229,\n",
       "  0.8879480957984924,\n",
       "  0.8750039935112,\n",
       "  0.861496090888977,\n",
       "  0.8500274419784546,\n",
       "  0.8369589447975159,\n",
       "  0.8248335123062134,\n",
       "  0.8131399750709534,\n",
       "  0.8009549379348755,\n",
       "  0.7895516753196716,\n",
       "  0.7785752415657043,\n",
       "  0.7682669162750244,\n",
       "  0.7583500146865845,\n",
       "  0.7482888698577881,\n",
       "  0.7395384311676025,\n",
       "  0.7306551337242126,\n",
       "  0.7218868136405945,\n",
       "  0.7128538489341736,\n",
       "  0.7055090069770813,\n",
       "  0.6966381669044495,\n",
       "  0.6893218755722046,\n",
       "  0.6813220977783203,\n",
       "  0.6743742227554321,\n",
       "  0.6667401790618896,\n",
       "  0.6603445410728455,\n",
       "  0.6532232761383057,\n",
       "  0.6469478607177734,\n",
       "  0.6403847336769104,\n",
       "  0.6348021626472473,\n",
       "  0.6284407377243042,\n",
       "  0.6222501397132874,\n",
       "  0.6174503564834595,\n",
       "  0.611426055431366,\n",
       "  0.6061058044433594,\n",
       "  0.601365327835083,\n",
       "  0.5961389541625977,\n",
       "  0.5913856625556946,\n",
       "  0.5864037871360779,\n",
       "  0.5821139216423035,\n",
       "  0.5775541067123413,\n",
       "  0.5730292201042175,\n",
       "  0.5686947107315063,\n",
       "  0.5641728639602661,\n",
       "  0.5602001547813416,\n",
       "  0.5569886565208435,\n",
       "  0.552414059638977,\n",
       "  0.5488996505737305,\n",
       "  0.5462442636489868,\n",
       "  0.5421192049980164,\n",
       "  0.5385981202125549,\n",
       "  0.5366836190223694,\n",
       "  0.5320648550987244,\n",
       "  0.5289475321769714,\n",
       "  0.5264920592308044,\n",
       "  0.522355854511261,\n",
       "  0.5198779106140137,\n",
       "  0.5175338387489319,\n",
       "  0.5133781433105469,\n",
       "  0.5108162760734558,\n",
       "  0.5094149708747864,\n",
       "  0.5052305459976196,\n",
       "  0.5033560395240784,\n",
       "  0.5016520619392395,\n",
       "  0.4978867173194885,\n",
       "  0.49577972292900085,\n",
       "  0.4961567521095276,\n",
       "  0.49193063378334045,\n",
       "  0.48960015177726746,\n",
       "  0.4891044795513153,\n",
       "  0.4856075048446655,\n",
       "  0.4843171536922455,\n",
       "  0.48329442739486694,\n",
       "  0.4801775813102722,\n",
       "  0.4793797731399536,\n",
       "  0.47822806239128113,\n",
       "  0.47652941942214966,\n",
       "  0.4753563702106476,\n",
       "  0.47397950291633606,\n",
       "  0.47148698568344116,\n",
       "  0.46984946727752686,\n",
       "  0.4702643156051636,\n",
       "  0.46672168374061584,\n",
       "  0.4656776487827301,\n",
       "  0.46456846594810486,\n",
       "  0.46278810501098633,\n",
       "  0.4612709879875183,\n",
       "  0.46075448393821716,\n",
       "  0.4585943818092346,\n",
       "  0.4578794538974762,\n",
       "  0.45706743001937866,\n",
       "  0.45454490184783936,\n",
       "  0.45350292325019836,\n",
       "  0.4529368579387665,\n",
       "  0.45132097601890564,\n",
       "  0.45043328404426575,\n",
       "  0.4498748183250427,\n",
       "  0.44795987010002136,\n",
       "  0.4473499655723572,\n",
       "  0.44680923223495483,\n",
       "  0.4453096091747284,\n",
       "  0.44471409916877747,\n",
       "  0.4439859986305237,\n",
       "  0.442552387714386,\n",
       "  0.4424188733100891,\n",
       "  0.44064489006996155,\n",
       "  0.4398881494998932,\n",
       "  0.43955108523368835,\n",
       "  0.43849289417266846,\n",
       "  0.4371519684791565,\n",
       "  0.43652960658073425,\n",
       "  0.4362005591392517,\n",
       "  0.43520689010620117,\n",
       "  0.4348667860031128,\n",
       "  0.4343290328979492,\n",
       "  0.43373921513557434,\n",
       "  0.43255943059921265,\n",
       "  0.43167659640312195,\n",
       "  0.4306029677391052,\n",
       "  0.430119127035141,\n",
       "  0.42911630868911743,\n",
       "  0.4291258156299591,\n",
       "  0.4273838698863983,\n",
       "  0.42774873971939087,\n",
       "  0.426136314868927,\n",
       "  0.42641812562942505,\n",
       "  0.4252789318561554,\n",
       "  0.4249245524406433,\n",
       "  0.42402756214141846,\n",
       "  0.4237751066684723,\n",
       "  0.422901451587677,\n",
       "  0.4224328398704529,\n",
       "  0.42200997471809387,\n",
       "  0.4210638701915741,\n",
       "  0.42097002267837524,\n",
       "  0.42061758041381836,\n",
       "  0.4199506938457489,\n",
       "  0.41904398798942566,\n",
       "  0.4197317957878113,\n",
       "  0.41815218329429626,\n",
       "  0.41840729117393494,\n",
       "  0.41709083318710327,\n",
       "  0.41758137941360474,\n",
       "  0.4166828691959381,\n",
       "  0.4159013628959656,\n",
       "  0.41503188014030457,\n",
       "  0.41459307074546814,\n",
       "  0.41411107778549194,\n",
       "  0.4135173559188843,\n",
       "  0.41279029846191406,\n",
       "  0.41252607107162476,\n",
       "  0.4115853011608124,\n",
       "  0.41097405552864075,\n",
       "  0.41084015369415283,\n",
       "  0.4100843667984009,\n",
       "  0.4098983407020569,\n",
       "  0.4094659090042114,\n",
       "  0.40878504514694214,\n",
       "  0.40875303745269775,\n",
       "  0.407620370388031,\n",
       "  0.4081922769546509,\n",
       "  0.40738609433174133,\n",
       "  0.40704581141471863,\n",
       "  0.4066876769065857,\n",
       "  0.4060370624065399,\n",
       "  0.40594208240509033,\n",
       "  0.40564587712287903,\n",
       "  0.4051980972290039,\n",
       "  0.40450090169906616,\n",
       "  0.4040754735469818,\n",
       "  0.40342336893081665,\n",
       "  0.4034876227378845,\n",
       "  0.4027878940105438,\n",
       "  0.40246671438217163,\n",
       "  0.40156903862953186,\n",
       "  0.4014665484428406,\n",
       "  0.40065011382102966,\n",
       "  0.40045297145843506,\n",
       "  0.3997403085231781,\n",
       "  0.3992798924446106,\n",
       "  0.39917081594467163,\n",
       "  0.39864954352378845,\n",
       "  0.3978005647659302,\n",
       "  0.3968467116355896,\n",
       "  0.39674943685531616,\n",
       "  0.3965305984020233,\n",
       "  0.3964216113090515,\n",
       "  0.3952163755893707,\n",
       "  0.39504215121269226,\n",
       "  0.3940087556838989,\n",
       "  0.3936043977737427,\n",
       "  0.392861932516098,\n",
       "  0.3928466737270355,\n",
       "  0.39169827103614807,\n",
       "  0.3913555145263672,\n",
       "  0.39094212651252747,\n",
       "  0.39020195603370667,\n",
       "  0.3897610604763031,\n",
       "  0.38937222957611084,\n",
       "  0.3884885013103485,\n",
       "  0.38782933354377747,\n",
       "  0.3875144422054291,\n",
       "  0.386284202337265,\n",
       "  0.3859196901321411,\n",
       "  0.38545581698417664,\n",
       "  0.3849402666091919,\n",
       "  0.3844088613986969,\n",
       "  0.3837096095085144,\n",
       "  0.38331133127212524,\n",
       "  0.38258373737335205,\n",
       "  0.38222625851631165,\n",
       "  0.38161495327949524,\n",
       "  0.38125401735305786,\n",
       "  0.38009005784988403,\n",
       "  0.3794572055339813,\n",
       "  0.37910544872283936,\n",
       "  0.3787122070789337,\n",
       "  0.3780743479728699,\n",
       "  0.37787336111068726,\n",
       "  0.3771129250526428,\n",
       "  0.37674084305763245,\n",
       "  0.37646371126174927,\n",
       "  0.37631675601005554,\n",
       "  0.37615421414375305,\n",
       "  0.37568607926368713,\n",
       "  0.37578973174095154,\n",
       "  0.3753250539302826,\n",
       "  0.37547072768211365,\n",
       "  0.37506893277168274,\n",
       "  0.3751243054866791,\n",
       "  0.3751519024372101,\n",
       "  0.3751987814903259,\n",
       "  0.3751603364944458,\n",
       "  0.3749158978462219,\n",
       "  0.3748278319835663,\n",
       "  0.3749675154685974,\n",
       "  0.37500593066215515,\n",
       "  0.37481021881103516,\n",
       "  0.3750206530094147,\n",
       "  0.3749123215675354,\n",
       "  0.37495216727256775,\n",
       "  0.37488120794296265,\n",
       "  0.3747662901878357,\n",
       "  0.37481456995010376,\n",
       "  0.37461262941360474,\n",
       "  0.3743232488632202,\n",
       "  0.37433725595474243,\n",
       "  0.374343603849411,\n",
       "  0.37427493929862976,\n",
       "  0.37398597598075867,\n",
       "  0.37372249364852905,\n",
       "  0.37368351221084595,\n",
       "  0.373378187417984,\n",
       "  0.3730391263961792,\n",
       "  0.37315037846565247,\n",
       "  0.37260398268699646,\n",
       "  0.3725210726261139,\n",
       "  0.37260928750038147,\n",
       "  0.3725026845932007,\n",
       "  0.37257063388824463,\n",
       "  0.3724140226840973,\n",
       "  0.3718201220035553,\n",
       "  0.37164565920829773,\n",
       "  0.37151986360549927,\n",
       "  0.37151286005973816,\n",
       "  0.37160974740982056,\n",
       "  0.37148597836494446,\n",
       "  0.3714866638183594,\n",
       "  0.37164947390556335,\n",
       "  0.37108638882637024,\n",
       "  0.3712955415248871,\n",
       "  0.37110239267349243,\n",
       "  0.3712014853954315,\n",
       "  0.3707740902900696,\n",
       "  0.3706951141357422,\n",
       "  0.37049582600593567,\n",
       "  0.3701132535934448,\n",
       "  0.3694598376750946,\n",
       "  0.3692017197608948,\n",
       "  0.3691331446170807,\n",
       "  0.36880257725715637,\n",
       "  0.3687068521976471,\n",
       "  0.36827000975608826,\n",
       "  0.3683744966983795,\n",
       "  0.3678852617740631,\n",
       "  0.36746910214424133,\n",
       "  0.36723047494888306,\n",
       "  0.36710911989212036,\n",
       "  0.3665783107280731,\n",
       "  0.36628472805023193,\n",
       "  0.36576321721076965,\n",
       "  0.3655253052711487,\n",
       "  0.3649393618106842,\n",
       "  0.3646223247051239,\n",
       "  0.36397045850753784,\n",
       "  0.36398154497146606,\n",
       "  0.3631780445575714,\n",
       "  0.36308369040489197,\n",
       "  0.36215683817863464,\n",
       "  0.3614426553249359,\n",
       "  0.36082980036735535,\n",
       "  0.3602680265903473,\n",
       "  0.3597552478313446,\n",
       "  0.35804489254951477,\n",
       "  0.3581177294254303,\n",
       "  0.35695722699165344,\n",
       "  0.35626524686813354,\n",
       "  0.35537129640579224,\n",
       "  0.3547157943248749,\n",
       "  0.3537842929363251,\n",
       "  0.3530100882053375,\n",
       "  0.35189664363861084,\n",
       "  0.35096004605293274,\n",
       "  0.35010069608688354,\n",
       "  0.3491434156894684,\n",
       "  0.34803885221481323,\n",
       "  0.34673914313316345,\n",
       "  0.34637251496315,\n",
       "  0.34487566351890564,\n",
       "  0.343798965215683,\n",
       "  0.34252700209617615,\n",
       "  0.3416686952114105,\n",
       "  0.3404616117477417,\n",
       "  0.33888372778892517,\n",
       "  0.33819344639778137,\n",
       "  0.33698970079421997,\n",
       "  0.3356930613517761,\n",
       "  0.33466294407844543,\n",
       "  0.3336940109729767,\n",
       "  0.33304208517074585,\n",
       "  0.33179154992103577,\n",
       "  0.3305378556251526,\n",
       "  0.32971179485321045,\n",
       "  0.32839301228523254,\n",
       "  0.32739683985710144,\n",
       "  0.3264968991279602,\n",
       "  0.3256630003452301,\n",
       "  0.32466694712638855,\n",
       "  0.32325324416160583,\n",
       "  0.3227633833885193,\n",
       "  0.32179391384124756,\n",
       "  0.32090646028518677,\n",
       "  0.3200138211250305,\n",
       "  0.3192296624183655,\n",
       "  0.3180390000343323,\n",
       "  0.3173380494117737,\n",
       "  0.3171464800834656,\n",
       "  0.3155616223812103,\n",
       "  0.3157518208026886,\n",
       "  0.3143225908279419,\n",
       "  0.3140535056591034,\n",
       "  0.3137677013874054,\n",
       "  0.3126243054866791,\n",
       "  0.31271857023239136,\n",
       "  0.3115435242652893,\n",
       "  0.31157398223876953,\n",
       "  0.31118491291999817,\n",
       "  0.3104366958141327,\n",
       "  0.3107151687145233,\n",
       "  0.30970412492752075,\n",
       "  0.3094943165779114,\n",
       "  0.3090953230857849,\n",
       "  0.3088516891002655,\n",
       "  0.30843836069107056,\n",
       "  0.30823972821235657,\n",
       "  0.307493656873703,\n",
       "  0.3076319694519043,\n",
       "  0.3068610727787018,\n",
       "  0.3068638741970062,\n",
       "  0.3061751425266266,\n",
       "  0.3060874342918396,\n",
       "  0.305460661649704,\n",
       "  0.30540263652801514,\n",
       "  0.304861843585968,\n",
       "  0.30453941226005554,\n",
       "  0.30429133772850037,\n",
       "  0.30387839674949646,\n",
       "  0.3035908043384552,\n",
       "  0.3033861219882965,\n",
       "  0.3025703728199005,\n",
       "  0.302453875541687,\n",
       "  0.30219343304634094,\n",
       "  0.30171072483062744,\n",
       "  0.30202198028564453,\n",
       "  0.30107980966567993,\n",
       "  0.30089521408081055,\n",
       "  0.3008160889148712,\n",
       "  0.30053237080574036,\n",
       "  0.3003312945365906,\n",
       "  0.2998177111148834,\n",
       "  0.29963093996047974,\n",
       "  0.29873090982437134,\n",
       "  0.29906508326530457,\n",
       "  0.29857438802719116,\n",
       "  0.2982367277145386,\n",
       "  0.2979721426963806,\n",
       "  0.2972196638584137,\n",
       "  0.2972245514392853,\n",
       "  0.2967658042907715,\n",
       "  0.2964385449886322,\n",
       "  0.295268177986145,\n",
       "  0.29491105675697327,\n",
       "  0.2942955195903778,\n",
       "  0.29422345757484436,\n",
       "  0.2938792407512665,\n",
       "  0.29371121525764465,\n",
       "  0.29325783252716064,\n",
       "  0.29320383071899414,\n",
       "  0.2928835451602936,\n",
       "  0.29244232177734375,\n",
       "  0.29209986329078674,\n",
       "  0.2921961545944214,\n",
       "  0.2920163571834564,\n",
       "  0.2913194000720978,\n",
       "  0.2913488447666168,\n",
       "  0.29105204343795776,\n",
       "  0.29140907526016235,\n",
       "  0.291051983833313,\n",
       "  0.2910013794898987,\n",
       "  0.2905403971672058,\n",
       "  0.2908344566822052,\n",
       "  0.2902663052082062,\n",
       "  0.2904733121395111,\n",
       "  0.28991997241973877,\n",
       "  0.2891525328159332,\n",
       "  0.28915679454803467,\n",
       "  0.28941744565963745,\n",
       "  0.2888958752155304,\n",
       "  0.2886523902416229,\n",
       "  0.28822556138038635,\n",
       "  0.28806236386299133,\n",
       "  0.2878957986831665,\n",
       "  0.2875518798828125,\n",
       "  0.2876943349838257,\n",
       "  0.2870486378669739,\n",
       "  0.2877701222896576,\n",
       "  0.28688859939575195,\n",
       "  0.28678184747695923,\n",
       "  0.2865472733974457,\n",
       "  0.28634464740753174,\n",
       "  0.2862364649772644,\n",
       "  0.2864949405193329,\n",
       "  0.2859991192817688,\n",
       "  0.286679208278656,\n",
       "  0.2857983708381653,\n",
       "  0.2861084043979645,\n",
       "  0.2862086594104767,\n",
       "  0.2852246165275574,\n",
       "  0.28570839762687683,\n",
       "  0.2847360670566559,\n",
       "  0.28566205501556396,\n",
       "  0.28524667024612427,\n",
       "  0.28535500168800354,\n",
       "  0.2853025496006012,\n",
       "  0.28501686453819275,\n",
       "  0.28509795665740967,\n",
       "  0.28478410840034485,\n",
       "  0.28516873717308044,\n",
       "  0.2845818102359772,\n",
       "  0.28505998849868774,\n",
       "  0.28428417444229126,\n",
       "  0.28439241647720337,\n",
       "  0.284365713596344,\n",
       "  0.28409168124198914,\n",
       "  0.28381070494651794,\n",
       "  0.28372305631637573,\n",
       "  0.28404560685157776,\n",
       "  0.28350263833999634,\n",
       "  0.28413838148117065,\n",
       "  0.283841997385025,\n",
       "  0.2834344506263733,\n",
       "  0.28358379006385803,\n",
       "  0.28382211923599243,\n",
       "  0.28344282507896423,\n",
       "  0.2836540639400482,\n",
       "  0.28349068760871887,\n",
       "  0.2832578122615814,\n",
       "  0.2832544445991516,\n",
       "  0.2834024727344513,\n",
       "  0.28338658809661865,\n",
       "  0.2833191454410553,\n",
       "  0.28273314237594604,\n",
       "  0.28350478410720825,\n",
       "  0.28359222412109375,\n",
       "  0.2833200991153717,\n",
       "  0.28315994143486023,\n",
       "  0.28367364406585693,\n",
       "  0.28354671597480774,\n",
       "  0.2839719355106354,\n",
       "  0.28311893343925476,\n",
       "  0.2837655246257782,\n",
       "  0.28388258814811707,\n",
       "  0.28379446268081665,\n",
       "  0.2835296094417572,\n",
       "  0.2842163145542145,\n",
       "  0.2835335433483124,\n",
       "  0.284061074256897,\n",
       "  0.2836323082447052,\n",
       "  0.2840425670146942,\n",
       "  0.2840527892112732,\n",
       "  0.28394222259521484,\n",
       "  0.2838783264160156,\n",
       "  0.28402435779571533,\n",
       "  0.2841702103614807,\n",
       "  0.2842589020729065,\n",
       "  0.2842065691947937,\n",
       "  0.2840976417064667,\n",
       "  0.2839028537273407,\n",
       "  0.28395774960517883,\n",
       "  0.28463032841682434,\n",
       "  0.2839933931827545,\n",
       "  0.28309524059295654,\n",
       "  0.28343889117240906,\n",
       "  0.28383681178092957,\n",
       "  0.2843209505081177,\n",
       "  0.28398099541664124,\n",
       "  0.28329822421073914,\n",
       "  0.28320470452308655,\n",
       "  0.2841585874557495,\n",
       "  0.28359705209732056,\n",
       "  0.2828889489173889,\n",
       "  0.28386735916137695,\n",
       "  0.2832128405570984,\n",
       "  0.2837384045124054,\n",
       "  0.28356507420539856,\n",
       "  0.28297966718673706,\n",
       "  0.2832716405391693,\n",
       "  0.2839197814464569,\n",
       "  0.28363385796546936,\n",
       "  0.28301507234573364,\n",
       "  0.2835528552532196,\n",
       "  0.28396981954574585,\n",
       "  0.2830713093280792,\n",
       "  0.28320324420928955,\n",
       "  0.28299441933631897,\n",
       "  0.283046156167984,\n",
       "  0.2835206389427185,\n",
       "  0.28359273076057434,\n",
       "  0.2829822599887848,\n",
       "  0.28406327962875366,\n",
       "  0.2834201455116272,\n",
       "  0.28350070118904114,\n",
       "  0.2830631136894226,\n",
       "  0.2833581864833832,\n",
       "  0.28345516324043274,\n",
       "  0.2832740247249603,\n",
       "  0.2836286425590515,\n",
       "  0.2837437093257904,\n",
       "  0.2832585573196411,\n",
       "  0.2831716537475586,\n",
       "  0.2839919626712799,\n",
       "  0.2832286059856415,\n",
       "  0.28364279866218567,\n",
       "  0.283844530582428,\n",
       "  0.2835629880428314,\n",
       "  0.2835415303707123,\n",
       "  0.28352123498916626,\n",
       "  0.2825920879840851,\n",
       "  0.28292980790138245,\n",
       "  0.282688707113266,\n",
       "  0.28329771757125854,\n",
       "  0.2825380265712738,\n",
       "  0.2828797698020935,\n",
       "  0.2821209728717804,\n",
       "  0.2824728786945343,\n",
       "  0.2822968065738678,\n",
       "  0.28245478868484497,\n",
       "  0.2815600037574768,\n",
       "  0.2817628085613251,\n",
       "  0.2816508114337921,\n",
       "  0.28236833214759827,\n",
       "  0.28154218196868896,\n",
       "  0.2815219759941101,\n",
       "  0.28146934509277344,\n",
       "  0.28137868642807007,\n",
       "  0.2809828519821167,\n",
       "  0.28128236532211304,\n",
       "  0.28033050894737244,\n",
       "  0.2811368405818939,\n",
       "  0.2806350290775299,\n",
       "  0.28075727820396423,\n",
       "  0.28016605973243713,\n",
       "  0.28097641468048096,\n",
       "  0.28089046478271484,\n",
       "  0.2805618941783905,\n",
       "  0.28058403730392456,\n",
       "  0.28058189153671265,\n",
       "  0.2803630232810974,\n",
       "  0.2806159257888794,\n",
       "  0.2805999219417572,\n",
       "  0.2809101641178131,\n",
       "  0.280544251203537,\n",
       "  0.2815157175064087,\n",
       "  0.28091323375701904,\n",
       "  0.2815568149089813,\n",
       "  0.2815912365913391,\n",
       "  0.281324177980423,\n",
       "  0.28159600496292114,\n",
       "  0.2825457751750946,\n",
       "  0.28160735964775085,\n",
       "  0.2818226218223572,\n",
       "  0.28239110112190247,\n",
       "  0.2820330262184143,\n",
       "  0.28247931599617004,\n",
       "  0.2831976115703583,\n",
       "  0.2825974225997925,\n",
       "  0.2829092741012573,\n",
       "  0.28335851430892944,\n",
       "  0.28295576572418213,\n",
       "  0.2832748293876648,\n",
       "  0.2830687165260315,\n",
       "  0.28367742896080017,\n",
       "  0.2842085659503937,\n",
       "  0.2839448153972626,\n",
       "  0.2841373085975647,\n",
       "  0.283726304769516,\n",
       "  0.2839716076850891,\n",
       "  0.2838190197944641,\n",
       "  0.28438714146614075,\n",
       "  0.28473806381225586,\n",
       "  0.2839964032173157,\n",
       "  0.2845200300216675,\n",
       "  0.2842871844768524,\n",
       "  0.28462427854537964,\n",
       "  0.2842201292514801,\n",
       "  0.2850002646446228,\n",
       "  0.2848883867263794,\n",
       "  0.2840944230556488,\n",
       "  0.284570574760437,\n",
       "  0.28444501757621765,\n",
       "  0.28464576601982117,\n",
       "  0.28459683060646057,\n",
       "  0.2838953733444214,\n",
       "  0.284180611371994,\n",
       "  0.28389403223991394,\n",
       "  0.2841586470603943,\n",
       "  0.28434187173843384,\n",
       "  0.2839314043521881,\n",
       "  0.28402888774871826,\n",
       "  0.28399473428726196,\n",
       "  0.28404539823532104,\n",
       "  0.28344470262527466,\n",
       "  0.2842348515987396,\n",
       "  0.2837199568748474,\n",
       "  0.28356581926345825,\n",
       "  0.28382664918899536,\n",
       "  0.2829720079898834,\n",
       "  0.28353068232536316,\n",
       "  0.2828659117221832,\n",
       "  0.28339526057243347,\n",
       "  0.28326910734176636,\n",
       "  0.2835036516189575,\n",
       "  0.2829374372959137,\n",
       "  0.2838572859764099,\n",
       "  0.2833322286605835,\n",
       "  0.2832236588001251,\n",
       "  0.2835754156112671,\n",
       "  0.2834409773349762,\n",
       "  0.2838485538959503,\n",
       "  0.28352710604667664,\n",
       "  0.28379616141319275,\n",
       "  0.2837692201137543,\n",
       "  0.28311365842819214,\n",
       "  0.2836266756057739,\n",
       "  0.2836126983165741,\n",
       "  0.2840755581855774,\n",
       "  0.28330618143081665,\n",
       "  0.28420576453208923,\n",
       "  0.2838994562625885,\n",
       "  0.28361666202545166,\n",
       "  0.2843618392944336,\n",
       "  0.28419047594070435,\n",
       "  0.28487429022789,\n",
       "  0.2840665578842163,\n",
       "  0.28444093465805054,\n",
       "  0.28464069962501526,\n",
       "  0.2847740054130554,\n",
       "  0.2847417891025543,\n",
       "  0.284500390291214,\n",
       "  0.2849633991718292,\n",
       "  0.28482770919799805,\n",
       "  0.28513920307159424,\n",
       "  0.2847192883491516,\n",
       "  0.2851579785346985,\n",
       "  0.2854854166507721,\n",
       "  0.2855893671512604,\n",
       "  0.28551891446113586,\n",
       "  0.285539448261261,\n",
       "  0.2856224477291107,\n",
       "  0.28590208292007446,\n",
       "  0.28608614206314087,\n",
       "  0.2862794101238251,\n",
       "  0.2856140732765198,\n",
       "  0.2863810360431671,\n",
       "  0.2866784930229187,\n",
       "  0.2861439287662506,\n",
       "  0.28705182671546936,\n",
       "  0.28646737337112427,\n",
       "  0.2873521149158478,\n",
       "  0.28733623027801514,\n",
       "  0.2871198058128357,\n",
       "  0.28755250573158264,\n",
       "  0.28733500838279724,\n",
       "  0.2877858579158783,\n",
       "  0.28781941533088684,\n",
       "  0.2875017523765564,\n",
       "  0.2883448898792267,\n",
       "  0.2879126965999603,\n",
       "  0.2887909710407257,\n",
       "  0.28903383016586304,\n",
       "  0.2885821461677551,\n",
       "  0.28945714235305786,\n",
       "  0.289061963558197,\n",
       "  0.2897852957248688,\n",
       "  0.28978589177131653,\n",
       "  0.2899344265460968,\n",
       "  0.2900620698928833,\n",
       "  0.2901049852371216,\n",
       "  0.2902350127696991,\n",
       "  0.2906322777271271,\n",
       "  0.29070359468460083,\n",
       "  0.29127728939056396,\n",
       "  0.29134443402290344,\n",
       "  0.2917483150959015,\n",
       "  0.292151540517807,\n",
       "  0.29159408807754517,\n",
       "  0.2919742166996002,\n",
       "  0.29235246777534485,\n",
       "  0.2921611964702606,\n",
       "  0.2930700182914734,\n",
       "  0.2923806309700012,\n",
       "  0.2931765913963318,\n",
       "  0.29320958256721497,\n",
       "  0.29360824823379517,\n",
       "  0.2937941253185272,\n",
       "  0.29293200373649597,\n",
       "  0.2939867377281189,\n",
       "  0.2936393618583679,\n",
       "  0.29438960552215576,\n",
       "  0.2935747802257538,\n",
       "  0.2946421504020691,\n",
       "  0.294189453125,\n",
       "  0.2943704426288605,\n",
       "  0.2945287823677063,\n",
       "  0.2946818470954895,\n",
       "  0.2954486906528473,\n",
       "  0.2945318818092346,\n",
       "  0.2956380248069763,\n",
       "  0.29491060972213745,\n",
       "  0.2953384518623352,\n",
       "  0.29550686478614807,\n",
       "  0.29676553606987,\n",
       "  0.29682254791259766,\n",
       "  0.2971120774745941,\n",
       "  0.29642531275749207,\n",
       "  0.29760080575942993,\n",
       "  0.2974190413951874,\n",
       "  0.29793986678123474,\n",
       "  0.29798397421836853,\n",
       "  0.29906165599823,\n",
       "  0.2982446849346161,\n",
       "  0.29908058047294617,\n",
       "  0.2997537851333618,\n",
       "  0.3000621199607849,\n",
       "  0.29990872740745544,\n",
       "  0.3011241853237152,\n",
       "  0.3007163405418396,\n",
       "  0.301115483045578,\n",
       "  0.30235958099365234,\n",
       "  0.30140620470046997,\n",
       "  0.3020471930503845,\n",
       "  0.3022536635398865,\n",
       "  0.3039478361606598,\n",
       "  0.30350837111473083,\n",
       "  0.3043508529663086,\n",
       "  0.30388423800468445,\n",
       "  0.30509084463119507,\n",
       "  0.30476057529449463,\n",
       "  0.3051844835281372,\n",
       "  0.3063310384750366,\n",
       "  0.30600351095199585,\n",
       "  0.30698099732398987,\n",
       "  0.3062976896762848,\n",
       "  0.30782878398895264,\n",
       "  0.30785462260246277,\n",
       "  0.3085187077522278,\n",
       "  0.30801886320114136,\n",
       "  0.3090721368789673,\n",
       "  0.30968913435935974,\n",
       "  0.3096170425415039,\n",
       "  0.3104819655418396,\n",
       "  0.3111878037452698,\n",
       "  0.31066834926605225,\n",
       "  0.31190410256385803,\n",
       "  0.31134384870529175,\n",
       "  0.3128339648246765,\n",
       "  0.31233131885528564,\n",
       "  0.31321951746940613,\n",
       "  0.3128567039966583,\n",
       "  0.31437525153160095,\n",
       "  0.3141760230064392,\n",
       "  0.3148481249809265,\n",
       "  0.3152928054332733,\n",
       "  0.3157256841659546,\n",
       "  0.31557828187942505,\n",
       "  0.3157845735549927,\n",
       "  0.31728023290634155,\n",
       "  0.3166287839412689,\n",
       "  0.3173489570617676,\n",
       "  0.3177871108055115,\n",
       "  0.3164612054824829,\n",
       "  0.3185539245605469,\n",
       "  0.31809893250465393,\n",
       "  0.3187829256057739,\n",
       "  0.31937357783317566,\n",
       "  0.3189605474472046,\n",
       "  0.31954148411750793,\n",
       "  0.31932127475738525,\n",
       "  0.32039833068847656,\n",
       "  0.32016706466674805,\n",
       "  0.3204246461391449,\n",
       "  0.32134777307510376,\n",
       "  0.32005131244659424,\n",
       "  0.32172781229019165,\n",
       "  0.3210708498954773,\n",
       "  0.3215632736682892,\n",
       "  0.3217225968837738,\n",
       "  0.3208005130290985,\n",
       "  0.32295310497283936,\n",
       "  0.32181403040885925,\n",
       "  0.3217853903770447,\n",
       "  0.32304847240448,\n",
       "  0.32261836528778076,\n",
       "  0.3232012689113617,\n",
       "  0.32220929861068726,\n",
       "  0.3231036365032196,\n",
       "  0.3228970170021057,\n",
       "  0.32285645604133606,\n",
       "  0.32401254773139954,\n",
       "  0.32299932837486267,\n",
       "  0.32308509945869446,\n",
       "  0.3239824175834656,\n",
       "  0.3236590027809143,\n",
       "  0.32361921668052673,\n",
       "  0.3235374391078949,\n",
       "  0.32334059476852417,\n",
       "  0.32398369908332825,\n",
       "  0.3235955536365509,\n",
       "  0.32413238286972046,\n",
       "  0.323908269405365,\n",
       "  0.324153333902359,\n",
       "  0.3240622878074646,\n",
       "  0.3248541057109833,\n",
       "  0.32490625977516174,\n",
       "  0.3242248594760895,\n",
       "  0.3252432644367218,\n",
       "  0.32495468854904175,\n",
       "  0.3246082365512848,\n",
       "  0.32480236887931824,\n",
       "  0.3248573839664459,\n",
       "  0.3254835903644562,\n",
       "  0.3257845342159271,\n",
       "  0.3256855309009552,\n",
       "  0.32585409283638,\n",
       "  0.325968474149704,\n",
       "  0.3261522352695465,\n",
       "  0.32547426223754883,\n",
       "  0.32606855034828186,\n",
       "  0.3269253373146057,\n",
       "  0.3263382017612457,\n",
       "  0.3263929486274719,\n",
       "  0.32632938027381897,\n",
       "  0.3264506757259369,\n",
       "  0.3276001513004303,\n",
       "  0.3267839550971985,\n",
       "  0.3263706862926483,\n",
       "  0.327147513628006,\n",
       "  0.3278411328792572,\n",
       "  0.32823219895362854,\n",
       "  0.32759302854537964,\n",
       "  0.32783612608909607,\n",
       "  0.3281458914279938,\n",
       "  0.3281784951686859,\n",
       "  0.3287689983844757,\n",
       "  0.32898417115211487,\n",
       "  0.3289056718349457,\n",
       "  0.3271375000476837,\n",
       "  0.3281659185886383,\n",
       "  0.32861626148223877,\n",
       "  0.3296022415161133,\n",
       "  0.3293727934360504,\n",
       "  0.32950249314308167,\n",
       "  0.3288401663303375,\n",
       "  0.32852447032928467,\n",
       "  0.33003300428390503,\n",
       "  0.3302851915359497,\n",
       "  0.33175981044769287,\n",
       "  0.3295968174934387,\n",
       "  0.33010074496269226,\n",
       "  0.33204618096351624,\n",
       "  0.3315291106700897,\n",
       "  0.3315015137195587,\n",
       "  0.3315698504447937,\n",
       "  0.33120399713516235,\n",
       "  0.3338961601257324,\n",
       "  0.332855224609375,\n",
       "  0.33256757259368896,\n",
       "  0.3322409391403198,\n",
       "  0.33381983637809753,\n",
       "  0.3342464566230774,\n",
       "  0.3347712457180023,\n",
       "  0.334635466337204,\n",
       "  0.33466485142707825,\n",
       "  0.335568904876709,\n",
       "  0.33569419384002686,\n",
       "  0.33608824014663696,\n",
       "  0.33585506677627563,\n",
       "  0.3359786570072174,\n",
       "  0.3378758132457733,\n",
       "  0.3369811773300171,\n",
       "  0.3375920057296753,\n",
       "  0.33803874254226685,\n",
       "  0.33816638588905334,\n",
       "  0.3399007022380829,\n",
       "  0.33978691697120667,\n",
       "  0.33925628662109375,\n",
       "  0.3397384583950043,\n",
       "  0.34125322103500366,\n",
       "  0.34221357107162476,\n",
       "  0.34157803654670715,\n",
       "  0.3433776795864105,\n",
       "  0.34253793954849243,\n",
       "  0.34307950735092163,\n",
       "  0.34420543909072876,\n",
       "  0.34554168581962585,\n",
       "  0.34579145908355713,\n",
       "  0.34666749835014343,\n",
       "  0.3466138243675232,\n",
       "  0.34675997495651245,\n",
       "  0.34899744391441345,\n",
       "  0.34850603342056274,\n",
       "  0.3502468764781952,\n",
       "  0.34988778829574585,\n",
       "  0.3509604036808014,\n",
       "  0.352175772190094,\n",
       "  0.35205575823783875,\n",
       "  0.35256427526474,\n",
       "  0.3548535406589508,\n",
       "  0.3531787693500519,\n",
       "  0.35539907217025757,\n",
       "  0.35528454184532166,\n",
       "  0.35571035742759705,\n",
       "  0.3583061099052429,\n",
       "  0.3582257926464081,\n",
       "  ...],\n",
       " 'val_accuracy': [0.17499999701976776,\n",
       "  0.15000000596046448,\n",
       "  0.14166666567325592,\n",
       "  0.13333334028720856,\n",
       "  0.1666666716337204,\n",
       "  0.23333333432674408,\n",
       "  0.25,\n",
       "  0.23333333432674408,\n",
       "  0.30000001192092896,\n",
       "  0.30000001192092896,\n",
       "  0.3333333432674408,\n",
       "  0.36666667461395264,\n",
       "  0.38333332538604736,\n",
       "  0.40833333134651184,\n",
       "  0.4416666626930237,\n",
       "  0.44999998807907104,\n",
       "  0.4749999940395355,\n",
       "  0.49166667461395264,\n",
       "  0.4749999940395355,\n",
       "  0.44999998807907104,\n",
       "  0.4583333432674408,\n",
       "  0.46666666865348816,\n",
       "  0.4749999940395355,\n",
       "  0.5083333253860474,\n",
       "  0.5,\n",
       "  0.5249999761581421,\n",
       "  0.5333333611488342,\n",
       "  0.5166666507720947,\n",
       "  0.5249999761581421,\n",
       "  0.5333333611488342,\n",
       "  0.5333333611488342,\n",
       "  0.5333333611488342,\n",
       "  0.550000011920929,\n",
       "  0.5666666626930237,\n",
       "  0.5833333134651184,\n",
       "  0.5833333134651184,\n",
       "  0.5916666388511658,\n",
       "  0.6166666746139526,\n",
       "  0.6333333253860474,\n",
       "  0.6166666746139526,\n",
       "  0.6416666507720947,\n",
       "  0.6416666507720947,\n",
       "  0.6416666507720947,\n",
       "  0.6333333253860474,\n",
       "  0.6416666507720947,\n",
       "  0.6499999761581421,\n",
       "  0.6583333611488342,\n",
       "  0.6666666865348816,\n",
       "  0.6666666865348816,\n",
       "  0.6666666865348816,\n",
       "  0.699999988079071,\n",
       "  0.699999988079071,\n",
       "  0.699999988079071,\n",
       "  0.7250000238418579,\n",
       "  0.7250000238418579,\n",
       "  0.7333333492279053,\n",
       "  0.7250000238418579,\n",
       "  0.7250000238418579,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.7416666746139526,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.7416666746139526,\n",
       "  0.7333333492279053,\n",
       "  0.7333333492279053,\n",
       "  0.75,\n",
       "  0.7416666746139526,\n",
       "  0.75,\n",
       "  0.7416666746139526,\n",
       "  0.7583333253860474,\n",
       "  0.7583333253860474,\n",
       "  0.7583333253860474,\n",
       "  0.7833333611488342,\n",
       "  0.7833333611488342,\n",
       "  0.7833333611488342,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8083333373069763,\n",
       "  0.8166666626930237,\n",
       "  0.8166666626930237,\n",
       "  0.8166666626930237,\n",
       "  0.824999988079071,\n",
       "  0.824999988079071,\n",
       "  0.824999988079071,\n",
       "  0.824999988079071,\n",
       "  0.824999988079071,\n",
       "  0.8333333134651184,\n",
       "  0.824999988079071,\n",
       "  0.824999988079071,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8416666388511658,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8416666388511658,\n",
       "  0.8333333134651184,\n",
       "  0.8416666388511658,\n",
       "  0.8500000238418579,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8416666388511658,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.875,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8583333492279053,\n",
       "  0.8666666746139526,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8583333492279053,\n",
       "  0.8666666746139526,\n",
       "  0.8583333492279053,\n",
       "  0.8666666746139526,\n",
       "  0.8583333492279053,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8583333492279053,\n",
       "  0.8666666746139526,\n",
       "  0.8583333492279053,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.8833333253860474,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8833333253860474,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8833333253860474,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9083333611488342,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9083333611488342,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9166666865348816,\n",
       "  0.9083333611488342,\n",
       "  0.9166666865348816,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.8999999761581421,\n",
       "  0.9083333611488342,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.9083333611488342,\n",
       "  0.8999999761581421,\n",
       "  0.9083333611488342,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8916666507720947,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  0.8833333253860474,\n",
       "  ...]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize the loss over time using `history.history`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'Accuracy'), Text(0.5, 0, 'Epoch')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The history.history[\"loss\"] entry is a dictionary with as many values as epochs that the\n",
    "# model was trained on. \n",
    "df_loss_acc = pd.DataFrame(history.history)\n",
    "df_loss= df_loss_acc[['loss','val_loss']]\n",
    "df_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)\n",
    "df_acc= df_loss_acc[['accuracy','val_accuracy']]\n",
    "df_acc.rename(columns={'accuracy':'train','val_accuracy':'validation'},inplace=True)\n",
    "df_loss.plot(title='Model loss',figsize=(12,8)).set(xlabel='Epoch',ylabel='Loss')\n",
    "df_acc.plot(title='Model Accuracy',figsize=(12,8)).set(xlabel='Epoch',ylabel='Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations**! You've finished the assignment and built two models: One that recognizes  smiles, and another that recognizes SIGN language with almost 80% accuracy on the test set. In addition to that, you now also understand the applications of two Keras APIs: Sequential and Functional. Nicely done! \n",
    "\n",
    "By now, you know a bit about how the Functional API works and may have glimpsed the possibilities. In your next assignment, you'll really get a feel for its power when you get the opportunity to build a very deep ConvNet, using ResNets! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Bibliography\n",
    "\n",
    "You're always encouraged to read the official documentation. To that end, you can find the docs for the Sequential and Functional APIs here: \n",
    "\n",
    "https://www.tensorflow.org/guide/keras/sequential_model\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/functional"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "bwbJV",
   "launcher_item_id": "0TkXB"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
